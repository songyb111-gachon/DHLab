{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:39:09\n",
      "Logging to ./ppo_with_mask/PPO_119\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:39:10.196012\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633627, PSNR Change: -0.000071, PSNR Diff: -0.000071 (New Max), Reward: -5.65, 08:39:11\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631062, PSNR After: 27.631058, PSNR Change: -0.000004, PSNR Diff: -0.002640, Reward: -0.31, 08:41:12\n",
      "Step: 200, PSNR Before: 27.627619, PSNR After: 27.627668, PSNR Change: 0.000050, PSNR Diff: -0.006029, Reward: 3.97, 08:43:14\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 0   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 312 |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010082 < -0.01 at step 300\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 1, Max consecutive episode failures: 1\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:45:24\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:45:24.869915\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 08:45:26\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629232, PSNR After: 27.629147, PSNR Change: -0.000086, PSNR Diff: -0.004551, Reward: -6.87, 08:47:28\n",
      "Step: 200, PSNR Before: 27.626316, PSNR After: 27.626459, PSNR Change: 0.000143, PSNR Diff: -0.007238, Reward: 11.44, 08:49:32\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 301           |\n",
      "|    ep_rew_mean          | -882          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 636           |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6763806e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000289      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.206         |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -7.21e-05     |\n",
      "|    value_loss           | 0.84          |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 251\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 2, Max consecutive episode failures: 2\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:50:43\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:50:44.026132\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633430, PSNR Change: -0.000267, PSNR Diff: -0.000267 (New Max), Reward: -21.36, 08:50:45\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633430, PSNR After: 27.633453, PSNR Change: 0.000023, PSNR Diff: -0.000244 (New Max), Reward: 1.83, 08:50:46\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629452, PSNR After: 27.629499, PSNR Change: 0.000048, PSNR Diff: -0.004198, Reward: 3.81, 08:52:47\n",
      "Step: 200, PSNR Before: 27.626345, PSNR After: 27.626259, PSNR Change: -0.000086, PSNR Diff: -0.007439, Reward: -6.87, 08:54:52\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 276           |\n",
      "|    ep_rew_mean          | -888          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 961           |\n",
      "|    total_timesteps      | 768           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8184226e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.144         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.038         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000105     |\n",
      "|    value_loss           | 0.148         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 277\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 3, Max consecutive episode failures: 3\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:56:35\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:56:36.374402\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633526, PSNR Change: -0.000172, PSNR Diff: -0.000172 (New Max), Reward: -13.73, 08:56:37\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629129, PSNR After: 27.629129, PSNR Change: 0.000000, PSNR Diff: -0.004568, Reward: 0.00, 08:58:41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 277          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1290         |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.098991e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.174        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00012     |\n",
      "|    value_loss           | 0.225        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624681, PSNR After: 27.624666, PSNR Change: -0.000015, PSNR Diff: -0.009031, Reward: -1.22, 09:00:55\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010063 < -0.01 at step 220\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 4, Max consecutive episode failures: 4\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:01:21\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:01:22.280500\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633671, PSNR Change: -0.000027, PSNR Diff: -0.000027 (New Max), Reward: -2.14, 09:01:23\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631344, PSNR After: 27.631342, PSNR Change: -0.000002, PSNR Diff: -0.002356, Reward: -0.15, 09:03:30\n",
      "Step: 200, PSNR Before: 27.627567, PSNR After: 27.627552, PSNR Change: -0.000015, PSNR Diff: -0.006145, Reward: -1.22, 09:05:38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 263          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1623         |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.613562e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.00521      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.215        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    value_loss           | 0.457        |\n",
      "------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 288\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 5, Max consecutive episode failures: 5\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:07:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:07:37.780428\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633669, PSNR Change: -0.000029, PSNR Diff: -0.000029 (New Max), Reward: -2.29, 09:07:38\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630955, PSNR After: 27.630930, PSNR Change: -0.000025, PSNR Diff: -0.002768, Reward: -1.98, 09:09:46\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 268          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1958         |\n",
      "|    total_timesteps      | 1536         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.476207e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0168      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0727       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000161    |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.625391, PSNR After: 27.625238, PSNR Change: -0.000153, PSNR Diff: -0.008459, Reward: -12.21, 09:12:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010080 < -0.01 at step 270\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 6, Max consecutive episode failures: 6\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:13:30\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:13:31.544093\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633694, PSNR Change: -0.000004, PSNR Diff: -0.000004 (New Max), Reward: -0.31, 09:13:32\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633688, PSNR After: 27.633709, PSNR Change: 0.000021, PSNR Diff: 0.000011 (New Max), Reward: 1.68, 09:13:39\u001b[0m\n",
      "\u001b[94mStep: 8, PSNR Before: 27.633682, PSNR After: 27.633728, PSNR Change: 0.000046, PSNR Diff: 0.000031 (New Max), Reward: 3.66, 09:13:41\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630022, PSNR After: 27.630033, PSNR Change: 0.000011, PSNR Diff: -0.003664, Reward: 0.92, 09:15:38\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 2290          |\n",
      "|    total_timesteps      | 1792          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0221265e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0108       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.16          |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000178     |\n",
      "|    value_loss           | 0.383         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627369, PSNR After: 27.627262, PSNR Change: -0.000107, PSNR Diff: -0.006435, Reward: -8.54, 09:17:52\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010014 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 7, Max consecutive episode failures: 7\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:19:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:19:49.211097\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.634045, PSNR Change: 0.000347, PSNR Diff: 0.000347 (New Max), Reward: 27.77, 09:19:50\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631104, PSNR After: 27.631069, PSNR Change: -0.000034, PSNR Diff: -0.002628, Reward: -2.75, 09:21:56\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 2622          |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1664815e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0332        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0406        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000197     |\n",
      "|    value_loss           | 0.108         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.628286, PSNR After: 27.628162, PSNR Change: -0.000124, PSNR Diff: -0.005535, Reward: -9.92, 09:24:10\n",
      "Step: 300, PSNR Before: 27.624249, PSNR After: 27.624205, PSNR Change: -0.000044, PSNR Diff: -0.009493, Reward: -3.51, 09:26:19\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010073 < -0.01 at step 312\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 8, Max consecutive episode failures: 8\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:26:35\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:26:36.286397\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633619, PSNR Change: -0.000078, PSNR Diff: -0.000078 (New Max), Reward: -6.26, 09:26:37\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 277           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2957          |\n",
      "|    total_timesteps      | 2304          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4039688e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.104         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0633        |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000215     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630299, PSNR After: 27.630318, PSNR Change: 0.000019, PSNR Diff: -0.003380, Reward: 1.53, 09:28:51\n",
      "Step: 200, PSNR Before: 27.625862, PSNR After: 27.625835, PSNR Change: -0.000027, PSNR Diff: -0.007862, Reward: -2.14, 09:30:59\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010078 < -0.01 at step 251\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 9, Max consecutive episode failures: 9\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:32:05\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:32:06.024651\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633724, PSNR Change: 0.000027, PSNR Diff: 0.000027 (New Max), Reward: 2.14, 09:32:07\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 274           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 3293          |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4854595e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0726        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.047         |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000222     |\n",
      "|    value_loss           | 0.142         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629721, PSNR After: 27.629650, PSNR Change: -0.000071, PSNR Diff: -0.004047, Reward: -5.65, 09:34:22\n",
      "Step: 200, PSNR Before: 27.626022, PSNR After: 27.626028, PSNR Change: 0.000006, PSNR Diff: -0.007669, Reward: 0.46, 09:36:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 260\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 10, Max consecutive episode failures: 10\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:37:47\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:37:48.156766\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633535, PSNR Change: -0.000162, PSNR Diff: -0.000162 (New Max), Reward: -12.97, 09:37:49\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633535, PSNR After: 27.633537, PSNR Change: 0.000002, PSNR Diff: -0.000160 (New Max), Reward: 0.15, 09:37:50\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633537, PSNR After: 27.633608, PSNR Change: 0.000071, PSNR Diff: -0.000090 (New Max), Reward: 5.65, 09:37:51\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633574, PSNR After: 27.633663, PSNR Change: 0.000090, PSNR Diff: -0.000034 (New Max), Reward: 7.17, 09:37:54\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633663, PSNR After: 27.633698, PSNR Change: 0.000034, PSNR Diff: 0.000000 (New Max), Reward: 2.75, 09:37:55\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633698, PSNR After: 27.633751, PSNR Change: 0.000053, PSNR Diff: 0.000053 (New Max), Reward: 4.27, 09:37:56\u001b[0m\n",
      "\u001b[94mStep: 8, PSNR Before: 27.633751, PSNR After: 27.633785, PSNR Change: 0.000034, PSNR Diff: 0.000088 (New Max), Reward: 2.75, 09:37:58\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 273          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 3628         |\n",
      "|    total_timesteps      | 2816         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.557637e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0804       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0975       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000231    |\n",
      "|    value_loss           | 0.222        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629406, PSNR After: 27.629456, PSNR Change: 0.000050, PSNR Diff: -0.004242, Reward: 3.97, 09:40:03\n",
      "Step: 200, PSNR Before: 27.625372, PSNR After: 27.625322, PSNR Change: -0.000050, PSNR Diff: -0.008375, Reward: -3.97, 09:42:12\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010042 < -0.01 at step 229\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 11, Max consecutive episode failures: 11\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:42:49\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:42:49.765071\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633625, PSNR Change: -0.000072, PSNR Diff: -0.000072 (New Max), Reward: -5.80, 09:42:50\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633625, PSNR After: 27.633675, PSNR Change: 0.000050, PSNR Diff: -0.000023 (New Max), Reward: 3.97, 09:42:52\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633640, PSNR After: 27.633715, PSNR Change: 0.000074, PSNR Diff: 0.000017 (New Max), Reward: 5.95, 09:42:54\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630293, PSNR After: 27.630299, PSNR Change: 0.000006, PSNR Diff: -0.003399, Reward: 0.46, 09:44:57\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 3962          |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8277206e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00226       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0944        |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000245     |\n",
      "|    value_loss           | 0.19          |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626860, PSNR After: 27.626831, PSNR Change: -0.000029, PSNR Diff: -0.006866, Reward: -2.29, 09:47:12\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010048 < -0.01 at step 289\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 12, Max consecutive episode failures: 12\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:49:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:49:07.740027\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633665, PSNR Change: -0.000032, PSNR Diff: -0.000032 (New Max), Reward: -2.59, 09:49:08\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 271          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 4297         |\n",
      "|    total_timesteps      | 3328         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.876615e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0396       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0822       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000256    |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630798, PSNR After: 27.630730, PSNR Change: -0.000069, PSNR Diff: -0.002968, Reward: -5.49, 09:51:23\n",
      "Step: 200, PSNR Before: 27.627804, PSNR After: 27.627792, PSNR Change: -0.000011, PSNR Diff: -0.005905, Reward: -0.92, 09:53:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010023 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 13, Max consecutive episode failures: 13\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:55:27\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:55:28.002670\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633802, PSNR Change: 0.000105, PSNR Diff: 0.000105 (New Max), Reward: 8.39, 09:55:29\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633734, PSNR After: 27.633818, PSNR Change: 0.000084, PSNR Diff: 0.000120 (New Max), Reward: 6.71, 09:55:32\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 272          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 4630         |\n",
      "|    total_timesteps      | 3584         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.188608e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0582       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00028     |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630262, PSNR After: 27.630157, PSNR Change: -0.000105, PSNR Diff: -0.003540, Reward: -8.39, 09:57:41\n",
      "Step: 200, PSNR Before: 27.626638, PSNR After: 27.626534, PSNR Change: -0.000105, PSNR Diff: -0.007164, Reward: -8.39, 09:59:48\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010029 < -0.01 at step 272\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 14, Max consecutive episode failures: 14\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:01:19\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:01:20.438316\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 10:01:21\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633677, PSNR After: 27.633684, PSNR Change: 0.000008, PSNR Diff: -0.000013 (New Max), Reward: 0.61, 10:01:24\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633677, PSNR After: 27.633705, PSNR Change: 0.000029, PSNR Diff: 0.000008 (New Max), Reward: 2.29, 10:01:26\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633705, PSNR After: 27.633745, PSNR Change: 0.000040, PSNR Diff: 0.000048 (New Max), Reward: 3.20, 10:01:28\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 4962          |\n",
      "|    total_timesteps      | 3840          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1606684e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0186        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0663        |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000278     |\n",
      "|    value_loss           | 0.147         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.631290, PSNR After: 27.631165, PSNR Change: -0.000126, PSNR Diff: -0.002533, Reward: -10.07, 10:03:34\n",
      "Step: 200, PSNR Before: 27.626322, PSNR After: 27.626322, PSNR Change: 0.000000, PSNR Diff: -0.007376, Reward: 0.00, 10:05:41\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010017 < -0.01 at step 255\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 15, Max consecutive episode failures: 15\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:06:51\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:06:51.849668\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633554, PSNR Change: -0.000143, PSNR Diff: -0.000143 (New Max), Reward: -11.44, 10:06:53\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 5293          |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2328459e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.000799     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0596        |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000275     |\n",
      "|    value_loss           | 0.133         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.628977, PSNR After: 27.629005, PSNR Change: 0.000029, PSNR Diff: -0.004692, Reward: 2.29, 10:09:06\n",
      "Step: 200, PSNR Before: 27.626842, PSNR After: 27.626808, PSNR Change: -0.000034, PSNR Diff: -0.006889, Reward: -2.75, 10:11:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010166 < -0.01 at step 280\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 16, Max consecutive episode failures: 16\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:12:56\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 5627          |\n",
      "|    total_timesteps      | 4352          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3259781e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000441      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.101         |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000284     |\n",
      "|    value_loss           | 0.219         |\n",
      "-------------------------------------------\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:13:04.129921\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633696, PSNR Change: -0.000002, PSNR Diff: -0.000002 (New Max), Reward: -0.15, 10:13:05\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630795, PSNR After: 27.630775, PSNR Change: -0.000019, PSNR Diff: -0.002922, Reward: -1.53, 10:15:12\n",
      "Step: 200, PSNR Before: 27.626589, PSNR After: 27.626034, PSNR Change: -0.000555, PSNR Diff: -0.007664, Reward: -44.40, 10:17:19\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 5960          |\n",
      "|    total_timesteps      | 4608          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6868656e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0155        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0655        |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000305     |\n",
      "|    value_loss           | 0.151         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010233 < -0.01 at step 286\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 17, Max consecutive episode failures: 17\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:19:14\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:19:15.156829\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633657, PSNR Change: -0.000040, PSNR Diff: -0.000040 (New Max), Reward: -3.20, 10:19:16\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629692, PSNR After: 27.629679, PSNR Change: -0.000013, PSNR Diff: -0.004019, Reward: -1.07, 10:21:19\n",
      "Step: 200, PSNR Before: 27.627211, PSNR After: 27.627066, PSNR Change: -0.000145, PSNR Diff: -0.006632, Reward: -11.60, 10:23:23\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 273           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 6284          |\n",
      "|    total_timesteps      | 4864          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7869828e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00162       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0424        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000296     |\n",
      "|    value_loss           | 0.151         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.623873, PSNR After: 27.623974, PSNR Change: 0.000101, PSNR Diff: -0.009724, Reward: 8.09, 10:25:36\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010025 < -0.01 at step 319\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 18, Max consecutive episode failures: 18\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:26:00\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:26:01.597234\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633625, PSNR Change: -0.000072, PSNR Diff: -0.000072 (New Max), Reward: -5.80, 10:26:02\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629200, PSNR After: 27.629101, PSNR Change: -0.000099, PSNR Diff: -0.004597, Reward: -7.93, 10:28:08\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 276           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 6615          |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2107346e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0405        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0424        |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00033      |\n",
      "|    value_loss           | 0.0951        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624018, PSNR After: 27.623941, PSNR Change: -0.000076, PSNR Diff: -0.009756, Reward: -6.10, 10:30:21\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010059 < -0.01 at step 205\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 19, Max consecutive episode failures: 19\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:30:28\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:30:29.089712\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633636, PSNR Change: -0.000061, PSNR Diff: -0.000061 (New Max), Reward: -4.88, 10:30:30\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629753, PSNR After: 27.629700, PSNR Change: -0.000053, PSNR Diff: -0.003998, Reward: -4.27, 10:32:35\n",
      "Step: 200, PSNR Before: 27.625778, PSNR After: 27.625587, PSNR Change: -0.000191, PSNR Diff: -0.008110, Reward: -15.26, 10:34:42\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 6946          |\n",
      "|    total_timesteps      | 5376          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8847717e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0225       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0902        |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.000322     |\n",
      "|    value_loss           | 0.213         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010042 < -0.01 at step 235\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 20, Max consecutive episode failures: 20\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:35:33\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:35:34.508316\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633783, PSNR Change: 0.000086, PSNR Diff: 0.000086 (New Max), Reward: 6.87, 10:35:35\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633747, PSNR After: 27.633850, PSNR Change: 0.000103, PSNR Diff: 0.000153 (New Max), Reward: 8.24, 10:35:40\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630051, PSNR After: 27.629833, PSNR Change: -0.000217, PSNR Diff: -0.003864, Reward: -17.40, 10:37:40\n",
      "Step: 200, PSNR Before: 27.625856, PSNR After: 27.625807, PSNR Change: -0.000050, PSNR Diff: -0.007891, Reward: -3.97, 10:39:48\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 7277          |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3038668e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000925      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0927        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000354     |\n",
      "|    value_loss           | 0.193         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010014 < -0.01 at step 234\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 21, Max consecutive episode failures: 21\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:40:38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:40:38.987547\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633575, PSNR Change: -0.000122, PSNR Diff: -0.000122 (New Max), Reward: -9.77, 10:40:40\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629776, PSNR After: 27.629698, PSNR Change: -0.000078, PSNR Diff: -0.004000, Reward: -6.26, 10:42:45\n",
      "Step: 200, PSNR Before: 27.627342, PSNR After: 27.627254, PSNR Change: -0.000088, PSNR Diff: -0.006443, Reward: -7.02, 10:44:52\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 7607          |\n",
      "|    total_timesteps      | 5888          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2340176e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0188       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0941        |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000338     |\n",
      "|    value_loss           | 0.234         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010078 < -0.01 at step 269\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 22, Max consecutive episode failures: 22\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:46:26\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:46:27.143285\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633717, PSNR Change: 0.000019, PSNR Diff: 0.000019 (New Max), Reward: 1.53, 10:46:28\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633717, PSNR After: 27.633739, PSNR Change: 0.000023, PSNR Diff: 0.000042 (New Max), Reward: 1.83, 10:46:29\u001b[0m\n",
      "Step: 100, PSNR Before: 27.632118, PSNR After: 27.632113, PSNR Change: -0.000006, PSNR Diff: -0.001585, Reward: -0.46, 10:48:34\n",
      "Step: 200, PSNR Before: 27.629353, PSNR After: 27.629337, PSNR Change: -0.000015, PSNR Diff: -0.004360, Reward: -1.22, 10:50:41\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 7939          |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6344863e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0117       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0996        |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000363     |\n",
      "|    value_loss           | 0.187         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.625692, PSNR After: 27.625648, PSNR Change: -0.000044, PSNR Diff: -0.008049, Reward: -3.51, 10:52:55\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 352\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 23, Max consecutive episode failures: 23\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:54:01\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:54:02.448001\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633570, PSNR Change: -0.000128, PSNR Diff: -0.000128 (New Max), Reward: -10.22, 10:54:03\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630060, PSNR After: 27.629976, PSNR Change: -0.000084, PSNR Diff: -0.003721, Reward: -6.71, 10:56:09\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 8271          |\n",
      "|    total_timesteps      | 6400          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7416037e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0422       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0138        |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000456     |\n",
      "|    value_loss           | 0.0408        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624435, PSNR After: 27.624226, PSNR Change: -0.000210, PSNR Diff: -0.009472, Reward: -16.78, 10:58:24\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010193 < -0.01 at step 220\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 24, Max consecutive episode failures: 24\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:58:49\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:58:50.230278\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633774, PSNR Change: 0.000076, PSNR Diff: 0.000076 (New Max), Reward: 6.10, 10:58:51\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633774, PSNR After: 27.633791, PSNR Change: 0.000017, PSNR Diff: 0.000093 (New Max), Reward: 1.37, 10:58:52\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633759, PSNR After: 27.634029, PSNR Change: 0.000271, PSNR Diff: 0.000332 (New Max), Reward: 21.67, 10:58:55\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629910, PSNR After: 27.629833, PSNR Change: -0.000076, PSNR Diff: -0.003864, Reward: -6.10, 11:00:57\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 8603          |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0698797e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0278       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0859        |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00039      |\n",
      "|    value_loss           | 0.179         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627260, PSNR After: 27.627220, PSNR Change: -0.000040, PSNR Diff: -0.006477, Reward: -3.20, 11:03:10\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010010 < -0.01 at step 282\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 25, Max consecutive episode failures: 25\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:04:53\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:04:54.125706\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633703, PSNR Change: 0.000006, PSNR Diff: 0.000006 (New Max), Reward: 0.46, 11:04:55\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630327, PSNR After: 27.630318, PSNR Change: -0.000010, PSNR Diff: -0.003380, Reward: -0.76, 11:07:00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 8933          |\n",
      "|    total_timesteps      | 6912          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2724423e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00178       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.082         |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.000398     |\n",
      "|    value_loss           | 0.169         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626186, PSNR After: 27.625984, PSNR Change: -0.000202, PSNR Diff: -0.007713, Reward: -16.17, 11:09:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010002 < -0.01 at step 240\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 26, Max consecutive episode failures: 26\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:10:05\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:10:05.821525\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 11:10:07\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630272, PSNR After: 27.630522, PSNR Change: 0.000250, PSNR Diff: -0.003176, Reward: 19.99, 11:12:13\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 9264         |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.206093e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.00748     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000416    |\n",
      "|    value_loss           | 0.0863       |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624197, PSNR After: 27.624159, PSNR Change: -0.000038, PSNR Diff: -0.009539, Reward: -3.05, 11:14:26\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010201 < -0.01 at step 209\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 27, Max consecutive episode failures: 27\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:14:38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:14:38.685296\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633644, PSNR Change: -0.000053, PSNR Diff: -0.000053 (New Max), Reward: -4.27, 11:14:39\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633644, PSNR After: 27.633665, PSNR Change: 0.000021, PSNR Diff: -0.000032 (New Max), Reward: 1.68, 11:14:41\u001b[0m\n",
      "Step: 100, PSNR Before: 27.628887, PSNR After: 27.628895, PSNR Change: 0.000008, PSNR Diff: -0.004803, Reward: 0.61, 11:16:44\n",
      "Step: 200, PSNR Before: 27.626301, PSNR After: 27.626276, PSNR Change: -0.000025, PSNR Diff: -0.007421, Reward: -1.98, 11:18:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 267           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 9594          |\n",
      "|    total_timesteps      | 7424          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0105155e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0139        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0926        |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000441     |\n",
      "|    value_loss           | 0.223         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010130 < -0.01 at step 292\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 28, Max consecutive episode failures: 28\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:20:54\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:20:55.582822\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633656, PSNR Change: -0.000042, PSNR Diff: -0.000042 (New Max), Reward: -3.36, 11:20:56\u001b[0m\n",
      "\u001b[94mStep: 29, PSNR Before: 27.633644, PSNR After: 27.633694, PSNR Change: 0.000050, PSNR Diff: -0.000004 (New Max), Reward: 3.97, 11:21:32\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630753, PSNR After: 27.630751, PSNR Change: -0.000002, PSNR Diff: -0.002947, Reward: -0.15, 11:23:02\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 9924          |\n",
      "|    total_timesteps      | 7680          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6216883e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0295        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.102         |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000412     |\n",
      "|    value_loss           | 0.178         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626627, PSNR After: 27.626600, PSNR Change: -0.000027, PSNR Diff: -0.007097, Reward: -2.14, 11:25:15\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010136 < -0.01 at step 264\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 29, Max consecutive episode failures: 29\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:26:36\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:26:37.057146\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633638, PSNR Change: -0.000059, PSNR Diff: -0.000059 (New Max), Reward: -4.73, 11:26:38\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633638, PSNR After: 27.633646, PSNR Change: 0.000008, PSNR Diff: -0.000051 (New Max), Reward: 0.61, 11:26:39\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630236, PSNR After: 27.630312, PSNR Change: 0.000076, PSNR Diff: -0.003386, Reward: 6.10, 11:28:43\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 10254         |\n",
      "|    total_timesteps      | 7936          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5146014e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0225       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.021         |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000497     |\n",
      "|    value_loss           | 0.0681        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626129, PSNR After: 27.625788, PSNR Change: -0.000341, PSNR Diff: -0.007910, Reward: -27.31, 11:30:56\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010216 < -0.01 at step 261\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 30, Max consecutive episode failures: 30\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:32:11\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:32:12.531057\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633652, PSNR Change: -0.000046, PSNR Diff: -0.000046 (New Max), Reward: -3.66, 11:32:13\u001b[0m\n",
      "\u001b[94mStep: 11, PSNR Before: 27.633648, PSNR After: 27.633675, PSNR Change: 0.000027, PSNR Diff: -0.000023 (New Max), Reward: 2.14, 11:32:25\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631357, PSNR After: 27.631369, PSNR Change: 0.000011, PSNR Diff: -0.002329, Reward: 0.92, 11:34:15\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 268          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 10577        |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.455222e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.012        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0627       |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000448    |\n",
      "|    value_loss           | 0.141        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627153, PSNR After: 27.627159, PSNR Change: 0.000006, PSNR Diff: -0.006538, Reward: 0.46, 11:36:27\n",
      "Step: 300, PSNR Before: 27.623951, PSNR After: 27.623949, PSNR Change: -0.000002, PSNR Diff: -0.009748, Reward: -0.15, 11:38:34\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010380 < -0.01 at step 302\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 31, Max consecutive episode failures: 31\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:38:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:38:38.128443\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633726, PSNR Change: 0.000029, PSNR Diff: 0.000029 (New Max), Reward: 2.29, 11:38:39\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633726, PSNR After: 27.633762, PSNR Change: 0.000036, PSNR Diff: 0.000065 (New Max), Reward: 2.90, 11:38:40\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633762, PSNR After: 27.633770, PSNR Change: 0.000008, PSNR Diff: 0.000072 (New Max), Reward: 0.61, 11:38:41\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633770, PSNR After: 27.633797, PSNR Change: 0.000027, PSNR Diff: 0.000099 (New Max), Reward: 2.14, 11:38:43\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630634, PSNR After: 27.630617, PSNR Change: -0.000017, PSNR Diff: -0.003080, Reward: -1.37, 11:40:44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 10908        |\n",
      "|    total_timesteps      | 8448         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.565824e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0191      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0353       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    value_loss           | 0.0817       |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626656, PSNR After: 27.626659, PSNR Change: 0.000004, PSNR Diff: -0.007038, Reward: 0.31, 11:42:58\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010023 < -0.01 at step 271\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 32, Max consecutive episode failures: 32\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:44:29\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:44:29.756410\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633638, PSNR Change: -0.000059, PSNR Diff: -0.000059 (New Max), Reward: -4.73, 11:44:30\u001b[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 269         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 11239       |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.48899e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.0293     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.000484   |\n",
      "|    value_loss           | 0.091       |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 27.629238, PSNR After: 27.629429, PSNR Change: 0.000191, PSNR Diff: -0.004269, Reward: 15.26, 11:46:42\n",
      "Step: 200, PSNR Before: 27.625864, PSNR After: 27.625877, PSNR Change: 0.000013, PSNR Diff: -0.007820, Reward: 1.07, 11:48:49\n",
      "Step: 300, PSNR Before: 27.624676, PSNR After: 27.624586, PSNR Change: -0.000090, PSNR Diff: -0.009111, Reward: -7.17, 11:50:56\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010250 < -0.01 at step 335\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 33, Max consecutive episode failures: 33\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:51:41\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:51:41.741123\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633644, PSNR Change: -0.000053, PSNR Diff: -0.000053 (New Max), Reward: -4.27, 11:51:42\u001b[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 271         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 11570       |\n",
      "|    total_timesteps      | 8960        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.91507e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.00516    |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000501   |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 27.629280, PSNR After: 27.629271, PSNR Change: -0.000010, PSNR Diff: -0.004427, Reward: -0.76, 11:53:55\n",
      "Step: 200, PSNR Before: 27.624996, PSNR After: 27.624985, PSNR Change: -0.000011, PSNR Diff: -0.008713, Reward: -0.92, 11:56:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010084 < -0.01 at step 227\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 34, Max consecutive episode failures: 34\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:56:36\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:56:36.853010\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633636, PSNR Change: -0.000061, PSNR Diff: -0.000061 (New Max), Reward: -4.88, 11:56:38\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633636, PSNR After: 27.633726, PSNR Change: 0.000090, PSNR Diff: 0.000029 (New Max), Reward: 7.17, 11:56:39\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633726, PSNR After: 27.633762, PSNR Change: 0.000036, PSNR Diff: 0.000065 (New Max), Reward: 2.90, 11:56:40\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 11901         |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1858065e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0179        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.031         |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00066      |\n",
      "|    value_loss           | 0.063         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629835, PSNR After: 27.629805, PSNR Change: -0.000031, PSNR Diff: -0.003893, Reward: -2.44, 11:58:50\n",
      "Step: 200, PSNR Before: 27.626228, PSNR After: 27.626188, PSNR Change: -0.000040, PSNR Diff: -0.007509, Reward: -3.20, 12:00:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010073 < -0.01 at step 255\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 35, Max consecutive episode failures: 35\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:02:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:02:08.261542\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633781, PSNR Change: 0.000084, PSNR Diff: 0.000084 (New Max), Reward: 6.71, 12:02:09\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633781, PSNR After: 27.633823, PSNR Change: 0.000042, PSNR Diff: 0.000126 (New Max), Reward: 3.36, 12:02:10\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 12232        |\n",
      "|    total_timesteps      | 9472         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.399358e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.019        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000523    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630777, PSNR After: 27.630486, PSNR Change: -0.000292, PSNR Diff: -0.003212, Reward: -23.35, 12:04:22\n",
      "Step: 200, PSNR Before: 27.626263, PSNR After: 27.626259, PSNR Change: -0.000004, PSNR Diff: -0.007439, Reward: -0.31, 12:06:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010101 < -0.01 at step 259\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 36, Max consecutive episode failures: 36\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:07:46\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:07:47.394767\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633440, PSNR Change: -0.000257, PSNR Diff: -0.000257 (New Max), Reward: -20.60, 12:07:48\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633440, PSNR After: 27.633480, PSNR Change: 0.000040, PSNR Diff: -0.000217 (New Max), Reward: 3.20, 12:07:49\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633480, PSNR After: 27.633484, PSNR Change: 0.000004, PSNR Diff: -0.000214 (New Max), Reward: 0.31, 12:07:51\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633404, PSNR After: 27.633535, PSNR Change: 0.000132, PSNR Diff: -0.000162 (New Max), Reward: 10.53, 12:07:53\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 12567        |\n",
      "|    total_timesteps      | 9728         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.923227e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.035        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000534    |\n",
      "|    value_loss           | 0.0986       |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629971, PSNR After: 27.629612, PSNR Change: -0.000359, PSNR Diff: -0.004086, Reward: -28.69, 12:10:01\n",
      "Step: 200, PSNR Before: 27.626476, PSNR After: 27.626446, PSNR Change: -0.000031, PSNR Diff: -0.007252, Reward: -2.44, 12:12:09\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010056 < -0.01 at step 272\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 37, Max consecutive episode failures: 37\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:13:41\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:13:42.371349\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633690, PSNR Change: -0.000008, PSNR Diff: -0.000008 (New Max), Reward: -0.61, 12:13:43\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633598, PSNR After: 27.633772, PSNR Change: 0.000174, PSNR Diff: 0.000074 (New Max), Reward: 13.89, 12:13:46\u001b[0m\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 269        |\n",
      "|    ep_rew_mean          | -892       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 12900      |\n",
      "|    total_timesteps      | 9984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 7.6578e-07 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -14.6      |\n",
      "|    explained_variance   | 0.0178     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0669     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.000533  |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Step: 100, PSNR Before: 27.632347, PSNR After: 27.632355, PSNR Change: 0.000008, PSNR Diff: -0.001343, Reward: 0.61, 12:15:57\n",
      "Step: 200, PSNR Before: 27.627407, PSNR After: 27.627375, PSNR Change: -0.000032, PSNR Diff: -0.006323, Reward: -2.59, 12:18:04\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 13233         |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1085067e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.015         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0556        |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000664     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.624771, PSNR After: 27.624828, PSNR Change: 0.000057, PSNR Diff: -0.008869, Reward: 4.58, 12:20:18\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010122 < -0.01 at step 329\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 38, Max consecutive episode failures: 38\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:20:55\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:20:56.356121\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633652, PSNR Change: -0.000046, PSNR Diff: -0.000046 (New Max), Reward: -3.66, 12:20:57\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629644, PSNR After: 27.629654, PSNR Change: 0.000010, PSNR Diff: -0.004044, Reward: 0.76, 12:23:03\n",
      "Step: 200, PSNR Before: 27.626110, PSNR After: 27.626459, PSNR Change: 0.000349, PSNR Diff: -0.007238, Reward: 27.92, 12:25:09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 271          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 13564        |\n",
      "|    total_timesteps      | 10496        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.969808e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.00762     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0325       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000621    |\n",
      "|    value_loss           | 0.109        |\n",
      "------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 283\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 39, Max consecutive episode failures: 39\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:27:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:27:03.984741\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633646, PSNR Change: -0.000051, PSNR Diff: -0.000051 (New Max), Reward: -4.12, 12:27:05\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630825, PSNR After: 27.630833, PSNR Change: 0.000008, PSNR Diff: -0.002865, Reward: 0.61, 12:29:10\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 13896         |\n",
      "|    total_timesteps      | 10752         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1820812e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000957      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0416        |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000674     |\n",
      "|    value_loss           | 0.0963        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626955, PSNR After: 27.626869, PSNR Change: -0.000086, PSNR Diff: -0.006828, Reward: -6.87, 12:31:24\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010021 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 40, Max consecutive episode failures: 40\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:33:20\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:33:21.265995\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633713, PSNR Change: 0.000015, PSNR Diff: 0.000015 (New Max), Reward: 1.22, 12:33:22\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629879, PSNR After: 27.629835, PSNR Change: -0.000044, PSNR Diff: -0.003862, Reward: -3.51, 12:35:27\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 14229         |\n",
      "|    total_timesteps      | 11008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2239907e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0115       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0307        |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.000664     |\n",
      "|    value_loss           | 0.0568        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.625216, PSNR After: 27.625237, PSNR Change: 0.000021, PSNR Diff: -0.008461, Reward: 1.68, 12:37:41\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010035 < -0.01 at step 244\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 41, Max consecutive episode failures: 41\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:38:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:38:37.870606\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633665, PSNR Change: -0.000032, PSNR Diff: -0.000032 (New Max), Reward: -2.59, 12:38:39\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633629, PSNR After: 27.633701, PSNR Change: 0.000072, PSNR Diff: 0.000004 (New Max), Reward: 5.80, 12:38:42\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633600, PSNR After: 27.633705, PSNR Change: 0.000105, PSNR Diff: 0.000008 (New Max), Reward: 8.39, 12:38:45\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633705, PSNR After: 27.633722, PSNR Change: 0.000017, PSNR Diff: 0.000025 (New Max), Reward: 1.37, 12:38:46\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630796, PSNR After: 27.630798, PSNR Change: 0.000002, PSNR Diff: -0.002899, Reward: 0.15, 12:40:44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 271         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 14558       |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.80217e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.0178     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.000597   |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Step: 200, PSNR Before: 27.627762, PSNR After: 27.627737, PSNR Change: -0.000025, PSNR Diff: -0.005960, Reward: -1.98, 12:42:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010094 < -0.01 at step 287\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 42, Max consecutive episode failures: 42\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:44:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:44:49.277709\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633648, PSNR Change: -0.000050, PSNR Diff: -0.000050 (New Max), Reward: -3.97, 12:44:50\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633631, PSNR After: 27.633663, PSNR Change: 0.000032, PSNR Diff: -0.000034 (New Max), Reward: 2.59, 12:44:55\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633663, PSNR After: 27.633675, PSNR Change: 0.000011, PSNR Diff: -0.000023 (New Max), Reward: 0.92, 12:44:56\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633675, PSNR After: 27.633690, PSNR Change: 0.000015, PSNR Diff: -0.000008 (New Max), Reward: 1.22, 12:44:58\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630178, PSNR After: 27.630213, PSNR Change: 0.000034, PSNR Diff: -0.003485, Reward: 2.75, 12:46:55\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 14890         |\n",
      "|    total_timesteps      | 11520         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3469253e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00727      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0198        |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.000681     |\n",
      "|    value_loss           | 0.0553        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626574, PSNR After: 27.626448, PSNR Change: -0.000126, PSNR Diff: -0.007250, Reward: -10.07, 12:49:08\n",
      "Step: 300, PSNR Before: 27.624083, PSNR After: 27.624088, PSNR Change: 0.000006, PSNR Diff: -0.009609, Reward: 0.46, 12:51:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010094 < -0.01 at step 309\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 43, Max consecutive episode failures: 43\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:51:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:51:26.478288\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633663, PSNR Change: -0.000034, PSNR Diff: -0.000034 (New Max), Reward: -2.75, 12:51:27\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633663, PSNR After: 27.633734, PSNR Change: 0.000071, PSNR Diff: 0.000036 (New Max), Reward: 5.65, 12:51:28\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 15218         |\n",
      "|    total_timesteps      | 11776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1418015e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00105      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0316        |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000626     |\n",
      "|    value_loss           | 0.0799        |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630823, PSNR After: 27.630447, PSNR Change: -0.000376, PSNR Diff: -0.003250, Reward: -30.06, 12:53:40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 576\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# PPO 학습\u001b[39;00m\n\u001b[1;32m    561\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 576\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    579\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_normalize.py:181\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 413\u001b[0m, in \u001b[0;36mBinaryHologramEnv.step\u001b[0;34m(self, action, lr, z)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# 행동 마스크를 적용\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mflatten()[action] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# 잘못된 행동 시 패널티와 함께 상태 유지\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m#print(f\"Invalid action taken at step {self.steps}, action: {action}\")\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m: mask}\n",
      "Cell \u001b[0;32mIn[2], line 381\u001b[0m, in \u001b[0;36mBinaryHologramEnv.create_action_mask\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    380\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((obs[channel] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (obs[channel] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindices):\n\u001b[1;32m    382\u001b[0m         pixel_idx \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m+\u001b[39m row \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m+\u001b[39m col\n\u001b[1;32m    383\u001b[0m         mask[pixel_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 가능한 행동으로 설정\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 512, 512).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(512)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((512, 512))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 512 or target.shape[-2] < 512:\n",
    "            target = torchvision.transforms.Resize(512)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        T_PSNR_DIFF: PSNR 차이의 임계값.\n",
    "        max_allowed_changes: 한 번에 변경 가능한 최대 픽셀 수 (기본값: 1).\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간: (1, 8, 512, 512)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: 픽셀 하나를 선택하는 인덱스 (512 * 512 * 8)\n",
    "        self.num_pixels = 8 * 512 * 512\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        # 타겟 함수와 데이터 로더 설정\n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # 환경 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  # 추가된 속성\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 데이터 로더에서 첫 배치 설정\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "        # 연속 실패 관련 변수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        데이터셋에서 새로운 이미지를 가져오고 초기 상태를 설정합니다.\n",
    "        - 데이터셋의 다음 이미지를 불러옵니다. \n",
    "        - BinaryNet을 사용해 초기 관찰값을 생성합니다.\n",
    "        - 초기 상태(state)는 관찰값을 이진화한 결과입니다.\n",
    "        - 초기 PSNR과 MSE를 계산하고 출력합니다.\n",
    "        - 실패 시 이전 데이터를 다시 불러옵니다.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): 랜덤 시드 값. Default는 None.\n",
    "            options (dict, optional): 추가 옵션. Default는 None.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.retry_current_target:  # 이전 에피소드에서 실패한 경우\n",
    "            self.consecutive_fail_count += 1\n",
    "        else:\n",
    "            self.consecutive_fail_count = 0  # 성공적인 에피소드로 연속 실패 초기화\n",
    "\n",
    "        self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        # 매 에피소드마다 최대 PSNR 차이 초기화\n",
    "        self.max_psnr_diff = float('-inf')\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            z (float): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 모델로 초기 관찰값 생성\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # 관찰값을 numpy 배열로 변환\n",
    "\n",
    "        # 초기 상태는 이진화된 값으로 설정\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  # 상태를 Torch 텐서로 변환\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # 메타 정보 추가\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # 초기 MSE와 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 값 출력\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        # 시뮬레이션 결과를 별도로 저장\n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 관찰값(초기 모델 출력)을 반환\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        - 관찰값이 0.2 ~ 0.8 범위에 해당하는 픽셀만 변경 가능.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): 관찰값.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 가능한 행동에 대해 1, 불가능한 행동에 대해 0.\n",
    "        \"\"\"\n",
    "        # 모든 픽셀을 고려한 기본 마스크\n",
    "        mask = np.zeros(self.num_pixels, dtype=np.int8)\n",
    "\n",
    "        # (1, 8, 512, 512) -> (8, 512, 512)로 변환\n",
    "        obs = observation.squeeze()\n",
    "\n",
    "        # 조건을 만족하는 위치에 대해 마스크 설정\n",
    "        for channel in range(obs.shape[0]):\n",
    "            indices = np.where((obs[channel] > 0) & (obs[channel] < 1))\n",
    "            for row, col in zip(*indices):\n",
    "                pixel_idx = channel * 512 * 512 + row * 512 + col\n",
    "                mask[pixel_idx] = 1  # 가능한 행동으로 설정\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경의 한 타임스텝을 진행합니다.\n",
    "        - 주어진 행동(action)을 적용하고, 새로운 상태를 계산합니다.\n",
    "        - 보상은 행동 전후 PSNR 변화량(psnr_change)을 기반으로 계산합니다.\n",
    "        - psnr_change가 0보다 작을 경우 잘못된 행동으로 처리됩니다.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): 에이전트가 수행한 행동.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 새로운 관찰값.\n",
    "            float: 보상 값.\n",
    "            bool: 종료 여부.\n",
    "            bool: Truncated 여부.\n",
    "            dict: 추가 정보 (MSE, PSNR, PSNR_DIFF, 행동 마스크 등).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        # 행동 마스크를 적용\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        if mask.flatten()[action] == 0:\n",
    "            # 잘못된 행동 시 패널티와 함께 상태 유지\n",
    "            #print(f\"Invalid action taken at step {self.steps}, action: {action}\")\n",
    "            return self.observation, -10.0, False, False, {\"mask\": mask}\n",
    "\n",
    "        # 행동 전 PSNR 계산\n",
    "        binary_before = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_before = tt.Tensor(binary_before, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_before = tt.simulate(binary_before, z).abs()**2\n",
    "        result_before = torch.mean(sim_before, dim=1, keepdim=True)\n",
    "        psnr_before = tt.relativeLoss(result_before, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 행동을 기반으로 픽셀 좌표 계산\n",
    "        channel = action // (512 * 512)\n",
    "        pixel_index = action % (512 * 512)\n",
    "        row = pixel_index // 512\n",
    "        col = pixel_index % 512\n",
    "\n",
    "        # 선택한 픽셀만 플립\n",
    "        self.state[0, channel, row, col] = 1 - self.state[0, channel, row, col]\n",
    "\n",
    "        # 현재 상태로 새로운 시뮬레이션 수행\n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # PSNR 변화량 계산\n",
    "        psnr_change = psnr_after - psnr_before\n",
    "\n",
    "        # PSNR_CHANGE가 0보다 작을 경우 잘못된 행동으로 처리\n",
    "        #if psnr_change < 0:\n",
    "            #print(f\"Invalid action: PSNR Change {psnr_change:.6f} < 0 at step {self.steps}\")\n",
    "        #    return self.observation, -10.0, False, False, {\"psnr_before\": psnr_before, \"psnr_after\": psnr_after, \"psnr_change\": psnr_change, \"mask\": mask}\n",
    "\n",
    "        # 기존 PSNR_DIFF 계산\n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "\n",
    "        # 보상 계산\n",
    "        reward = psnr_change * 80000  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "        # 실패 조건 확인\n",
    "        if psnr_diff < -0.01:\n",
    "            print(f\"\\033[91mEpisode failed: PSNR Diff {psnr_diff:.6f} < -0.01 at step {self.steps}\\033[0m\")\n",
    "            self.retry_current_target = True  # 실패 시 반복 플래그 활성화\n",
    "            return self.observation, -100.0, True, False, {\"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        # 최고 PSNR_DIFF일 때 출력\n",
    "        if is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time}\\033[0m\"\n",
    "            )\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                  f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        # 성공 종료 조건: PSNR >= T_PSNR 또는 PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  # 성공 에피소드 조건\n",
    "                reward += 100  # 에피소드 성공 시 추가 보상\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "\n",
    "        del binary_before, binary_after, sim_before, sim_after, result_before, result_after\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,  # 학습률 감소\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping 추가\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO 정책 네트워크 구성\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])]  # 더 복잡한 네트워크 구조\n",
    "#)\n",
    "\n",
    "# PPO 모델 초기화\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",  # LSTM 정책 대신 기본 MLP 정책\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "# 학습\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO 학습\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "#from sb3_contrib import RecurrentPPO\n",
    "\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  # 더 복잡한 네트워크 구조\n",
    "#    lstm_hidden_size=128,  # LSTM 크기 유지\n",
    "#    shared_lstm=False  # 별도 LSTM 사용\n",
    "#)\n",
    "\n",
    "#ppo_model = RecurrentPPO(\n",
    "#    \"MlpLstmPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "\n",
    "# 학습\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
