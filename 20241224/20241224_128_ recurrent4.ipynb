{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 128, 128])\n",
      "Using cuda device\n",
      "Initial MSE: 0.002199, Initial PSNR: 21.792362, 05:04:03\n",
      "Logging to ./ppo_with_mask/RecurrentPPO_60\n",
      "첫 번째 스텝에서 초기화 로직 실행\n",
      "Initial MSE: 0.002199, Initial PSNR: 21.792362, 2024-12-24 05:04:03.474972\n",
      "\u001b[91mStep: 1, MSE: 0.002200, PSNR: 21.792250, PSNR Diff: -0.000113 (New Max), Changes: 4, Reward: -0.00, 05:04:03\u001b[0m\n",
      "\u001b[91mStep: 2, MSE: 0.002197, PSNR: 21.796606, PSNR Diff: 0.004244 (New Max), Changes: 5, Reward: 0.03, 05:04:03\u001b[0m\n",
      "Step: 100, MSE: 0.002232, PSNR: 21.728065, PSNR Diff: -0.068541, Max PSNR Diff: 0.004244, Changes: 49, Reward: -0.48, 05:04:03\n",
      "Step: 200, MSE: 0.002232, PSNR: 21.728306, PSNR Diff: -0.068798, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.48, 05:04:04\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 159 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "Step: 300, MSE: 0.002239, PSNR: 21.714947, PSNR Diff: -0.082157, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.58, 05:04:13\n",
      "Step: 400, MSE: 0.002232, PSNR: 21.728582, PSNR Diff: -0.068521, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.48, 05:04:14\n",
      "Step: 500, MSE: 0.002226, PSNR: 21.740711, PSNR Diff: -0.056393, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.39, 05:04:14\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.837398e+21 |\n",
      "|    clip_fraction        | 0.9          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -9.09e+04    |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.38e+10     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 1.68e+14     |\n",
      "|    value_loss           | 0.98         |\n",
      "------------------------------------------\n",
      "Step: 600, MSE: 0.002220, PSNR: 21.752422, PSNR Diff: -0.044682, Max PSNR Diff: 0.004244, Changes: 34, Reward: -0.31, 05:04:24\n",
      "Step: 700, MSE: 0.002234, PSNR: 21.724302, PSNR Diff: -0.072802, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.51, 05:04:25\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 34        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 768       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 164515.28 |\n",
      "|    clip_fraction        | 0.89      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.00684   |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.165    |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 6.19e+03  |\n",
      "|    value_loss           | 0.0534    |\n",
      "---------------------------------------\n",
      "Step: 800, MSE: 0.002246, PSNR: 21.700840, PSNR Diff: -0.096264, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.67, 05:04:34\n",
      "Step: 900, MSE: 0.002232, PSNR: 21.729221, PSNR Diff: -0.067883, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.48, 05:04:35\n",
      "Step: 1000, MSE: 0.002225, PSNR: 21.741642, PSNR Diff: -0.055462, Max PSNR Diff: 0.004244, Changes: 49, Reward: -0.39, 05:04:35\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 31        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 32        |\n",
      "|    total_timesteps      | 1024      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 19603.977 |\n",
      "|    clip_fraction        | 0.866     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.118     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 0.0961    |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | 4.77e+04  |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Step: 1100, MSE: 0.002226, PSNR: 21.739510, PSNR Diff: -0.057594, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.40, 05:04:45\n",
      "Step: 1200, MSE: 0.002231, PSNR: 21.731178, PSNR Diff: -0.065926, Max PSNR Diff: 0.004244, Changes: 37, Reward: -0.46, 05:04:45\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 29        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 42        |\n",
      "|    total_timesteps      | 1280      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 116.98239 |\n",
      "|    clip_fraction        | 0.882     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.171     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.147    |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 195       |\n",
      "|    value_loss           | 0.0456    |\n",
      "---------------------------------------\n",
      "Step: 1300, MSE: 0.002230, PSNR: 21.733038, PSNR Diff: -0.064066, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.45, 05:04:55\n",
      "Step: 1400, MSE: 0.002220, PSNR: 21.752317, PSNR Diff: -0.044786, Max PSNR Diff: 0.004244, Changes: 45, Reward: -0.31, 05:04:55\n",
      "Step: 1500, MSE: 0.002230, PSNR: 21.732296, PSNR Diff: -0.064808, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.45, 05:04:56\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 28        |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 53        |\n",
      "|    total_timesteps      | 1536      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 74.96236  |\n",
      "|    clip_fraction        | 0.87      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0424    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.0258   |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | 30.8      |\n",
      "|    value_loss           | 0.0593    |\n",
      "---------------------------------------\n",
      "Step: 1600, MSE: 0.002231, PSNR: 21.730698, PSNR Diff: -0.066406, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.46, 05:05:05\n",
      "Step: 1700, MSE: 0.002220, PSNR: 21.752184, PSNR Diff: -0.044920, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.31, 05:05:05\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 28        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 63        |\n",
      "|    total_timesteps      | 1792      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 127.85269 |\n",
      "|    clip_fraction        | 0.872     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.187     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 6.1       |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.319     |\n",
      "|    value_loss           | 0.097     |\n",
      "---------------------------------------\n",
      "Step: 1800, MSE: 0.002229, PSNR: 21.734699, PSNR Diff: -0.062405, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.44, 05:05:15\n",
      "Step: 1900, MSE: 0.002231, PSNR: 21.730976, PSNR Diff: -0.066128, Max PSNR Diff: 0.004244, Changes: 43, Reward: -0.46, 05:05:15\n",
      "Step: 2000, MSE: 0.002223, PSNR: 21.745354, PSNR Diff: -0.051750, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.36, 05:05:16\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 27        |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 73        |\n",
      "|    total_timesteps      | 2048      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 5.838566  |\n",
      "|    clip_fraction        | 0.854     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.156     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.157    |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | 1.25      |\n",
      "|    value_loss           | 0.0542    |\n",
      "---------------------------------------\n",
      "Step: 2100, MSE: 0.002230, PSNR: 21.732882, PSNR Diff: -0.064222, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.45, 05:05:25\n",
      "Step: 2200, MSE: 0.002229, PSNR: 21.734943, PSNR Diff: -0.062160, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.44, 05:05:26\n",
      "Step: 2300, MSE: 0.002227, PSNR: 21.738205, PSNR Diff: -0.058899, Max PSNR Diff: 0.004244, Changes: 43, Reward: -0.41, 05:05:26\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 2304       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80220044 |\n",
      "|    clip_fraction        | 0.831      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.132      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.152     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.0596     |\n",
      "----------------------------------------\n",
      "Step: 2400, MSE: 0.002212, PSNR: 21.767849, PSNR Diff: -0.029255, Max PSNR Diff: 0.004244, Changes: 33, Reward: -0.20, 05:05:35\n",
      "Step: 2500, MSE: 0.002219, PSNR: 21.753334, PSNR Diff: -0.043770, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.31, 05:05:36\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 93         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45579183 |\n",
      "|    clip_fraction        | 0.816      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 0.073      |\n",
      "----------------------------------------\n",
      "Step: 2600, MSE: 0.002230, PSNR: 21.731590, PSNR Diff: -0.065514, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.46, 05:05:46\n",
      "Step: 2700, MSE: 0.002255, PSNR: 21.683575, PSNR Diff: -0.113529, Max PSNR Diff: 0.004244, Changes: 47, Reward: -0.79, 05:05:46\n",
      "Step: 2800, MSE: 0.002225, PSNR: 21.741844, PSNR Diff: -0.055260, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.39, 05:05:46\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 2816       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10428184 |\n",
      "|    clip_fraction        | 0.785      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0548     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.123     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.106     |\n",
      "|    value_loss           | 0.0993     |\n",
      "----------------------------------------\n",
      "Step: 2900, MSE: 0.002232, PSNR: 21.729336, PSNR Diff: -0.067768, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.47, 05:05:56\n",
      "Step: 3000, MSE: 0.002241, PSNR: 21.710533, PSNR Diff: -0.086571, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.61, 05:05:57\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 114        |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15263367 |\n",
      "|    clip_fraction        | 0.788      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0105     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.15      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.118     |\n",
      "|    value_loss           | 0.0699     |\n",
      "----------------------------------------\n",
      "Step: 3100, MSE: 0.002249, PSNR: 21.695860, PSNR Diff: -0.101244, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.71, 05:06:06\n",
      "Step: 3200, MSE: 0.002219, PSNR: 21.754675, PSNR Diff: -0.042429, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.30, 05:06:06\n",
      "Step: 3300, MSE: 0.002208, PSNR: 21.775272, PSNR Diff: -0.021832, Max PSNR Diff: 0.004244, Changes: 34, Reward: -0.15, 05:06:07\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 26        |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 124       |\n",
      "|    total_timesteps      | 3328      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1102698 |\n",
      "|    clip_fraction        | 0.72      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0105    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0906   |\n",
      "|    value_loss           | 0.0979    |\n",
      "---------------------------------------\n",
      "Step: 3400, MSE: 0.002269, PSNR: 21.657351, PSNR Diff: -0.139753, Max PSNR Diff: 0.004244, Changes: 51, Reward: -0.98, 05:06:16\n",
      "Step: 3500, MSE: 0.002224, PSNR: 21.743929, PSNR Diff: -0.053175, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.37, 05:06:17\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17221808 |\n",
      "|    clip_fraction        | 0.75       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.154     |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.121     |\n",
      "|    value_loss           | 0.0499     |\n",
      "----------------------------------------\n",
      "Step: 3600, MSE: 0.002212, PSNR: 21.767122, PSNR Diff: -0.029982, Max PSNR Diff: 0.004244, Changes: 34, Reward: -0.21, 05:06:26\n",
      "Step: 3700, MSE: 0.002211, PSNR: 21.769440, PSNR Diff: -0.027664, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.19, 05:06:27\n",
      "Step: 3800, MSE: 0.002259, PSNR: 21.676056, PSNR Diff: -0.121048, Max PSNR Diff: 0.004244, Changes: 47, Reward: -0.85, 05:06:27\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 3840       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20866011 |\n",
      "|    clip_fraction        | 0.773      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.116     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.1       |\n",
      "|    value_loss           | 0.0827     |\n",
      "----------------------------------------\n",
      "Step: 3900, MSE: 0.002222, PSNR: 21.747551, PSNR Diff: -0.049553, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.35, 05:06:36\n",
      "Step: 4000, MSE: 0.002238, PSNR: 21.717768, PSNR Diff: -0.079336, Max PSNR Diff: 0.004244, Changes: 46, Reward: -0.56, 05:06:37\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048402872 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.0815     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.107      |\n",
      "|    value_loss           | 0.0687      |\n",
      "-----------------------------------------\n",
      "Step: 4100, MSE: 0.002238, PSNR: 21.716902, PSNR Diff: -0.080202, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.56, 05:06:46\n",
      "Step: 4200, MSE: 0.002230, PSNR: 21.732262, PSNR Diff: -0.064842, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.45, 05:06:47\n",
      "Step: 4300, MSE: 0.002218, PSNR: 21.756691, PSNR Diff: -0.040413, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.28, 05:06:47\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 4352        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055800956 |\n",
      "|    clip_fraction        | 0.732       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.0631      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.135      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.119      |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n",
      "Step: 4400, MSE: 0.002234, PSNR: 21.723871, PSNR Diff: -0.073233, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.51, 05:06:57\n",
      "Step: 4500, MSE: 0.002231, PSNR: 21.731113, PSNR Diff: -0.065990, Max PSNR Diff: 0.004244, Changes: 45, Reward: -0.46, 05:06:57\n",
      "Step: 4600, MSE: 0.002216, PSNR: 21.758942, PSNR Diff: -0.038162, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.27, 05:06:58\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 175        |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20779765 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0844     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.137     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.122     |\n",
      "|    value_loss           | 0.0527     |\n",
      "----------------------------------------\n",
      "Step: 4700, MSE: 0.002221, PSNR: 21.749626, PSNR Diff: -0.047478, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.33, 05:07:07\n",
      "Step: 4800, MSE: 0.002230, PSNR: 21.731627, PSNR Diff: -0.065477, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.46, 05:07:08\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 26        |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 185       |\n",
      "|    total_timesteps      | 4864      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.290073  |\n",
      "|    clip_fraction        | 0.688     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.145    |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.133    |\n",
      "|    value_loss           | 0.0464    |\n",
      "---------------------------------------\n",
      "Step: 4900, MSE: 0.002237, PSNR: 21.719225, PSNR Diff: -0.077879, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.55, 05:07:17\n",
      "Step: 5000, MSE: 0.002226, PSNR: 21.739689, PSNR Diff: -0.057415, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.40, 05:07:18\n",
      "Step: 5100, MSE: 0.002241, PSNR: 21.711645, PSNR Diff: -0.085459, Max PSNR Diff: 0.004244, Changes: 43, Reward: -0.60, 05:07:18\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20956746 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0835     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.118     |\n",
      "|    value_loss           | 0.0623     |\n",
      "----------------------------------------\n",
      "Step: 5200, MSE: 0.002241, PSNR: 21.711426, PSNR Diff: -0.085678, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.60, 05:07:28\n",
      "Step: 5300, MSE: 0.002223, PSNR: 21.745438, PSNR Diff: -0.051666, Max PSNR Diff: 0.004244, Changes: 36, Reward: -0.36, 05:07:28\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 205        |\n",
      "|    total_timesteps      | 5376       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17443663 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00237    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.116     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.106     |\n",
      "|    value_loss           | 0.0761     |\n",
      "----------------------------------------\n",
      "Step: 5400, MSE: 0.002217, PSNR: 21.757969, PSNR Diff: -0.039135, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.27, 05:07:38\n",
      "Step: 5500, MSE: 0.002218, PSNR: 21.755459, PSNR Diff: -0.041645, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.29, 05:07:38\n",
      "Step: 5600, MSE: 0.002209, PSNR: 21.773149, PSNR Diff: -0.023954, Max PSNR Diff: 0.004244, Changes: 31, Reward: -0.17, 05:07:38\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 216        |\n",
      "|    total_timesteps      | 5632       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08083343 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0241    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.141     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.115     |\n",
      "|    value_loss           | 0.0351     |\n",
      "----------------------------------------\n",
      "Step: 5700, MSE: 0.002230, PSNR: 21.731823, PSNR Diff: -0.065281, Max PSNR Diff: 0.004244, Changes: 46, Reward: -0.46, 05:07:48\n",
      "Step: 5800, MSE: 0.002245, PSNR: 21.702782, PSNR Diff: -0.094322, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.66, 05:07:48\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 226        |\n",
      "|    total_timesteps      | 5888       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06751801 |\n",
      "|    clip_fraction        | 0.724      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0346     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.114     |\n",
      "|    value_loss           | 0.0157     |\n",
      "----------------------------------------\n",
      "Step: 5900, MSE: 0.002229, PSNR: 21.734507, PSNR Diff: -0.062597, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.44, 05:07:58\n",
      "Step: 6000, MSE: 0.002225, PSNR: 21.743195, PSNR Diff: -0.053909, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.38, 05:07:58\n",
      "Step: 6100, MSE: 0.002235, PSNR: 21.723402, PSNR Diff: -0.073702, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.52, 05:07:59\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 236        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08284186 |\n",
      "|    clip_fraction        | 0.717      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0203    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.122     |\n",
      "|    value_loss           | 0.0454     |\n",
      "----------------------------------------\n",
      "Step: 6200, MSE: 0.002223, PSNR: 21.745258, PSNR Diff: -0.051846, Max PSNR Diff: 0.004244, Changes: 45, Reward: -0.36, 05:08:08\n",
      "Step: 6300, MSE: 0.002234, PSNR: 21.724369, PSNR Diff: -0.072735, Max PSNR Diff: 0.004244, Changes: 43, Reward: -0.51, 05:08:08\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 6400       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32414442 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.041      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.161     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.125     |\n",
      "|    value_loss           | 0.027      |\n",
      "----------------------------------------\n",
      "Step: 6400, MSE: 0.002219, PSNR: 21.753523, PSNR Diff: -0.043581, Max PSNR Diff: 0.004244, Changes: 34, Reward: -0.31, 05:08:18\n",
      "Step: 6500, MSE: 0.002232, PSNR: 21.728947, PSNR Diff: -0.068157, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.48, 05:08:19\n",
      "Step: 6600, MSE: 0.002249, PSNR: 21.696217, PSNR Diff: -0.100887, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.71, 05:08:19\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 256        |\n",
      "|    total_timesteps      | 6656       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17500742 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0269     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.16      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.122     |\n",
      "|    value_loss           | 0.0281     |\n",
      "----------------------------------------\n",
      "Step: 6700, MSE: 0.002253, PSNR: 21.687721, PSNR Diff: -0.109383, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.77, 05:08:28\n",
      "Step: 6800, MSE: 0.002223, PSNR: 21.745670, PSNR Diff: -0.051434, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.36, 05:08:29\n",
      "Step: 6900, MSE: 0.002239, PSNR: 21.715542, PSNR Diff: -0.081562, Max PSNR Diff: 0.004244, Changes: 50, Reward: -0.57, 05:08:29\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 266        |\n",
      "|    total_timesteps      | 6912       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10902892 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0147    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.104     |\n",
      "|    value_loss           | 0.0417     |\n",
      "----------------------------------------\n",
      "Step: 7000, MSE: 0.002234, PSNR: 21.724091, PSNR Diff: -0.073013, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.51, 05:08:39\n",
      "Step: 7100, MSE: 0.002218, PSNR: 21.755264, PSNR Diff: -0.041840, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.29, 05:08:39\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 276        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19476454 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.02       |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.158     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.125     |\n",
      "|    value_loss           | 0.023      |\n",
      "----------------------------------------\n",
      "Step: 7200, MSE: 0.002219, PSNR: 21.753218, PSNR Diff: -0.043886, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.31, 05:08:48\n",
      "Step: 7300, MSE: 0.002223, PSNR: 21.745564, PSNR Diff: -0.051540, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.36, 05:08:49\n",
      "Step: 7400, MSE: 0.002227, PSNR: 21.738060, PSNR Diff: -0.059044, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.41, 05:08:49\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 29        |\n",
      "|    time_elapsed         | 286       |\n",
      "|    total_timesteps      | 7424      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4086246 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | -0.0171   |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.155    |\n",
      "|    n_updates            | 280       |\n",
      "|    policy_gradient_loss | -0.114    |\n",
      "|    value_loss           | 0.0423    |\n",
      "---------------------------------------\n",
      "Step: 7500, MSE: 0.002235, PSNR: 21.723612, PSNR Diff: -0.073492, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.51, 05:08:59\n",
      "Step: 7600, MSE: 0.002239, PSNR: 21.715155, PSNR Diff: -0.081949, Max PSNR Diff: 0.004244, Changes: 50, Reward: -0.57, 05:08:59\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 296        |\n",
      "|    total_timesteps      | 7680       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10694903 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0166    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.114     |\n",
      "|    value_loss           | 0.03       |\n",
      "----------------------------------------\n",
      "Step: 7700, MSE: 0.002241, PSNR: 21.710381, PSNR Diff: -0.086723, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.61, 05:09:08\n",
      "Step: 7800, MSE: 0.002224, PSNR: 21.743385, PSNR Diff: -0.053719, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.38, 05:09:09\n",
      "Step: 7900, MSE: 0.002228, PSNR: 21.736727, PSNR Diff: -0.060377, Max PSNR Diff: 0.004244, Changes: 47, Reward: -0.42, 05:09:09\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 306        |\n",
      "|    total_timesteps      | 7936       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17918561 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0144     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.151     |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.122     |\n",
      "|    value_loss           | 0.0121     |\n",
      "----------------------------------------\n",
      "Step: 8000, MSE: 0.002231, PSNR: 21.730177, PSNR Diff: -0.066927, Max PSNR Diff: 0.004244, Changes: 36, Reward: -0.47, 05:09:18\n",
      "Step: 8100, MSE: 0.002216, PSNR: 21.759716, PSNR Diff: -0.037388, Max PSNR Diff: 0.004244, Changes: 30, Reward: -0.26, 05:09:19\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 316        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13090175 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0326     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.145     |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.111     |\n",
      "|    value_loss           | 0.0446     |\n",
      "----------------------------------------\n",
      "Step: 8200, MSE: 0.002221, PSNR: 21.750332, PSNR Diff: -0.046772, Max PSNR Diff: 0.004244, Changes: 39, Reward: -0.33, 05:09:28\n",
      "Step: 8300, MSE: 0.002226, PSNR: 21.739487, PSNR Diff: -0.057617, Max PSNR Diff: 0.004244, Changes: 52, Reward: -0.40, 05:09:29\n",
      "Step: 8400, MSE: 0.002242, PSNR: 21.708496, PSNR Diff: -0.088608, Max PSNR Diff: 0.004244, Changes: 46, Reward: -0.62, 05:09:29\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 33        |\n",
      "|    time_elapsed         | 326       |\n",
      "|    total_timesteps      | 8448      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1726799 |\n",
      "|    clip_fraction        | 0.634     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | -0.00622  |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.165    |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.124    |\n",
      "|    value_loss           | 0.0222    |\n",
      "---------------------------------------\n",
      "Step: 8500, MSE: 0.002236, PSNR: 21.719992, PSNR Diff: -0.077112, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.54, 05:09:39\n",
      "Step: 8600, MSE: 0.002227, PSNR: 21.738159, PSNR Diff: -0.058945, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.41, 05:09:39\n",
      "Step: 8700, MSE: 0.002231, PSNR: 21.730463, PSNR Diff: -0.066641, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.47, 05:09:39\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 336        |\n",
      "|    total_timesteps      | 8704       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11336893 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00868    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.141     |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.101     |\n",
      "|    value_loss           | 0.0336     |\n",
      "----------------------------------------\n",
      "Step: 8800, MSE: 0.002244, PSNR: 21.706182, PSNR Diff: -0.090921, Max PSNR Diff: 0.004244, Changes: 37, Reward: -0.64, 05:09:49\n",
      "Step: 8900, MSE: 0.002238, PSNR: 21.716496, PSNR Diff: -0.080608, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.56, 05:09:49\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 8960        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024221983 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.0204      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.116      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0889     |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "Step: 9000, MSE: 0.002237, PSNR: 21.719555, PSNR Diff: -0.077549, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.54, 05:09:59\n",
      "Step: 9100, MSE: 0.002224, PSNR: 21.744440, PSNR Diff: -0.052664, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.37, 05:09:59\n",
      "Step: 9200, MSE: 0.002231, PSNR: 21.730919, PSNR Diff: -0.066185, Max PSNR Diff: 0.004244, Changes: 45, Reward: -0.46, 05:10:00\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 357        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13494186 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00363    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.166     |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.12      |\n",
      "|    value_loss           | 0.0406     |\n",
      "----------------------------------------\n",
      "Step: 9300, MSE: 0.002228, PSNR: 21.735630, PSNR Diff: -0.061474, Max PSNR Diff: 0.004244, Changes: 42, Reward: -0.43, 05:10:09\n",
      "Step: 9400, MSE: 0.002234, PSNR: 21.725224, PSNR Diff: -0.071880, Max PSNR Diff: 0.004244, Changes: 44, Reward: -0.50, 05:10:09\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 367        |\n",
      "|    total_timesteps      | 9472       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09363285 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0136     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.151     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.108     |\n",
      "|    value_loss           | 0.049      |\n",
      "----------------------------------------\n",
      "Step: 9500, MSE: 0.002237, PSNR: 21.718391, PSNR Diff: -0.078712, Max PSNR Diff: 0.004244, Changes: 41, Reward: -0.55, 05:10:19\n",
      "Step: 9600, MSE: 0.002236, PSNR: 21.720484, PSNR Diff: -0.076620, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.54, 05:10:19\n",
      "Step: 9700, MSE: 0.002222, PSNR: 21.747496, PSNR Diff: -0.049608, Max PSNR Diff: 0.004244, Changes: 35, Reward: -0.35, 05:10:20\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 38        |\n",
      "|    time_elapsed         | 377       |\n",
      "|    total_timesteps      | 9728      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0930909 |\n",
      "|    clip_fraction        | 0.579     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | -0.0165   |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 370       |\n",
      "|    policy_gradient_loss | -0.1      |\n",
      "|    value_loss           | 0.0191    |\n",
      "---------------------------------------\n",
      "Step: 9800, MSE: 0.002235, PSNR: 21.722784, PSNR Diff: -0.074320, Max PSNR Diff: 0.004244, Changes: 40, Reward: -0.52, 05:10:29\n",
      "Step: 9900, MSE: 0.002236, PSNR: 21.719906, PSNR Diff: -0.077198, Max PSNR Diff: 0.004244, Changes: 38, Reward: -0.54, 05:10:29\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 387        |\n",
      "|    total_timesteps      | 9984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26475257 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00486    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.164     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.124     |\n",
      "|    value_loss           | 0.0319     |\n",
      "----------------------------------------\n",
      "Step: 10000, MSE: 0.002232, PSNR: 21.728937, PSNR Diff: -0.068167, Max PSNR Diff: 0.004244, Changes: 37, Reward: -0.48, 05:10:39\n",
      "Episode ended. Max PSNR_DIFF: 0.004244\n",
      "Initial MSE: 0.007627, Initial PSNR: 21.176189, 05:10:39\n",
      "첫 번째 스텝에서 초기화 로직 실행\n",
      "Initial MSE: 0.007627, Initial PSNR: 21.176189, 2024-12-24 05:10:39.425955\n",
      "\u001b[91mStep: 1, MSE: 0.007631, PSNR: 21.174269, PSNR Diff: -0.001921 (New Max), Changes: 5, Reward: -0.01, 05:10:39\u001b[0m\n",
      "\u001b[91mStep: 6, MSE: 0.007629, PSNR: 21.175432, PSNR Diff: -0.000757 (New Max), Changes: 6, Reward: -0.01, 05:10:39\u001b[0m\n",
      "\u001b[91mStep: 7, MSE: 0.007628, PSNR: 21.176113, PSNR Diff: -0.000076 (New Max), Changes: 8, Reward: -0.00, 05:10:39\u001b[0m\n",
      "\u001b[91mStep: 9, MSE: 0.007627, PSNR: 21.176395, PSNR Diff: 0.000206 (New Max), Changes: 4, Reward: 0.00, 05:10:39\u001b[0m\n",
      "Step: 100, MSE: 0.007651, PSNR: 21.162918, PSNR Diff: -0.013477, Max PSNR Diff: 0.000206, Changes: 31, Reward: -0.09, 05:10:39\n",
      "Step: 200, MSE: 0.007684, PSNR: 21.144236, PSNR Diff: -0.032160, Max PSNR Diff: 0.000206, Changes: 33, Reward: -0.23, 05:10:40\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+04      |\n",
      "|    ep_rew_mean          | -4.56e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 397        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14503966 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.00595   |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.141     |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.115     |\n",
      "|    value_loss           | 0.0282     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (64, 131072)) of distribution Bernoulli(logits: torch.Size([64, 131072])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 583\u001b[0m\n\u001b[1;32m    565\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m RecurrentPPO(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpLstmPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    567\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m    586\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:450\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[1;32m    443\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:345\u001b[0m, in \u001b[0;36mRecurrentPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Convert mask from float to bool\u001b[39;00m\n\u001b[1;32m    343\u001b[0m mask \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[0;32m--> 345\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sb3_contrib/common/recurrent/policies.py:342\u001b[0m, in \u001b[0;36mRecurrentActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions, lstm_states, episode_starts)\u001b[0m\n\u001b[1;32m    339\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(latent_pi)\n\u001b[1;32m    340\u001b[0m latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(latent_vf)\n\u001b[0;32m--> 342\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    344\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:703\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, BernoulliDistribution):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits (before rounding to get the binary actions)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, StateDependentNoiseDistribution):\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std, latent_pi)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/distributions.py:395\u001b[0m, in \u001b[0;36mBernoulliDistribution.proba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfBernoulliDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfBernoulliDistribution:\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mBernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/bernoulli.py:59\u001b[0m, in \u001b[0;36mBernoulli.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/distribution.py:70\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 70\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (64, 131072)) of distribution Bernoulli(logits: torch.Size([64, 131072])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 128, 128).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(128)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((128, 128))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 128 or target.shape[-2] < 128:\n",
    "            target = torchvision.transforms.Resize(128)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, max_allowed_changes=10):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        max_allowed_changes: 한 번에 조작할 수 있는 최대 픽셀 수.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 128, 128)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 128, 128), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.MultiBinary(1 * 8 * 128 * 128)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.max_allowed_changes = max_allowed_changes  # 한 번에 조작할 수 있는 최대 픽셀 수\n",
    "\n",
    "        # PSNR 개선 여부 플래그\n",
    "        self.psnr_improved = False  # PSNR이 처음으로 개선되기 전까지 True로 전환되지 않음\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        데이터셋에서 새로운 이미지를 가져오고 초기 상태를 설정합니다.\n",
    "        - 데이터셋의 다음 이미지를 불러옵니다. \n",
    "        - BinaryNet을 사용해 초기 관찰값을 생성합니다.\n",
    "        - 초기 상태(state)는 관찰값을 이진화한 결과입니다.\n",
    "        - 초기 PSNR과 MSE를 계산하고 출력합니다.\n",
    "        - 실패 시 이전 데이터를 다시 불러옵니다.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): 랜덤 시드 값. Default는 None.\n",
    "            options (dict, optional): 추가 옵션. Default는 None.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            z (float): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 모델로 초기 관찰값 생성\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # 관찰값을 numpy 배열로 변환\n",
    "\n",
    "        # 초기 상태는 이진화된 값으로 설정\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  # 상태를 Torch 텐서로 변환\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # 메타 정보 추가\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # 초기 MSE와 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 값 출력\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        # 시뮬레이션 결과를 별도로 저장\n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 관찰값(초기 모델 출력)을 반환\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        - 관찰값이 0~0.2인 경우 행동 0으로 고정.\n",
    "        - 관찰값이 0.8~1인 경우 행동 1로 고정.\n",
    "        - 최대 변경 가능 픽셀 수 제한.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): 관찰값.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 행동 마스크.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "\n",
    "        # 허용된 변경 수를 강제 적용\n",
    "        allowed_indices = np.where(mask.flatten() == 1)[0]\n",
    "        if len(allowed_indices) > self.max_allowed_changes:\n",
    "            # 초과 변경을 방지하도록 고정된 수의 픽셀만 선택 가능\n",
    "            selected_indices = np.random.choice(allowed_indices, self.max_allowed_changes, replace=False)\n",
    "            mask = np.zeros_like(mask.flatten())\n",
    "            mask[selected_indices] = 1\n",
    "            mask = mask.reshape(observation.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경의 한 타임스텝을 진행합니다.\n",
    "        PSNR이 낮아지는 경우 해당 행동은 적용되지 않습니다.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): 에이전트가 수행한 행동.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 새로운 관찰값.\n",
    "            float: 보상 값.\n",
    "            bool: 종료 여부.\n",
    "            bool: Truncated 여부.\n",
    "            dict: 추가 정보 (MSE, PSNR, 행동 마스크 등).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"첫 번째 스텝에서 초기화 로직 실행\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        action = np.reshape(action, (1, 8, 128, 128)).astype(np.int8)\n",
    "\n",
    "        # 행동에 마스크 강제 적용\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        masked_action = action * mask\n",
    "\n",
    "        # 허용된 최대 변경 수 초과 확인\n",
    "        num_changes = np.sum(masked_action)\n",
    "        reward = 0\n",
    "\n",
    "        # PSNR이 한 번도 개선되지 않았으면 초과 변경에 대해 패널티 적용하지 않음\n",
    "        if self.psnr_improved and num_changes * 5 > self.max_allowed_changes:\n",
    "            reward -= 0\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, masked_action).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # PSNR 차이 계산\n",
    "        psnr_diff = psnr - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "\n",
    "        # PSNR을 비교하여 행동 적용 여부 결정\n",
    "        if psnr >= self.initial_psnr:\n",
    "            # PSNR이 개선되거나 유지되는 경우에만 상태 업데이트\n",
    "            self.state = new_state\n",
    "            self.observation = result.detach().cpu().numpy()\n",
    "            reward += psnr_diff * 7  # PSNR 개선에 따른 보상\n",
    "            self.initial_psnr = psnr  # 기준 PSNR 업데이트\n",
    "\n",
    "            # PSNR이 처음으로 개선되었음을 기록\n",
    "            if not self.psnr_improved:\n",
    "                self.psnr_improved = True\n",
    "        else:\n",
    "            # PSNR이 낮아지면 보상도 감소 (패널티는 주지 않음)\n",
    "            reward += psnr_diff * 7\n",
    "\n",
    "        # PSNR이 한 번도 개선되지 않은 경우 보상에 10배를 곱함\n",
    "        if not self.psnr_improved:\n",
    "            reward *= 1\n",
    "\n",
    "        # 최고 PSNR_DIFF일 때 출력\n",
    "        if is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[91mStep: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, \"\n",
    "                f\"PSNR Diff: {psnr_diff:.6f} (New Max), Changes: {num_changes}, \"\n",
    "                f\"Reward: {reward:.2f}, {current_time}\\033[0m\"\n",
    "            )\n",
    "\n",
    "        # 추가 정보 출력 (100 스텝마다)\n",
    "        if self.steps % 100 == 0 and not is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, \"\n",
    "                  f\"PSNR Diff: {psnr_diff:.6f}, Max PSNR Diff: {self.max_psnr_diff:.6f}, \"\n",
    "                  f\"Changes: {num_changes}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 에피소드 종료 시 최고 PSNR_DIFF 출력\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode ended. Max PSNR_DIFF: {self.max_psnr_diff:.6f}\")\n",
    "            self.max_psnr_diff = float('-inf')  # 초기화\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"psnr_diff\": psnr_diff, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-22 22:14:06.172882_pre_reinforce_8_0.002/2024-12-22 22:14:06.172882_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  # 더 복잡한 네트워크 구조\n",
    "    lstm_hidden_size=128,  # LSTM 크기 유지\n",
    "    shared_lstm=False  # 별도 LSTM 사용\n",
    ")\n",
    "\n",
    "ppo_model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "    tensorboard_log=\"./ppo_with_mask/\",\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# 학습\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
