{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:24:16\n",
      "Logging to ./ppo_with_mask/PPO_121\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:24:17.952942\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277321, PSNR Change: -0.000010, PSNR Diff: -0.000010 (New Max), Reward: -0.76, 09:24:19\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273619, PSNR After: 25.273569, PSNR Change: -0.000050, PSNR Diff: -0.003761, Reward: -3.97, 09:26:23\n",
      "Step: 200, PSNR Before: 25.269659, PSNR After: 25.269627, PSNR Change: -0.000032, PSNR Diff: -0.007704, Reward: -2.59, 09:28:29\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 0   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 322 |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 297\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 1, Max consecutive episode failures: 1\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:30:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:30:38.231776\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277309, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 09:30:39\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277309, PSNR After: 25.277313, PSNR Change: 0.000004, PSNR Diff: -0.000017 (New Max), Reward: 0.31, 09:30:40\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277313, PSNR After: 25.277346, PSNR Change: 0.000032, PSNR Diff: 0.000015 (New Max), Reward: 2.59, 09:30:42\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 25.277346, PSNR After: 25.277355, PSNR Change: 0.000010, PSNR Diff: 0.000025 (New Max), Reward: 0.76, 09:30:43\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274967, PSNR After: 25.274782, PSNR Change: -0.000185, PSNR Diff: -0.002548, Reward: -14.80, 09:32:43\n",
      "Step: 200, PSNR Before: 25.271473, PSNR After: 25.271366, PSNR Change: -0.000107, PSNR Diff: -0.005964, Reward: -8.54, 09:34:49\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 298          |\n",
      "|    ep_rew_mean          | -899         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 649          |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.165325e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0037       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -6.45e-05    |\n",
      "|    value_loss           | 0.528        |\n",
      "------------------------------------------\n",
      "Step: 300, PSNR Before: 25.269047, PSNR After: 25.268858, PSNR Change: -0.000189, PSNR Diff: -0.008472, Reward: -15.11, 09:37:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010000 < -0.01 at step 349\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 2, Max consecutive episode failures: 2\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:38:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:38:04.418505\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277235, PSNR Change: -0.000095, PSNR Diff: -0.000095 (New Max), Reward: -7.63, 09:38:05\u001b[0m\n",
      "Step: 100, PSNR Before: 25.272736, PSNR After: 25.272690, PSNR Change: -0.000046, PSNR Diff: -0.004641, Reward: -3.66, 09:40:10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 324          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 978          |\n",
      "|    total_timesteps      | 768          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.934838e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.106       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0249       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    value_loss           | 0.104        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.268364, PSNR After: 25.268305, PSNR Change: -0.000059, PSNR Diff: -0.009026, Reward: -4.73, 09:42:22\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010168 < -0.01 at step 250\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 3, Max consecutive episode failures: 3\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:43:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:43:26.079795\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277117, PSNR Change: -0.000214, PSNR Diff: -0.000214 (New Max), Reward: -17.09, 09:43:27\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273014, PSNR After: 25.272940, PSNR Change: -0.000074, PSNR Diff: -0.004391, Reward: -5.95, 09:45:31\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1305         |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.961636e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0676      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0469       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000137    |\n",
      "|    value_loss           | 0.194        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.269812, PSNR After: 25.269735, PSNR Change: -0.000076, PSNR Diff: -0.007595, Reward: -6.10, 09:47:42\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010115 < -0.01 at step 258\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 4, Max consecutive episode failures: 4\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:48:55\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:48:56.176332\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277273, PSNR Change: -0.000057, PSNR Diff: -0.000057 (New Max), Reward: -4.58, 09:48:57\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277273, PSNR After: 25.277311, PSNR Change: 0.000038, PSNR Diff: -0.000019 (New Max), Reward: 3.05, 09:48:58\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277311, PSNR After: 25.277355, PSNR Change: 0.000044, PSNR Diff: 0.000025 (New Max), Reward: 3.51, 09:48:59\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273533, PSNR After: 25.273573, PSNR Change: 0.000040, PSNR Diff: -0.003757, Reward: 3.20, 09:51:00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 290          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1629         |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.032657e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0743       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0847       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000145    |\n",
      "|    value_loss           | 0.215        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.270880, PSNR After: 25.270826, PSNR Change: -0.000053, PSNR Diff: -0.006504, Reward: -4.27, 09:53:12\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010056 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 5, Max consecutive episode failures: 5\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 09:55:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 09:55:08.080318\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277281, PSNR Change: -0.000050, PSNR Diff: -0.000050 (New Max), Reward: -3.97, 09:55:09\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277281, PSNR After: 25.277328, PSNR Change: 0.000048, PSNR Diff: -0.000002 (New Max), Reward: 3.81, 09:55:10\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277328, PSNR After: 25.277359, PSNR Change: 0.000031, PSNR Diff: 0.000029 (New Max), Reward: 2.44, 09:55:11\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 290           |\n",
      "|    ep_rew_mean          | -891          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 1955          |\n",
      "|    total_timesteps      | 1536          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0221265e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0508        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0565        |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000175     |\n",
      "|    value_loss           | 0.162         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.272350, PSNR After: 25.272314, PSNR Change: -0.000036, PSNR Diff: -0.005016, Reward: -2.90, 09:57:18\n",
      "Step: 200, PSNR Before: 25.269861, PSNR After: 25.269844, PSNR Change: -0.000017, PSNR Diff: -0.007486, Reward: -1.37, 09:59:22\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010122 < -0.01 at step 256\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 6, Max consecutive episode failures: 6\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:00:32\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:00:33.222415\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277332, PSNR Change: 0.000002, PSNR Diff: 0.000002 (New Max), Reward: 0.15, 10:00:34\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277332, PSNR After: 25.277393, PSNR Change: 0.000061, PSNR Diff: 0.000063 (New Max), Reward: 4.88, 10:00:35\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 285           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 2280          |\n",
      "|    total_timesteps      | 1792          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0919757e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0887       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0961        |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00018      |\n",
      "|    value_loss           | 0.225         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.274603, PSNR After: 25.274536, PSNR Change: -0.000067, PSNR Diff: -0.002794, Reward: -5.34, 10:02:44\n",
      "Step: 200, PSNR Before: 25.271500, PSNR After: 25.271488, PSNR Change: -0.000011, PSNR Diff: -0.005842, Reward: -0.92, 10:04:48\n",
      "Step: 300, PSNR Before: 25.268984, PSNR After: 25.269003, PSNR Change: 0.000019, PSNR Diff: -0.008327, Reward: 1.53, 10:06:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 285           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 2604          |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2316741e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0565        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0688        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000205     |\n",
      "|    value_loss           | 0.153         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010099 < -0.01 at step 346\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 7, Max consecutive episode failures: 7\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:07:55\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:07:56.353252\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277376, PSNR Change: 0.000046, PSNR Diff: 0.000046 (New Max), Reward: 3.66, 10:07:57\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273611, PSNR After: 25.273504, PSNR Change: -0.000107, PSNR Diff: -0.003826, Reward: -8.54, 10:10:01\n",
      "Step: 200, PSNR Before: 25.270025, PSNR After: 25.270014, PSNR Change: -0.000011, PSNR Diff: -0.007317, Reward: -0.92, 10:12:06\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 294           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2930          |\n",
      "|    total_timesteps      | 2304          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3806857e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000544      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0462        |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000224     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010036 < -0.01 at step 271\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 8, Max consecutive episode failures: 8\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:13:41\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:13:42.611011\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277107, PSNR Change: -0.000223, PSNR Diff: -0.000223 (New Max), Reward: -17.85, 10:13:43\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277107, PSNR After: 25.277142, PSNR Change: 0.000034, PSNR Diff: -0.000189 (New Max), Reward: 2.75, 10:13:45\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277142, PSNR After: 25.277393, PSNR Change: 0.000252, PSNR Diff: 0.000063 (New Max), Reward: 20.14, 10:13:46\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274780, PSNR After: 25.274754, PSNR Change: -0.000027, PSNR Diff: -0.002577, Reward: -2.14, 10:15:47\n",
      "Step: 200, PSNR Before: 25.271580, PSNR After: 25.271561, PSNR Change: -0.000019, PSNR Diff: -0.005770, Reward: -1.53, 10:17:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 291           |\n",
      "|    ep_rew_mean          | -891          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 3255          |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5320256e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0549        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.093         |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000226     |\n",
      "|    value_loss           | 0.22          |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 25.267529, PSNR After: 25.267529, PSNR Change: 0.000000, PSNR Diff: -0.009802, Reward: 0.00, 10:20:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010025 < -0.01 at step 304\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 9, Max consecutive episode failures: 9\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:20:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:20:07.920550\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277327, PSNR Change: -0.000004, PSNR Diff: -0.000004 (New Max), Reward: -0.31, 10:20:09\u001b[0m\n",
      "Step: 100, PSNR Before: 25.272984, PSNR After: 25.272970, PSNR Change: -0.000013, PSNR Diff: -0.004360, Reward: -1.07, 10:22:11\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 292           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 3578          |\n",
      "|    total_timesteps      | 2816          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7508864e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00163      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0486        |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000246     |\n",
      "|    value_loss           | 0.123         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.269569, PSNR After: 25.269493, PSNR Change: -0.000076, PSNR Diff: -0.007837, Reward: -6.10, 10:24:22\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 275\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 10, Max consecutive episode failures: 10\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:25:56\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:25:56.964798\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277313, PSNR Change: -0.000017, PSNR Diff: -0.000017 (New Max), Reward: -1.37, 10:25:58\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273956, PSNR After: 25.273878, PSNR Change: -0.000078, PSNR Diff: -0.003452, Reward: -6.26, 10:28:00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 291           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 3902          |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7788261e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.077         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0998        |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000255     |\n",
      "|    value_loss           | 0.232         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.270617, PSNR After: 25.270641, PSNR Change: 0.000025, PSNR Diff: -0.006689, Reward: 1.98, 10:30:11\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010054 < -0.01 at step 274\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 11, Max consecutive episode failures: 11\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:31:43\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:31:44.303209\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277216, PSNR Change: -0.000114, PSNR Diff: -0.000114 (New Max), Reward: -9.16, 10:31:45\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277216, PSNR After: 25.277252, PSNR Change: 0.000036, PSNR Diff: -0.000078 (New Max), Reward: 2.90, 10:31:46\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274754, PSNR After: 25.274673, PSNR Change: -0.000080, PSNR Diff: -0.002657, Reward: -6.41, 10:33:48\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 4226          |\n",
      "|    total_timesteps      | 3328          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9860454e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0098        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0546        |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000264     |\n",
      "|    value_loss           | 0.134         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.270254, PSNR After: 25.270224, PSNR Change: -0.000031, PSNR Diff: -0.007107, Reward: -2.44, 10:35:59\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010023 < -0.01 at step 269\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 12, Max consecutive episode failures: 12\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:37:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:37:25.930540\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277283, PSNR Change: -0.000048, PSNR Diff: -0.000048 (New Max), Reward: -3.81, 10:37:27\u001b[0m\n",
      "\u001b[94mStep: 9, PSNR Before: 25.277205, PSNR After: 25.277309, PSNR Change: 0.000105, PSNR Diff: -0.000021 (New Max), Reward: 8.39, 10:37:37\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273510, PSNR After: 25.273487, PSNR Change: -0.000023, PSNR Diff: -0.003843, Reward: -1.83, 10:39:30\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 288           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 4551          |\n",
      "|    total_timesteps      | 3584          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0000152e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0443        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0581        |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000254     |\n",
      "|    value_loss           | 0.178         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.271235, PSNR After: 25.271187, PSNR Change: -0.000048, PSNR Diff: -0.006144, Reward: -3.81, 10:41:40\n",
      "Step: 300, PSNR Before: 25.267605, PSNR After: 25.267601, PSNR Change: -0.000004, PSNR Diff: -0.009729, Reward: -0.31, 10:43:44\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 305\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 13, Max consecutive episode failures: 13\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:43:51\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:43:51.664754\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277346, PSNR Change: 0.000015, PSNR Diff: 0.000015 (New Max), Reward: 1.22, 10:43:52\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277346, PSNR After: 25.277393, PSNR Change: 0.000048, PSNR Diff: 0.000063 (New Max), Reward: 3.81, 10:43:54\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -891          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 4874          |\n",
      "|    total_timesteps      | 3840          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1210872e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0363        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.076         |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000269     |\n",
      "|    value_loss           | 0.196         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.273359, PSNR After: 25.273338, PSNR Change: -0.000021, PSNR Diff: -0.003992, Reward: -1.68, 10:46:02\n",
      "Step: 200, PSNR Before: 25.270573, PSNR After: 25.270523, PSNR Change: -0.000050, PSNR Diff: -0.006807, Reward: -3.97, 10:48:05\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010000 < -0.01 at step 293\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 14, Max consecutive episode failures: 14\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:50:01\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:50:01.896292\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277308, PSNR Change: -0.000023, PSNR Diff: -0.000023 (New Max), Reward: -1.83, 10:50:03\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 290           |\n",
      "|    ep_rew_mean          | -891          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 5197          |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4889596e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0282       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0528        |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000299     |\n",
      "|    value_loss           | 0.131         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.273191, PSNR After: 25.273249, PSNR Change: 0.000057, PSNR Diff: -0.004082, Reward: 4.58, 10:52:12\n",
      "Step: 200, PSNR Before: 25.269360, PSNR After: 25.269360, PSNR Change: 0.000000, PSNR Diff: -0.007971, Reward: 0.00, 10:54:16\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010113 < -0.01 at step 284\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 15, Max consecutive episode failures: 15\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 10:56:01\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 10:56:01.645137\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277311, PSNR Change: -0.000019, PSNR Diff: -0.000019 (New Max), Reward: -1.53, 10:56:02\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 5521          |\n",
      "|    total_timesteps      | 4352          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4517067e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0288        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.049         |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000285     |\n",
      "|    value_loss           | 0.125         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.273094, PSNR After: 25.273046, PSNR Change: -0.000048, PSNR Diff: -0.004284, Reward: -3.81, 10:58:12\n",
      "Step: 200, PSNR Before: 25.269842, PSNR After: 25.269871, PSNR Change: 0.000029, PSNR Diff: -0.007460, Reward: 2.29, 11:00:16\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 5846          |\n",
      "|    total_timesteps      | 4608          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6123598e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0713        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0559        |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000305     |\n",
      "|    value_loss           | 0.15          |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010189 < -0.01 at step 292\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 16, Max consecutive episode failures: 16\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:02:17\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:02:18.135660\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277378, PSNR Change: 0.000048, PSNR Diff: 0.000048 (New Max), Reward: 3.81, 11:02:19\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277378, PSNR After: 25.277405, PSNR Change: 0.000027, PSNR Diff: 0.000074 (New Max), Reward: 2.14, 11:02:21\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 25.277405, PSNR After: 25.277489, PSNR Change: 0.000084, PSNR Diff: 0.000158 (New Max), Reward: 6.71, 11:02:23\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 25.277489, PSNR After: 25.277523, PSNR Change: 0.000034, PSNR Diff: 0.000193 (New Max), Reward: 2.75, 11:02:24\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273575, PSNR After: 25.273518, PSNR Change: -0.000057, PSNR Diff: -0.003813, Reward: -4.58, 11:04:21\n",
      "Step: 200, PSNR Before: 25.270050, PSNR After: 25.270004, PSNR Change: -0.000046, PSNR Diff: -0.007326, Reward: -3.66, 11:06:25\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 6168          |\n",
      "|    total_timesteps      | 4864          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8894283e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0652        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0542        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000328     |\n",
      "|    value_loss           | 0.14          |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010141 < -0.01 at step 249\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 17, Max consecutive episode failures: 17\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:07:32\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:07:33.521401\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277344, PSNR Change: 0.000013, PSNR Diff: 0.000013 (New Max), Reward: 1.07, 11:07:34\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277344, PSNR After: 25.277393, PSNR Change: 0.000050, PSNR Diff: 0.000063 (New Max), Reward: 3.97, 11:07:35\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273987, PSNR After: 25.274006, PSNR Change: 0.000019, PSNR Diff: -0.003325, Reward: 1.53, 11:09:37\n",
      "Step: 200, PSNR Before: 25.271360, PSNR After: 25.271341, PSNR Change: -0.000019, PSNR Diff: -0.005989, Reward: -1.53, 11:11:41\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 287           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 6492          |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6938505e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0211       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0725        |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000314     |\n",
      "|    value_loss           | 0.195         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010036 < -0.01 at step 293\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 18, Max consecutive episode failures: 18\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:13:43\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:13:44.117706\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277294, PSNR Change: -0.000036, PSNR Diff: -0.000036 (New Max), Reward: -2.90, 11:13:45\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274733, PSNR After: 25.274654, PSNR Change: -0.000078, PSNR Diff: -0.002676, Reward: -6.26, 11:15:47\n",
      "Step: 200, PSNR Before: 25.271339, PSNR After: 25.271322, PSNR Change: -0.000017, PSNR Diff: -0.006008, Reward: -1.37, 11:17:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 288           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 6815          |\n",
      "|    total_timesteps      | 5376          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9965304e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00703      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0495        |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.000325     |\n",
      "|    value_loss           | 0.12          |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 25.268194, PSNR After: 25.268114, PSNR Change: -0.000080, PSNR Diff: -0.009216, Reward: -6.41, 11:20:01\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010042 < -0.01 at step 309\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 19, Max consecutive episode failures: 19\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:20:13\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:20:13.730785\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277308, PSNR Change: -0.000023, PSNR Diff: -0.000023 (New Max), Reward: -1.83, 11:20:14\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277308, PSNR After: 25.277328, PSNR Change: 0.000021, PSNR Diff: -0.000002 (New Max), Reward: 1.68, 11:20:16\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277328, PSNR After: 25.277338, PSNR Change: 0.000010, PSNR Diff: 0.000008 (New Max), Reward: 0.76, 11:20:17\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 25.277338, PSNR After: 25.277498, PSNR Change: 0.000160, PSNR Diff: 0.000168 (New Max), Reward: 12.82, 11:20:18\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 25.277491, PSNR After: 25.277515, PSNR Change: 0.000025, PSNR Diff: 0.000185 (New Max), Reward: 1.98, 11:20:21\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274216, PSNR After: 25.274155, PSNR Change: -0.000061, PSNR Diff: -0.003176, Reward: -4.88, 11:22:17\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 7138          |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4668483e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000742      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0382        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000359     |\n",
      "|    value_loss           | 0.0885        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.271114, PSNR After: 25.271133, PSNR Change: 0.000019, PSNR Diff: -0.006197, Reward: 1.53, 11:24:27\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010088 < -0.01 at step 299\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 20, Max consecutive episode failures: 20\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:26:30\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:26:31.408993\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277328, PSNR Change: -0.000002, PSNR Diff: -0.000002 (New Max), Reward: -0.15, 11:26:32\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277328, PSNR After: 25.277340, PSNR Change: 0.000011, PSNR Diff: 0.000010 (New Max), Reward: 0.92, 11:26:33\u001b[0m\n",
      "Step: 100, PSNR Before: 25.272472, PSNR After: 25.272354, PSNR Change: -0.000118, PSNR Diff: -0.004976, Reward: -9.46, 11:28:35\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 289          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 7462         |\n",
      "|    total_timesteps      | 5888         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.639143e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0196       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0612       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000376    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.268763, PSNR After: 25.268738, PSNR Change: -0.000025, PSNR Diff: -0.008593, Reward: -1.98, 11:30:45\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010077 < -0.01 at step 251\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 21, Max consecutive episode failures: 21\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:31:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:31:49.506616\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277332, PSNR Change: 0.000002, PSNR Diff: 0.000002 (New Max), Reward: 0.15, 11:31:50\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273575, PSNR After: 25.273571, PSNR Change: -0.000004, PSNR Diff: -0.003759, Reward: -0.31, 11:33:54\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 287           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 7785          |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4528784e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0207       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0841        |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000359     |\n",
      "|    value_loss           | 0.193         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.270254, PSNR After: 25.270243, PSNR Change: -0.000011, PSNR Diff: -0.007088, Reward: -0.92, 11:36:04\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010235 < -0.01 at step 280\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 22, Max consecutive episode failures: 22\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:37:43\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:37:44.093222\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277214, PSNR Change: -0.000116, PSNR Diff: -0.000116 (New Max), Reward: -9.31, 11:37:45\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277214, PSNR After: 25.277218, PSNR Change: 0.000004, PSNR Diff: -0.000113 (New Max), Reward: 0.31, 11:37:46\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277218, PSNR After: 25.277271, PSNR Change: 0.000053, PSNR Diff: -0.000059 (New Max), Reward: 4.27, 11:37:47\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 25.277271, PSNR After: 25.277311, PSNR Change: 0.000040, PSNR Diff: -0.000019 (New Max), Reward: 3.20, 11:37:49\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 287           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 8108          |\n",
      "|    total_timesteps      | 6400          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3190084e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0118        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0507        |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000414     |\n",
      "|    value_loss           | 0.142         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.275311, PSNR After: 25.275286, PSNR Change: -0.000025, PSNR Diff: -0.002045, Reward: -1.98, 11:39:55\n",
      "Step: 200, PSNR Before: 25.271652, PSNR After: 25.271614, PSNR Change: -0.000038, PSNR Diff: -0.005716, Reward: -3.05, 11:41:58\n",
      "Step: 300, PSNR Before: 25.268433, PSNR After: 25.268442, PSNR Change: 0.000010, PSNR Diff: -0.008888, Reward: 0.76, 11:44:03\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010012 < -0.01 at step 337\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 23, Max consecutive episode failures: 23\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:44:49\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 8433          |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4493936e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0584        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0329        |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.0004       |\n",
      "|    value_loss           | 0.092         |\n",
      "-------------------------------------------\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:44:56.514054\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277206, PSNR Change: -0.000124, PSNR Diff: -0.000124 (New Max), Reward: -9.92, 11:44:57\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277206, PSNR After: 25.277351, PSNR Change: 0.000145, PSNR Diff: 0.000021 (New Max), Reward: 11.60, 11:44:59\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273090, PSNR After: 25.273058, PSNR Change: -0.000032, PSNR Diff: -0.004272, Reward: -2.59, 11:46:59\n",
      "Step: 200, PSNR Before: 25.270756, PSNR After: 25.270689, PSNR Change: -0.000067, PSNR Diff: -0.006641, Reward: -5.34, 11:49:03\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 289           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 8755          |\n",
      "|    total_timesteps      | 6912          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3446198e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.034         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0376        |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.000397     |\n",
      "|    value_loss           | 0.0987        |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010031 < -0.01 at step 268\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 24, Max consecutive episode failures: 24\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:50:34\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:50:35.405761\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277393, PSNR Change: 0.000063, PSNR Diff: 0.000063 (New Max), Reward: 5.04, 11:50:36\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273134, PSNR After: 25.273079, PSNR Change: -0.000055, PSNR Diff: -0.004251, Reward: -4.43, 11:52:38\n",
      "Step: 200, PSNR Before: 25.270350, PSNR After: 25.270332, PSNR Change: -0.000017, PSNR Diff: -0.006998, Reward: -1.37, 11:54:41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 289          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 9077         |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.512258e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0112       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00041     |\n",
      "|    value_loss           | 0.17         |\n",
      "------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010082 < -0.01 at step 299\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 25, Max consecutive episode failures: 25\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 11:56:50\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 11:56:51.404458\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277386, PSNR Change: 0.000055, PSNR Diff: 0.000055 (New Max), Reward: 4.43, 11:56:52\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273123, PSNR After: 25.273123, PSNR Change: 0.000000, PSNR Diff: -0.004208, Reward: 0.00, 11:58:54\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 289        |\n",
      "|    ep_rew_mean          | -893       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 9400       |\n",
      "|    total_timesteps      | 7424       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 4.8941e-07 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -14.6      |\n",
      "|    explained_variance   | 0.0184     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0442     |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.00043   |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Step: 200, PSNR Before: 25.268309, PSNR After: 25.268314, PSNR Change: 0.000006, PSNR Diff: -0.009016, Reward: 0.46, 12:01:05\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010000 < -0.01 at step 247\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 26, Max consecutive episode failures: 26\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:02:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:02:04.380759\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277252, PSNR Change: -0.000078, PSNR Diff: -0.000078 (New Max), Reward: -6.26, 12:02:05\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274300, PSNR After: 25.274298, PSNR Change: -0.000002, PSNR Diff: -0.003033, Reward: -0.15, 12:04:07\n",
      "Step: 200, PSNR Before: 25.270050, PSNR After: 25.270033, PSNR Change: -0.000017, PSNR Diff: -0.007298, Reward: -1.37, 12:06:12\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 287           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 9723          |\n",
      "|    total_timesteps      | 7680          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7823414e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0229       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0734        |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000431     |\n",
      "|    value_loss           | 0.205         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010014 < -0.01 at step 277\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 27, Max consecutive episode failures: 27\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:07:55\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:07:55.999193\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277321, PSNR Change: -0.000010, PSNR Diff: -0.000010 (New Max), Reward: -0.76, 12:07:57\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273041, PSNR After: 25.272943, PSNR Change: -0.000097, PSNR Diff: -0.004387, Reward: -7.78, 12:09:59\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 287           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 10047         |\n",
      "|    total_timesteps      | 7936          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1211176e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0149       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0653        |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000487     |\n",
      "|    value_loss           | 0.147         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.269451, PSNR After: 25.269444, PSNR Change: -0.000008, PSNR Diff: -0.007887, Reward: -0.61, 12:12:10\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010101 < -0.01 at step 268\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 28, Max consecutive episode failures: 28\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:13:34\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:13:35.343053\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277338, PSNR Change: 0.000008, PSNR Diff: 0.000008 (New Max), Reward: 0.61, 12:13:36\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277338, PSNR After: 25.277365, PSNR Change: 0.000027, PSNR Diff: 0.000034 (New Max), Reward: 2.14, 12:13:37\u001b[0m\n",
      "\u001b[94mStep: 14, PSNR Before: 25.277355, PSNR After: 25.277374, PSNR Change: 0.000019, PSNR Diff: 0.000044 (New Max), Reward: 1.53, 12:13:52\u001b[0m\n",
      "Step: 100, PSNR Before: 25.272896, PSNR After: 25.272705, PSNR Change: -0.000191, PSNR Diff: -0.004625, Reward: -15.26, 12:15:39\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 286           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 10371         |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2549876e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00883       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0746        |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000431     |\n",
      "|    value_loss           | 0.149         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 25.269321, PSNR After: 25.269253, PSNR Change: -0.000069, PSNR Diff: -0.008078, Reward: -5.49, 12:17:50\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010094 < -0.01 at step 272\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 29, Max consecutive episode failures: 29\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:19:18\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:19:19.534851\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277393, PSNR Change: 0.000063, PSNR Diff: 0.000063 (New Max), Reward: 5.04, 12:19:20\u001b[0m\n",
      "Step: 100, PSNR Before: 25.273842, PSNR After: 25.273865, PSNR Change: 0.000023, PSNR Diff: -0.003466, Reward: 1.83, 12:21:23\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 286          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 10694        |\n",
      "|    total_timesteps      | 8448         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.867332e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0138      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0856       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.270138, PSNR After: 25.270206, PSNR Change: 0.000069, PSNR Diff: -0.007124, Reward: 5.49, 12:23:33\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010017 < -0.01 at step 280\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 30, Max consecutive episode failures: 30\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:25:12\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:25:13.090758\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277267, PSNR Change: -0.000063, PSNR Diff: -0.000063 (New Max), Reward: -5.04, 12:25:14\u001b[0m\n",
      "Step: 100, PSNR Before: 25.274847, PSNR After: 25.274742, PSNR Change: -0.000105, PSNR Diff: -0.002588, Reward: -8.39, 12:27:17\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 286          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 11016        |\n",
      "|    total_timesteps      | 8704         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.975606e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0217      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0425       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    value_loss           | 0.0917       |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 25.271494, PSNR After: 25.271494, PSNR Change: 0.000000, PSNR Diff: -0.005836, Reward: 0.00, 12:29:27\n",
      "Step: 300, PSNR Before: 25.268738, PSNR After: 25.268620, PSNR Change: -0.000118, PSNR Diff: -0.008711, Reward: -9.46, 12:31:31\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010092 < -0.01 at step 341\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 31, Max consecutive episode failures: 31\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:32:21\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:32:22.401889\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277172, PSNR Change: -0.000158, PSNR Diff: -0.000158 (New Max), Reward: -12.66, 12:32:23\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277172, PSNR After: 25.277185, PSNR Change: 0.000013, PSNR Diff: -0.000145 (New Max), Reward: 1.07, 12:32:24\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 25.277185, PSNR After: 25.277203, PSNR Change: 0.000017, PSNR Diff: -0.000128 (New Max), Reward: 1.37, 12:32:26\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 288           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 11339         |\n",
      "|    total_timesteps      | 8960          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3376653e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.024         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0356        |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.000544     |\n",
      "|    value_loss           | 0.0756        |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 25.274336, PSNR After: 25.274324, PSNR Change: -0.000011, PSNR Diff: -0.003006, Reward: -0.92, 12:34:32\n",
      "Step: 200, PSNR Before: 25.269434, PSNR After: 25.269392, PSNR Change: -0.000042, PSNR Diff: -0.007938, Reward: -3.36, 12:36:36\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010050 < -0.01 at step 283\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 32, Max consecutive episode failures: 32\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:38:19\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:38:19.700213\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277237, PSNR Change: -0.000093, PSNR Diff: -0.000093 (New Max), Reward: -7.48, 12:38:20\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 288          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 11662        |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.926727e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0183       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0279       |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000576    |\n",
      "|    value_loss           | 0.0601       |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 25.273327, PSNR After: 25.273201, PSNR Change: -0.000126, PSNR Diff: -0.004129, Reward: -10.07, 12:40:29\n",
      "Step: 200, PSNR Before: 25.269337, PSNR After: 25.269251, PSNR Change: -0.000086, PSNR Diff: -0.008080, Reward: -6.87, 12:42:33\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010017 < -0.01 at step 231\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 33, Max consecutive episode failures: 33\u001b[0m\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 12:43:11\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002806, Initial PSNR: 25.277330, 2024-12-24 12:43:12.492840\n",
      "\u001b[94mStep: 1, PSNR Before: 25.277330, PSNR After: 25.277325, PSNR Change: -0.000006, PSNR Diff: -0.000006 (New Max), Reward: -0.46, 12:43:13\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 25.277325, PSNR After: 25.277330, PSNR Change: 0.000006, PSNR Diff: 0.000000 (New Max), Reward: 0.46, 12:43:14\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 286          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 11984        |\n",
      "|    total_timesteps      | 9472         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.260831e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0297       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0659       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000566    |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 25.273855, PSNR After: 25.273878, PSNR Change: 0.000023, PSNR Diff: -0.003452, Reward: 1.83, 12:45:23\n",
      "Step: 200, PSNR Before: 25.271627, PSNR After: 25.271502, PSNR Change: -0.000126, PSNR Diff: -0.005829, Reward: -10.07, 12:47:26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#      \n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 512, 512).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(512)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((512, 512))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 512 or target.shape[-2] < 512:\n",
    "            target = torchvision.transforms.Resize(512)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv \n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        \"\"\"\n",
    "          .\n",
    "        target_function:   (MSE  PSNR)  .\n",
    "        trainloader:   .\n",
    "        max_steps:   .\n",
    "        T_PSNR:  PSNR .\n",
    "        T_steps: PSNR     .\n",
    "        T_PSNR_DIFF: PSNR  .\n",
    "        max_allowed_changes:        (: 1).\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        #  : (1, 8, 512, 512)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.float32)\n",
    "\n",
    "        #  :     (512 * 512 * 8)\n",
    "        self.num_pixels = 8 * 512 * 512\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        #     \n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        #  \n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  #  \n",
    "\n",
    "        #   \n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        #     \n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        #    \n",
    "        self.retry_current_target = False  #    \n",
    "\n",
    "        #    \n",
    "        self.consecutive_fail_count = 0  #   \n",
    "        self.max_consecutive_failures = 0  #     \n",
    "\n",
    "        #  PSNR_DIFF  \n",
    "        self.max_psnr_diff = float('-inf')  #   PSNR_DIFF \n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        \"\"\"\n",
    "          .\n",
    "              .\n",
    "        -    . \n",
    "        - BinaryNet    .\n",
    "        -  (state)   .\n",
    "        -  PSNR MSE  .\n",
    "        -      .\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional):   . Default None.\n",
    "            options (dict, optional):  . Default None.\n",
    "            lr (float, optional): . Default 1e-4.\n",
    "            z (float, optional):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            dict:    .\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.retry_current_target:  #    \n",
    "            self.consecutive_fail_count += 1\n",
    "        else:\n",
    "            self.consecutive_fail_count = 0  #     \n",
    "\n",
    "        self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        if not self.retry_current_target:  #      \n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        #    PSNR  \n",
    "        self.max_psnr_diff = float('-inf')\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  #  \n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta  \n",
    "\n",
    "        # \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE  PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  #  PSNR \n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  #      \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "          ,     .\n",
    "\n",
    "        Args:\n",
    "            z (float):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            dict:    .\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            #    \n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  #  numpy  \n",
    "\n",
    "        #     \n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  #  Torch  \n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  #   \n",
    "\n",
    "        #  \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        #  MSE PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        #   \n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        #    \n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        #  \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # (  ) \n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "            .\n",
    "        -  0.2 ~ 0.8     .\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): .\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:    1,    0.\n",
    "        \"\"\"\n",
    "        #     \n",
    "        mask = np.zeros(self.num_pixels, dtype=np.int8)\n",
    "\n",
    "        # (1, 8, 512, 512) -> (8, 512, 512) \n",
    "        obs = observation.squeeze()\n",
    "\n",
    "        #      \n",
    "        for channel in range(obs.shape[0]):\n",
    "            indices = np.where((obs[channel] > 0) & (obs[channel] < 1))\n",
    "            for row, col in zip(*indices):\n",
    "                pixel_idx = channel * 512 * 512 + row * 512 + col\n",
    "                mask[pixel_idx] = 1  #   \n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "           .\n",
    "        -  (action) ,   .\n",
    "        -    PSNR (psnr_change)  .\n",
    "        - psnr_change 0     .\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray):   .\n",
    "            lr (float, optional): . Default 1e-4.\n",
    "            z (float, optional):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            float:  .\n",
    "            bool:  .\n",
    "            bool: Truncated .\n",
    "            dict:   (MSE, PSNR, PSNR_DIFF,   ).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        #   \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        if mask.flatten()[action] == 0:\n",
    "            #       \n",
    "            #print(f\"Invalid action taken at step {self.steps}, action: {action}\")\n",
    "            return self.observation, -10.0, False, False, {\"mask\": mask}\n",
    "\n",
    "        #   PSNR \n",
    "        binary_before = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_before = tt.Tensor(binary_before, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_before = tt.simulate(binary_before, z).abs()**2\n",
    "        result_before = torch.mean(sim_before, dim=1, keepdim=True)\n",
    "        psnr_before = tt.relativeLoss(result_before, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        #     \n",
    "        channel = action // (512 * 512)\n",
    "        pixel_index = action % (512 * 512)\n",
    "        row = pixel_index // 512\n",
    "        col = pixel_index % 512\n",
    "\n",
    "        #   \n",
    "        self.state[0, channel, row, col] = 1 - self.state[0, channel, row, col]\n",
    "\n",
    "        #     \n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # PSNR  \n",
    "        psnr_change = psnr_after - psnr_before\n",
    "\n",
    "        # PSNR_CHANGE 0     \n",
    "        #if psnr_change < 0:\n",
    "            #print(f\"Invalid action: PSNR Change {psnr_change:.6f} < 0 at step {self.steps}\")\n",
    "        #    return self.observation, -10.0, False, False, {\"psnr_before\": psnr_before, \"psnr_after\": psnr_after, \"psnr_change\": psnr_change, \"mask\": mask}\n",
    "\n",
    "        #  PSNR_DIFF \n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  #  PSNR_DIFF \n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  #  PSNR_DIFF \n",
    "\n",
    "        #  \n",
    "        reward = psnr_change * 80000  # PSNR (psnr_change)  \n",
    "\n",
    "        #   \n",
    "        if psnr_diff < -0.01:\n",
    "            print(f\"\\033[91mEpisode failed: PSNR Diff {psnr_diff:.6f} < -0.01 at step {self.steps}\\033[0m\")\n",
    "            self.retry_current_target = True  #     \n",
    "            return self.observation, -100.0, True, False, {\"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        #  PSNR_DIFF  \n",
    "        if is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time}\\033[0m\"\n",
    "            )\n",
    "\n",
    "        #   (100  )\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                  f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        #   : PSNR >= T_PSNR  PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  #   \n",
    "                reward += 100  #     \n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        #  \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "\n",
    "        del binary_before, binary_after, sim_before, sim_after, result_before, result_after\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  #  \n",
    "padding = 0\n",
    "\n",
    "# Dataset512  \n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet  \n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#   \n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "#      \n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  #  train_loader \n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker  \n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized  \n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO \n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,  #  \n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping \n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#   \n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO   \n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])]  #    \n",
    "#)\n",
    "\n",
    "# PPO  \n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",  # LSTM    MLP \n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  #   \n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "# \n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#  \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO \n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#   \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "#from sb3_contrib import RecurrentPPO\n",
    "\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  #    \n",
    "#    lstm_hidden_size=128,  # LSTM  \n",
    "#    shared_lstm=False  #  LSTM \n",
    "#)\n",
    "\n",
    "#ppo_model = RecurrentPPO(\n",
    "#    \"MlpLstmPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  #   \n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "\n",
    "# \n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#  \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "#   \n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback \n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  #   ( )\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#\n",
    "\n",
    "#   ( )\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
