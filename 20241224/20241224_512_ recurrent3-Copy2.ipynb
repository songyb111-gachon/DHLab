{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 05:54:00\n",
      "Logging to ./ppo_with_mask/PPO_90\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 05:54:00.787380\n",
      "Step: 100, MSE: 0.003787, PSNR: 24.044861, PSNR Diff: -0.012711, Changes: 0.6255106925964355, Reward: -1.02, 05:54:12\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.028763, PSNR Diff: -0.028809, Changes: 7.509818077087402, Reward: -2.30, 05:54:25\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 7   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 32  |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "Step: 300, MSE: 0.003816, PSNR: 24.012455, PSNR Diff: -0.045116, Changes: 0.8308112621307373, Reward: -3.61, 05:54:50\n",
      "Step: 400, MSE: 0.003829, PSNR: 23.997307, PSNR Diff: -0.060265, Changes: 3.887582302093506, Reward: -4.82, 05:55:02\n",
      "Step: 500, MSE: 0.003842, PSNR: 23.982973, PSNR Diff: -0.074598, Changes: 4.018254280090332, Reward: -5.97, 05:55:15\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 6             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 76            |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7677542e+19 |\n",
      "|    clip_fraction        | 0.921         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.00148       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 1.59          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 40.7          |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.93          |\n",
      "-------------------------------------------\n",
      "Step: 600, MSE: 0.003855, PSNR: 23.967752, PSNR Diff: -0.089819, Changes: 4.318077564239502, Reward: -7.19, 05:55:40\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100203 < -1 at step 664\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 05:55:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 05:55:48.422890\n",
      "Step: 100, MSE: 0.003787, PSNR: 24.045113, PSNR Diff: -0.012459, Changes: 2.3270151615142822, Reward: -1.00, 05:56:02\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 665          |\n",
      "|    ep_rew_mean          | -2.71e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 6            |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 768          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.409717e+18 |\n",
      "|    clip_fraction        | 0.912        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.346        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.000459     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 7.29e+09     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.295        |\n",
      "------------------------------------------\n",
      "Step: 200, MSE: 0.003799, PSNR: 24.030842, PSNR Diff: -0.026730, Changes: 4.954277038574219, Reward: -2.14, 05:56:28\n",
      "Step: 300, MSE: 0.003810, PSNR: 24.018499, PSNR Diff: -0.039072, Changes: 2.339592456817627, Reward: -3.13, 05:56:43\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 665          |\n",
      "|    ep_rew_mean          | -2.71e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 6            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.459365e+18 |\n",
      "|    clip_fraction        | 0.923        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.137       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 1.34e+04     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.14         |\n",
      "------------------------------------------\n",
      "Step: 400, MSE: 0.003823, PSNR: 24.004271, PSNR Diff: -0.053301, Changes: 3.655775547027588, Reward: -4.26, 05:57:07\n",
      "Step: 500, MSE: 0.003837, PSNR: 23.987816, PSNR Diff: -0.069756, Changes: 4.3747076988220215, Reward: -5.58, 05:57:20\n",
      "Step: 600, MSE: 0.003849, PSNR: 23.973991, PSNR Diff: -0.083580, Changes: 2.148810625076294, Reward: -6.69, 05:57:32\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 665          |\n",
      "|    ep_rew_mean          | -2.71e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.600891e+18 |\n",
      "|    clip_fraction        | 0.913        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | -0.741       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.159       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 2.94e+03     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0108       |\n",
      "------------------------------------------\n",
      "Step: 700, MSE: 0.003863, PSNR: 23.959187, PSNR Diff: -0.098385, Changes: 1.8078572750091553, Reward: -7.87, 05:57:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100132 < -1 at step 715\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 05:58:00\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 05:58:00.092310\n",
      "Step: 100, MSE: 0.003788, PSNR: 24.043941, PSNR Diff: -0.013630, Changes: 5.344422340393066, Reward: -1.09, 05:58:12\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 690           |\n",
      "|    ep_rew_mean          | -2.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 259           |\n",
      "|    total_timesteps      | 1536          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3188166e+19 |\n",
      "|    clip_fraction        | 0.91          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.121        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.0985       |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 1.75e+04      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.135         |\n",
      "-------------------------------------------\n",
      "Step: 200, MSE: 0.003800, PSNR: 24.030220, PSNR Diff: -0.027351, Changes: 1.513439416885376, Reward: -2.19, 05:58:37\n",
      "Step: 300, MSE: 0.003811, PSNR: 24.017040, PSNR Diff: -0.040531, Changes: 5.253434181213379, Reward: -3.24, 05:58:49\n",
      "Step: 400, MSE: 0.003825, PSNR: 24.001730, PSNR Diff: -0.055841, Changes: 2.878253221511841, Reward: -4.47, 05:59:01\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 690           |\n",
      "|    ep_rew_mean          | -2.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 302           |\n",
      "|    total_timesteps      | 1792          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8577418e+18 |\n",
      "|    clip_fraction        | 0.918         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.0764        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.15         |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 1.39          |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0815        |\n",
      "-------------------------------------------\n",
      "Step: 500, MSE: 0.003835, PSNR: 23.989851, PSNR Diff: -0.067720, Changes: 3.88246488571167, Reward: -5.42, 05:59:26\n",
      "Step: 600, MSE: 0.003848, PSNR: 23.975231, PSNR Diff: -0.082340, Changes: 1.9803423881530762, Reward: -6.59, 05:59:38\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 690           |\n",
      "|    ep_rew_mean          | -2.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 345           |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0868431e+18 |\n",
      "|    clip_fraction        | 0.912         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.282        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 9.86e+13      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 2.46e+12      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0258        |\n",
      "-------------------------------------------\n",
      "Step: 700, MSE: 0.003863, PSNR: 23.959175, PSNR Diff: -0.098396, Changes: 4.129975318908691, Reward: -7.87, 06:00:01\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100414 < -1 at step 708\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:00:02\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:00:02.965139\n",
      "Step: 100, MSE: 0.003788, PSNR: 24.043386, PSNR Diff: -0.014185, Changes: 2.4299135208129883, Reward: -1.13, 06:00:14\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.029114, PSNR Diff: -0.028458, Changes: 2.74625825881958, Reward: -2.28, 06:00:27\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 697          |\n",
      "|    ep_rew_mean          | -2.82e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 388          |\n",
      "|    total_timesteps      | 2304         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.549382e+17 |\n",
      "|    clip_fraction        | 0.912        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | -0.0414      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.0788      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 2.4e+07      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.148        |\n",
      "------------------------------------------\n",
      "Step: 300, MSE: 0.003812, PSNR: 24.016268, PSNR Diff: -0.041304, Changes: 1.1913998126983643, Reward: -3.30, 06:00:51\n",
      "Step: 400, MSE: 0.003824, PSNR: 24.002598, PSNR Diff: -0.054974, Changes: 3.7851059436798096, Reward: -4.40, 06:01:04\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 697          |\n",
      "|    ep_rew_mean          | -2.82e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 432          |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.641252e+17 |\n",
      "|    clip_fraction        | 0.907        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.0452       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.02e+15     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 7.55e+13     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0241       |\n",
      "------------------------------------------\n",
      "Step: 500, MSE: 0.003836, PSNR: 23.988693, PSNR Diff: -0.068878, Changes: 5.070806503295898, Reward: -5.51, 06:01:28\n",
      "Step: 600, MSE: 0.003848, PSNR: 23.975281, PSNR Diff: -0.082291, Changes: 2.7072973251342773, Reward: -6.58, 06:01:40\n",
      "Step: 700, MSE: 0.003863, PSNR: 23.958197, PSNR Diff: -0.099375, Changes: 3.480868339538574, Reward: -7.95, 06:01:53\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100130 < -1 at step 707\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:01:54\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:01:54.086923\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 700           |\n",
      "|    ep_rew_mean          | -2.83e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 476           |\n",
      "|    total_timesteps      | 2816          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8752012e+17 |\n",
      "|    clip_fraction        | 0.924         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.143        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.144        |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | 1.52e+13      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0514        |\n",
      "-------------------------------------------\n",
      "Step: 100, MSE: 0.003788, PSNR: 24.043539, PSNR Diff: -0.014032, Changes: 4.4752936363220215, Reward: -1.12, 06:02:18\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.028587, PSNR Diff: -0.028984, Changes: 4.053345680236816, Reward: -2.32, 06:02:31\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 700          |\n",
      "|    ep_rew_mean          | -2.83e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 520          |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.886464e+18 |\n",
      "|    clip_fraction        | 0.904        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.0291       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.00347     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 1.29e+07     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "Step: 300, MSE: 0.003813, PSNR: 24.015045, PSNR Diff: -0.042526, Changes: 4.0, Reward: -3.40, 06:02:56\n",
      "Step: 400, MSE: 0.003828, PSNR: 23.998604, PSNR Diff: -0.058968, Changes: 3.0338354110717773, Reward: -4.72, 06:03:08\n",
      "Step: 500, MSE: 0.003839, PSNR: 23.985718, PSNR Diff: -0.071854, Changes: 3.569469451904297, Reward: -5.75, 06:03:21\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 700           |\n",
      "|    ep_rew_mean          | -2.83e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 564           |\n",
      "|    total_timesteps      | 3328          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4076231e+18 |\n",
      "|    clip_fraction        | 0.917         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.195        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.177        |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.0868       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00662       |\n",
      "-------------------------------------------\n",
      "Step: 600, MSE: 0.003852, PSNR: 23.971313, PSNR Diff: -0.086258, Changes: 3.3157851696014404, Reward: -6.90, 06:03:45\n",
      "Step: 700, MSE: 0.003864, PSNR: 23.957611, PSNR Diff: -0.099960, Changes: 3.085017204284668, Reward: -8.00, 06:03:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100029 < -1 at step 701\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:03:58\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:03:58.176372\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 700         |\n",
      "|    ep_rew_mean          | -2.85e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 3584        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.93197e+18 |\n",
      "|    clip_fraction        | 0.917       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98e+06   |\n",
      "|    explained_variance   | -0.0679     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 208         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 5.16e+13    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.088       |\n",
      "-----------------------------------------\n",
      "Step: 100, MSE: 0.003787, PSNR: 24.045567, PSNR Diff: -0.012005, Changes: 2.672429084777832, Reward: -0.96, 06:04:22\n",
      "Step: 200, MSE: 0.003798, PSNR: 24.032734, PSNR Diff: -0.024837, Changes: 3.8723464012145996, Reward: -1.99, 06:04:34\n",
      "Step: 300, MSE: 0.003809, PSNR: 24.019928, PSNR Diff: -0.037643, Changes: 3.9193429946899414, Reward: -3.01, 06:04:47\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 700          |\n",
      "|    ep_rew_mean          | -2.85e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 651          |\n",
      "|    total_timesteps      | 3840         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.747119e+17 |\n",
      "|    clip_fraction        | 0.917        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.12        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.836        |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "Step: 400, MSE: 0.003821, PSNR: 24.006350, PSNR Diff: -0.051222, Changes: 3.574737310409546, Reward: -4.10, 06:05:10\n",
      "Step: 500, MSE: 0.003834, PSNR: 23.990929, PSNR Diff: -0.066643, Changes: 2.569458246231079, Reward: -5.33, 06:05:22\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 700           |\n",
      "|    ep_rew_mean          | -2.85e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 693           |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6312657e+18 |\n",
      "|    clip_fraction        | 0.914         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.172        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.0148       |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | 1.73e+05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00851       |\n",
      "-------------------------------------------\n",
      "Step: 600, MSE: 0.003847, PSNR: 23.976915, PSNR Diff: -0.080656, Changes: 3.283261299133301, Reward: -6.45, 06:05:46\n",
      "Step: 700, MSE: 0.003859, PSNR: 23.963181, PSNR Diff: -0.094391, Changes: 2.67319655418396, Reward: -7.55, 06:05:58\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100092 < -1 at step 744\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:06:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:06:04.082367\n",
      "Step: 100, MSE: 0.003788, PSNR: 24.043444, PSNR Diff: -0.014128, Changes: 4.822421073913574, Reward: -1.13, 06:06:16\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 708           |\n",
      "|    ep_rew_mean          | -2.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 736           |\n",
      "|    total_timesteps      | 4352          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7606002e+18 |\n",
      "|    clip_fraction        | 0.912         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0558       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 6.59          |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | 4.47e+08      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0942        |\n",
      "-------------------------------------------\n",
      "Step: 200, MSE: 0.003802, PSNR: 24.027863, PSNR Diff: -0.029709, Changes: 5.906558036804199, Reward: -2.38, 06:06:39\n",
      "Step: 300, MSE: 0.003817, PSNR: 24.010338, PSNR Diff: -0.047234, Changes: 3.0454094409942627, Reward: -3.78, 06:06:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 708           |\n",
      "|    ep_rew_mean          | -2.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 778           |\n",
      "|    total_timesteps      | 4608          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4892412e+19 |\n",
      "|    clip_fraction        | 0.919         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.0284        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 3.25          |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | 7.61e+09      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Step: 400, MSE: 0.003832, PSNR: 23.993404, PSNR Diff: -0.064167, Changes: 3.7708888053894043, Reward: -5.13, 06:07:15\n",
      "Step: 500, MSE: 0.003844, PSNR: 23.979738, PSNR Diff: -0.077833, Changes: 3.531048059463501, Reward: -6.23, 06:07:27\n",
      "Step: 600, MSE: 0.003857, PSNR: 23.965113, PSNR Diff: -0.092459, Changes: 5.683127403259277, Reward: -7.40, 06:07:39\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 708           |\n",
      "|    ep_rew_mean          | -2.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 821           |\n",
      "|    total_timesteps      | 4864          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3482915e+19 |\n",
      "|    clip_fraction        | 0.914         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.101        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.145        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | 2.01e+15      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.024         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100264 < -1 at step 649\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:07:57\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:07:57.515995\n",
      "Step: 100, MSE: 0.003789, PSNR: 24.042437, PSNR Diff: -0.015135, Changes: 1.8965215682983398, Reward: -1.21, 06:08:10\n",
      "Step: 200, MSE: 0.003802, PSNR: 24.027853, PSNR Diff: -0.029718, Changes: 1.6660555601119995, Reward: -2.38, 06:08:22\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 699           |\n",
      "|    ep_rew_mean          | -2.85e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 865           |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6517946e+17 |\n",
      "|    clip_fraction        | 0.913         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0208       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 810           |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | 7.02e+13      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.157         |\n",
      "-------------------------------------------\n",
      "Step: 300, MSE: 0.003816, PSNR: 24.012093, PSNR Diff: -0.045479, Changes: 1.4502402544021606, Reward: -3.64, 06:08:47\n",
      "Step: 400, MSE: 0.003828, PSNR: 23.998676, PSNR Diff: -0.058895, Changes: 2.8258907794952393, Reward: -4.71, 06:08:59\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 699          |\n",
      "|    ep_rew_mean          | -2.85e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 908          |\n",
      "|    total_timesteps      | 5376         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.035034e+18 |\n",
      "|    clip_fraction        | 0.903        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.00911      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.63e+06     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 7.24e+08     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0152       |\n",
      "------------------------------------------\n",
      "Step: 500, MSE: 0.003843, PSNR: 23.981232, PSNR Diff: -0.076340, Changes: 3.646456480026245, Reward: -6.11, 06:09:23\n",
      "Step: 600, MSE: 0.003855, PSNR: 23.967607, PSNR Diff: -0.089964, Changes: 0.8755332827568054, Reward: -7.20, 06:09:36\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100166 < -1 at step 672\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:09:44\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:09:44.404123\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 696           |\n",
      "|    ep_rew_mean          | -2.84e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 951           |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9055961e+19 |\n",
      "|    clip_fraction        | 0.919         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0612       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.134        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | 5.57e+11      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.064         |\n",
      "-------------------------------------------\n",
      "Step: 100, MSE: 0.003789, PSNR: 24.042259, PSNR Diff: -0.015312, Changes: 2.1039180755615234, Reward: -1.22, 06:10:08\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.028709, PSNR Diff: -0.028862, Changes: 1.1980466842651367, Reward: -2.31, 06:10:20\n",
      "Step: 300, MSE: 0.003814, PSNR: 24.014299, PSNR Diff: -0.043272, Changes: 3.456477165222168, Reward: -3.46, 06:10:32\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 696          |\n",
      "|    ep_rew_mean          | -2.84e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 994          |\n",
      "|    total_timesteps      | 5888         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.926872e+18 |\n",
      "|    clip_fraction        | 0.917        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | 0.0216       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.12        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 1.09e+07     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.147        |\n",
      "------------------------------------------\n",
      "Step: 400, MSE: 0.003826, PSNR: 24.000809, PSNR Diff: -0.056763, Changes: 4.250874042510986, Reward: -4.54, 06:10:56\n",
      "Step: 500, MSE: 0.003835, PSNR: 23.990210, PSNR Diff: -0.067362, Changes: 1.596592664718628, Reward: -5.39, 06:11:08\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 696          |\n",
      "|    ep_rew_mean          | -2.84e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 1036         |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.886044e+18 |\n",
      "|    clip_fraction        | 0.918        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.168       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | 1.95e+09     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0115       |\n",
      "------------------------------------------\n",
      "Step: 600, MSE: 0.003847, PSNR: 23.976431, PSNR Diff: -0.081141, Changes: 4.129583358764648, Reward: -6.49, 06:11:30\n",
      "Step: 700, MSE: 0.003859, PSNR: 23.963327, PSNR Diff: -0.094244, Changes: 2.4513463973999023, Reward: -7.54, 06:11:42\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100067 < -1 at step 740\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:11:46\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:11:46.913064\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 701           |\n",
      "|    ep_rew_mean          | -2.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 1077          |\n",
      "|    total_timesteps      | 6400          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0800006e+18 |\n",
      "|    clip_fraction        | 0.913         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0313       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.109        |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | 2.38e+09      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0908        |\n",
      "-------------------------------------------\n",
      "Step: 100, MSE: 0.003787, PSNR: 24.044771, PSNR Diff: -0.012800, Changes: 3.3713841438293457, Reward: -1.02, 06:12:10\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.029219, PSNR Diff: -0.028353, Changes: 2.5373783111572266, Reward: -2.27, 06:12:22\n",
      "Step: 300, MSE: 0.003812, PSNR: 24.016354, PSNR Diff: -0.041218, Changes: 3.2581865787506104, Reward: -3.30, 06:12:33\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 701           |\n",
      "|    ep_rew_mean          | -2.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 1118          |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8420273e+17 |\n",
      "|    clip_fraction        | 0.916         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.0201        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.133        |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.163        |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.128         |\n",
      "-------------------------------------------\n",
      "Step: 400, MSE: 0.003825, PSNR: 24.002180, PSNR Diff: -0.055391, Changes: 1.962303876876831, Reward: -4.43, 06:12:57\n",
      "Step: 500, MSE: 0.003838, PSNR: 23.986921, PSNR Diff: -0.070650, Changes: 3.2888715267181396, Reward: -5.65, 06:13:09\n",
      "Step: 600, MSE: 0.003851, PSNR: 23.972761, PSNR Diff: -0.084810, Changes: 6.630587100982666, Reward: -6.78, 06:13:20\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 701          |\n",
      "|    ep_rew_mean          | -2.87e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1160         |\n",
      "|    total_timesteps      | 6912         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.722859e+18 |\n",
      "|    clip_fraction        | 0.914        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | -0.0906      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.146       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 2.9e+12      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0123       |\n",
      "------------------------------------------\n",
      "Step: 700, MSE: 0.003862, PSNR: 23.959557, PSNR Diff: -0.098015, Changes: 4.2990899085998535, Reward: -7.84, 06:13:44\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100037 < -1 at step 710\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:13:45\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:13:45.488360\n",
      "Step: 100, MSE: 0.003788, PSNR: 24.044254, PSNR Diff: -0.013317, Changes: 3.935929775238037, Reward: -1.07, 06:13:57\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 702          |\n",
      "|    ep_rew_mean          | -2.88e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1203         |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.239789e+18 |\n",
      "|    clip_fraction        | 0.917        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.98e+06    |\n",
      "|    explained_variance   | -0.0291      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | -0.105       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 1.82e+05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.111        |\n",
      "------------------------------------------\n",
      "Step: 200, MSE: 0.003798, PSNR: 24.032166, PSNR Diff: -0.025406, Changes: 4.647790908813477, Reward: -2.03, 06:14:21\n",
      "Step: 300, MSE: 0.003813, PSNR: 24.015808, PSNR Diff: -0.041763, Changes: 4.38477897644043, Reward: -3.34, 06:14:33\n",
      "Step: 400, MSE: 0.003824, PSNR: 24.002611, PSNR Diff: -0.054960, Changes: 1.644489049911499, Reward: -4.40, 06:14:44\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 702           |\n",
      "|    ep_rew_mean          | -2.88e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 1245          |\n",
      "|    total_timesteps      | 7424          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0312468e+19 |\n",
      "|    clip_fraction        | 0.917         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.0186        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.147        |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.165        |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.081         |\n",
      "-------------------------------------------\n",
      "Step: 500, MSE: 0.003836, PSNR: 23.989439, PSNR Diff: -0.068132, Changes: 2.2282931804656982, Reward: -5.45, 06:15:09\n",
      "Step: 600, MSE: 0.003849, PSNR: 23.974758, PSNR Diff: -0.082813, Changes: 3.8328678607940674, Reward: -6.63, 06:15:21\n",
      "------------------------------------------------\n",
      "| rollout/                |                    |\n",
      "|    ep_len_mean          | 702                |\n",
      "|    ep_rew_mean          | -2.88e+03          |\n",
      "| time/                   |                    |\n",
      "|    fps                  | 5                  |\n",
      "|    iterations           | 30                 |\n",
      "|    time_elapsed         | 1289               |\n",
      "|    total_timesteps      | 7680               |\n",
      "| train/                  |                    |\n",
      "|    approx_kl            | 1221792000000000.0 |\n",
      "|    clip_fraction        | 0.917              |\n",
      "|    clip_range           | 0.2                |\n",
      "|    entropy_loss         | -2.98e+06          |\n",
      "|    explained_variance   | -0.0703            |\n",
      "|    learning_rate        | 1e-05              |\n",
      "|    loss                 | -0.164             |\n",
      "|    n_updates            | 290                |\n",
      "|    policy_gradient_loss | -0.0473            |\n",
      "|    std                  | 1                  |\n",
      "|    value_loss           | 0.0219             |\n",
      "------------------------------------------------\n",
      "Step: 700, MSE: 0.003860, PSNR: 23.961660, PSNR Diff: -0.095911, Changes: 2.704148769378662, Reward: -7.67, 06:15:45\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100002 < -1 at step 732\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:15:49\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:15:49.785190\n",
      "Step: 100, MSE: 0.003791, PSNR: 24.040508, PSNR Diff: -0.017063, Changes: 2.149456024169922, Reward: -1.37, 06:16:01\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 705           |\n",
      "|    ep_rew_mean          | -2.89e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 1331          |\n",
      "|    total_timesteps      | 7936          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8891987e+17 |\n",
      "|    clip_fraction        | 0.911         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0194       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.0928       |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | 7.11e+06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.13          |\n",
      "-------------------------------------------\n",
      "Step: 200, MSE: 0.003802, PSNR: 24.027325, PSNR Diff: -0.030247, Changes: 3.012319803237915, Reward: -2.42, 06:16:26\n",
      "Step: 300, MSE: 0.003813, PSNR: 24.014877, PSNR Diff: -0.042694, Changes: 0.1430375725030899, Reward: -3.42, 06:16:37\n",
      "Step: 400, MSE: 0.003826, PSNR: 24.000885, PSNR Diff: -0.056686, Changes: 1.4862900972366333, Reward: -4.53, 06:16:50\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 705           |\n",
      "|    ep_rew_mean          | -2.89e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 1374          |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1695494e+19 |\n",
      "|    clip_fraction        | 0.916         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | 0.0169        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.15         |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.145        |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0484        |\n",
      "-------------------------------------------\n",
      "Step: 500, MSE: 0.003838, PSNR: 23.986881, PSNR Diff: -0.070690, Changes: 4.5489935874938965, Reward: -5.66, 06:17:14\n",
      "Step: 600, MSE: 0.003851, PSNR: 23.972771, PSNR Diff: -0.084801, Changes: 1.6324392557144165, Reward: -6.78, 06:17:26\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 705           |\n",
      "|    ep_rew_mean          | -2.89e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 1417          |\n",
      "|    total_timesteps      | 8448          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3267506e+19 |\n",
      "|    clip_fraction        | 0.916         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0608       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.135        |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | 1.81e+05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0337        |\n",
      "-------------------------------------------\n",
      "Step: 700, MSE: 0.003863, PSNR: 23.959141, PSNR Diff: -0.098431, Changes: 4.3902411460876465, Reward: -7.87, 06:17:50\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100182 < -1 at step 716\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:17:52\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:17:52.790238\n",
      "Step: 100, MSE: 0.003789, PSNR: 24.043114, PSNR Diff: -0.014458, Changes: 3.1108994483947754, Reward: -1.16, 06:18:05\n",
      "Step: 200, MSE: 0.003801, PSNR: 24.029137, PSNR Diff: -0.028435, Changes: 2.5864980220794678, Reward: -2.27, 06:18:17\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 706           |\n",
      "|    ep_rew_mean          | -2.9e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 1460          |\n",
      "|    total_timesteps      | 8704          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8887488e+18 |\n",
      "|    clip_fraction        | 0.911         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0117       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | -0.0735       |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | 3.05e+12      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.162         |\n",
      "-------------------------------------------\n",
      "Step: 300, MSE: 0.003812, PSNR: 24.016499, PSNR Diff: -0.041073, Changes: 2.2298388481140137, Reward: -3.29, 06:18:39\n",
      "Step: 400, MSE: 0.003823, PSNR: 24.004150, PSNR Diff: -0.053421, Changes: 1.867809534072876, Reward: -4.27, 06:18:52\n",
      "------------------------------------------------\n",
      "| rollout/                |                    |\n",
      "|    ep_len_mean          | 706                |\n",
      "|    ep_rew_mean          | -2.9e+03           |\n",
      "| time/                   |                    |\n",
      "|    fps                  | 5                  |\n",
      "|    iterations           | 35                 |\n",
      "|    time_elapsed         | 1503               |\n",
      "|    total_timesteps      | 8960               |\n",
      "| train/                  |                    |\n",
      "|    approx_kl            | 4442172700000000.0 |\n",
      "|    clip_fraction        | 0.909              |\n",
      "|    clip_range           | 0.2                |\n",
      "|    entropy_loss         | -2.98e+06          |\n",
      "|    explained_variance   | -0.0106            |\n",
      "|    learning_rate        | 1e-05              |\n",
      "|    loss                 | -0.115             |\n",
      "|    n_updates            | 340                |\n",
      "|    policy_gradient_loss | 1.9e+09            |\n",
      "|    std                  | 1                  |\n",
      "|    value_loss           | 0.00677            |\n",
      "------------------------------------------------\n",
      "Step: 500, MSE: 0.003836, PSNR: 23.989744, PSNR Diff: -0.067827, Changes: 2.077235221862793, Reward: -5.43, 06:19:16\n",
      "Step: 600, MSE: 0.003847, PSNR: 23.976608, PSNR Diff: -0.080963, Changes: 1.7289738655090332, Reward: -6.48, 06:19:28\n",
      "Step: 700, MSE: 0.003859, PSNR: 23.963049, PSNR Diff: -0.094522, Changes: 3.3144283294677734, Reward: -7.56, 06:19:39\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.100296 < -1 at step 739\u001b[0m\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 06:19:44\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.003776, Initial PSNR: 24.057571, 2024-12-24 06:19:44.679756\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 708           |\n",
      "|    ep_rew_mean          | -2.92e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 1544          |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4735294e+17 |\n",
      "|    clip_fraction        | 0.911         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.98e+06     |\n",
      "|    explained_variance   | -0.0478       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 1.8e+03       |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | 46.3          |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0466        |\n",
      "-------------------------------------------\n",
      "Step: 100, MSE: 0.003789, PSNR: 24.042866, PSNR Diff: -0.014706, Changes: 4.624006748199463, Reward: -1.18, 06:20:08\n",
      "Step: 200, MSE: 0.003800, PSNR: 24.029982, PSNR Diff: -0.027590, Changes: 1.7302477359771729, Reward: -2.21, 06:20:20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 512, 512).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(512)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((512, 512))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 512 or target.shape[-2] < 512:\n",
    "            target = torchvision.transforms.Resize(512)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, max_allowed_changes=10):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        max_allowed_changes: 한 번에 조작할 수 있는 최대 픽셀 수.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 128, 128)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.int8)\n",
    "                                       \n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.max_allowed_changes = max_allowed_changes  # 한 번에 조작할 수 있는 최대 픽셀 수\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "        \n",
    "        # 추가 변수: 연속 실패 횟수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        데이터셋에서 새로운 이미지를 가져오고 초기 상태를 설정합니다.\n",
    "        - 데이터셋의 다음 이미지를 불러옵니다. \n",
    "        - BinaryNet을 사용해 초기 관찰값을 생성합니다.\n",
    "        - 초기 상태(state)는 관찰값을 이진화한 결과입니다.\n",
    "        - 초기 PSNR과 MSE를 계산하고 출력합니다.\n",
    "        - 실패 시 이전 데이터를 다시 불러옵니다.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): 랜덤 시드 값. Default는 None.\n",
    "            options (dict, optional): 추가 옵션. Default는 None.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            z (float): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 모델로 초기 관찰값 생성\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # 관찰값을 numpy 배열로 변환\n",
    "\n",
    "        # 초기 상태는 이진화된 값으로 설정\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  # 상태를 Torch 텐서로 변환\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # 메타 정보 추가\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # 초기 MSE와 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 값 출력\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        # 시뮬레이션 결과를 별도로 저장\n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 관찰값(초기 모델 출력)을 반환\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        - 관찰값이 0~0.2인 경우 행동 0으로 고정.\n",
    "        - 관찰값이 0.8~1인 경우 행동 1로 고정.\n",
    "        - 최대 변경 가능 픽셀 수 제한.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): 관찰값.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 행동 마스크.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "\n",
    "        # 허용된 변경 수를 강제 적용\n",
    "        allowed_indices = np.where(mask.flatten() == 1)[0]\n",
    "        if len(allowed_indices) > self.max_allowed_changes:\n",
    "            # 초과 변경을 방지하도록 고정된 수의 픽셀만 선택 가능\n",
    "            selected_indices = np.random.choice(allowed_indices, self.max_allowed_changes, replace=False)\n",
    "            mask = np.zeros_like(mask.flatten())\n",
    "            mask[selected_indices] = 1\n",
    "            mask = mask.reshape(observation.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경의 한 타임스텝을 진행합니다.\n",
    "        - 주어진 행동(action)을 적용하고, 새로운 상태를 계산합니다.\n",
    "        - 관찰값은 원래 관찰값을 유지합니다.\n",
    "        - MSE와 PSNR 계산 후 보상을 반환합니다.\n",
    "        - 실패 시 현재 데이터셋 반복을 활성화합니다.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): 에이전트가 수행한 행동.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 새로운 관찰값.\n",
    "            float: 보상 값.\n",
    "            bool: 종료 여부.\n",
    "            bool: Truncated 여부.\n",
    "            dict: 추가 정보 (MSE, PSNR, 행동 마스크 등).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        # 행동에 마스크 강제 적용\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        masked_action = action * mask\n",
    "\n",
    "        # 조작 픽셀 수 확인\n",
    "        num_changes = np.sum(masked_action)\n",
    "        reward = 0\n",
    "\n",
    "        if num_changes > self.max_allowed_changes:\n",
    "            reward -= 50\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, masked_action).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 PSNR과의 차이 계산\n",
    "        psnr_diff = psnr - self.initial_psnr\n",
    "\n",
    "        # 실패 조건 확인\n",
    "        if psnr_diff < -0.1:\n",
    "            print(f\"\\033[91mEpisode failed: PSNR Diff {psnr_diff:.6f} < -1 at step {self.steps}\\033[0m\")\n",
    "            self.retry_current_target = True  # 실패 시 반복 플래그 활성화\n",
    "            return self.observation, -100.0, True, False, {\"mse\": mse, \"psnr\": psnr, \"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        # 보상 계산\n",
    "        reward += psnr_diff * 80\n",
    "        reward -= 0.1 * num_changes if num_changes > self.max_allowed_changes else 0\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, PSNR Diff: {psnr_diff:.6f}, \"\n",
    "                  f\"Changes: {num_changes}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        # 상태 업데이트 (self.state만 업데이트)\n",
    "        self.state = new_state\n",
    "\n",
    "        # 관찰값 유지 (초기 관찰값 유지)\n",
    "        observation = self.observation  # 관찰값은 초기 값 유지\n",
    "\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        mask = self.create_action_mask(observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"psnr_diff\": psnr_diff, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,  # 학습률 감소\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping 추가\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO 정책 네트워크 구성\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])]  # 더 복잡한 네트워크 구조\n",
    "#)\n",
    "\n",
    "# PPO 모델 초기화\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",  # LSTM 정책 대신 기본 MLP 정책\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "# 학습\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO 학습\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "#from sb3_contrib import RecurrentPPO\n",
    "\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  # 더 복잡한 네트워크 구조\n",
    "#    lstm_hidden_size=128,  # LSTM 크기 유지\n",
    "#    shared_lstm=False  # 별도 LSTM 사용\n",
    "#)\n",
    "\n",
    "#ppo_model = RecurrentPPO(\n",
    "#    \"MlpLstmPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "\n",
    "# 학습\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
