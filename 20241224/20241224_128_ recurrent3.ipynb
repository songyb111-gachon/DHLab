{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 128, 128])\n",
      "Using cuda device\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:09:41\n",
      "Logging to ./ppo_with_mask/RecurrentPPO_61\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:09:41.243851\n",
      "Step: 100, MSE: 0.008170, PSNR: 19.314558, PSNR Diff: -0.283794, Changes: 4, Reward: -1.99, 05:09:42\n",
      "Step: 200, MSE: 0.008738, PSNR: 19.022728, PSNR Diff: -0.575624, Changes: 3, Reward: -4.03, 05:09:42\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 93  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 2   |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "Step: 300, MSE: 0.009215, PSNR: 18.792053, PSNR Diff: -0.806299, Changes: 2, Reward: -5.64, 05:09:52\n",
      "Episode failed: PSNR Diff -1.004446 < -1 at step 373\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:09:53\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:09:53.305300\n",
      "Step: 100, MSE: 0.008085, PSNR: 19.360218, PSNR Diff: -0.238134, Changes: 5, Reward: -1.67, 05:09:54\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 374       |\n",
      "|    ep_rew_mean          | -1.44e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 37        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 512       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.429002  |\n",
      "|    clip_fraction        | 0.83      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.000462  |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 1.49      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.0852    |\n",
      "|    value_loss           | 5.12      |\n",
      "---------------------------------------\n",
      "Step: 200, MSE: 0.008598, PSNR: 19.092730, PSNR Diff: -0.505623, Changes: 5, Reward: -3.54, 05:10:03\n",
      "Step: 300, MSE: 0.009147, PSNR: 18.823917, PSNR Diff: -0.774435, Changes: 5, Reward: -5.42, 05:10:04\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 374       |\n",
      "|    ep_rew_mean          | -1.44e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 31        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 24        |\n",
      "|    total_timesteps      | 768       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3037953 |\n",
      "|    clip_fraction        | 0.753     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0114    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.017    |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.0181    |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "Step: 400, MSE: 0.009538, PSNR: 18.642399, PSNR Diff: -0.955954, Changes: 5, Reward: -6.69, 05:10:14\n",
      "Episode failed: PSNR Diff -1.004219 < -1 at step 416\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:10:14\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:10:14.145629\n",
      "Step: 100, MSE: 0.008142, PSNR: 19.329721, PSNR Diff: -0.268631, Changes: 4, Reward: -1.88, 05:10:14\n",
      "Step: 200, MSE: 0.008555, PSNR: 19.114563, PSNR Diff: -0.483789, Changes: 8, Reward: -3.39, 05:10:15\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 396        |\n",
      "|    ep_rew_mean          | -1.52e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 1024       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50262403 |\n",
      "|    clip_fraction        | 0.72       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.00597   |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.248      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0711     |\n",
      "|    value_loss           | 0.222      |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.009077, PSNR: 18.857559, PSNR Diff: -0.740793, Changes: 6, Reward: -5.19, 05:10:24\n",
      "Step: 400, MSE: 0.009515, PSNR: 18.652557, PSNR Diff: -0.945795, Changes: 3, Reward: -6.62, 05:10:25\n",
      "Episode failed: PSNR Diff -1.002094 < -1 at step 426\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:10:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:10:25.834348\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 406       |\n",
      "|    ep_rew_mean          | -1.56e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 28        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 45        |\n",
      "|    total_timesteps      | 1280      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6660333 |\n",
      "|    clip_fraction        | 0.736     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0169    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 0.417     |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.00639  |\n",
      "|    value_loss           | 0.0544    |\n",
      "---------------------------------------\n",
      "Step: 100, MSE: 0.008246, PSNR: 19.274193, PSNR Diff: -0.324160, Changes: 4, Reward: -2.27, 05:10:35\n",
      "Step: 200, MSE: 0.008755, PSNR: 19.014053, PSNR Diff: -0.584299, Changes: 8, Reward: -4.09, 05:10:35\n",
      "Step: 300, MSE: 0.009149, PSNR: 18.823269, PSNR Diff: -0.775084, Changes: 3, Reward: -5.43, 05:10:36\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.56e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 1536       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32450444 |\n",
      "|    clip_fraction        | 0.773      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0206     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.253      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0053    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Episode failed: PSNR Diff -1.002253 < -1 at step 397\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:10:46\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:10:46.402771\n",
      "Step: 100, MSE: 0.008121, PSNR: 19.340866, PSNR Diff: -0.257486, Changes: 5, Reward: -1.80, 05:10:47\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 404       |\n",
      "|    ep_rew_mean          | -1.58e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 26        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 66        |\n",
      "|    total_timesteps      | 1792      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3949629 |\n",
      "|    clip_fraction        | 0.701     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | -0.0023   |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 0.461     |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.00194   |\n",
      "|    value_loss           | 0.111     |\n",
      "---------------------------------------\n",
      "Step: 200, MSE: 0.008623, PSNR: 19.080450, PSNR Diff: -0.517902, Changes: 5, Reward: -3.63, 05:10:56\n",
      "Step: 300, MSE: 0.009155, PSNR: 18.820059, PSNR Diff: -0.778294, Changes: 6, Reward: -5.45, 05:10:57\n",
      "Step: 400, MSE: 0.009617, PSNR: 18.606384, PSNR Diff: -0.991968, Changes: 6, Reward: -6.94, 05:10:57\n",
      "Episode failed: PSNR Diff -1.000652 < -1 at step 407\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:10:57\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:10:57.935165\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 405        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 77         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40444627 |\n",
      "|    clip_fraction        | 0.73       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0179     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.0858    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    value_loss           | 0.0891     |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008215, PSNR: 19.290478, PSNR Diff: -0.307875, Changes: 5, Reward: -2.16, 05:11:07\n",
      "Step: 200, MSE: 0.008577, PSNR: 19.103453, PSNR Diff: -0.494900, Changes: 5, Reward: -3.46, 05:11:08\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 405       |\n",
      "|    ep_rew_mean          | -1.58e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 26        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 88        |\n",
      "|    total_timesteps      | 2304      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2592336 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0602    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 0.186     |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.0111    |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "Step: 300, MSE: 0.009097, PSNR: 18.847691, PSNR Diff: -0.750662, Changes: 4, Reward: -5.25, 05:11:17\n",
      "Episode failed: PSNR Diff -1.001394 < -1 at step 396\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:11:18\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:11:18.622575\n",
      "Step: 100, MSE: 0.008142, PSNR: 19.329445, PSNR Diff: -0.268908, Changes: 5, Reward: -1.88, 05:11:19\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 404        |\n",
      "|    ep_rew_mean          | -1.57e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21369833 |\n",
      "|    clip_fraction        | 0.699      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.00179   |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00957   |\n",
      "|    value_loss           | 0.0475     |\n",
      "----------------------------------------\n",
      "Step: 200, MSE: 0.008660, PSNR: 19.061800, PSNR Diff: -0.536552, Changes: 6, Reward: -3.76, 05:11:28\n",
      "Step: 300, MSE: 0.009214, PSNR: 18.792393, PSNR Diff: -0.805960, Changes: 6, Reward: -5.64, 05:11:29\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 404       |\n",
      "|    ep_rew_mean          | -1.57e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 109       |\n",
      "|    total_timesteps      | 2816      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.400906  |\n",
      "|    clip_fraction        | 0.702     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 0.14      |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.00248  |\n",
      "|    value_loss           | 0.106     |\n",
      "---------------------------------------\n",
      "Episode failed: PSNR Diff -1.004656 < -1 at step 397\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:11:38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:11:38.801637\n",
      "Step: 100, MSE: 0.008088, PSNR: 19.358213, PSNR Diff: -0.240139, Changes: 5, Reward: -1.68, 05:11:39\n",
      "Step: 200, MSE: 0.008636, PSNR: 19.073790, PSNR Diff: -0.524563, Changes: 4, Reward: -3.67, 05:11:40\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 403        |\n",
      "|    ep_rew_mean          | -1.57e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 119        |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15206176 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.000195  |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.169      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.0151     |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.009103, PSNR: 18.844942, PSNR Diff: -0.753410, Changes: 6, Reward: -5.27, 05:11:49\n",
      "Step: 400, MSE: 0.009611, PSNR: 18.609116, PSNR Diff: -0.989237, Changes: 4, Reward: -6.92, 05:11:50\n",
      "Episode failed: PSNR Diff -1.003063 < -1 at step 410\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:11:50\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:11:50.657130\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 404       |\n",
      "|    ep_rew_mean          | -1.57e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 130       |\n",
      "|    total_timesteps      | 3328      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.175115  |\n",
      "|    clip_fraction        | 0.65      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0545    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.00449  |\n",
      "|    value_loss           | 0.0299    |\n",
      "---------------------------------------\n",
      "Step: 100, MSE: 0.008170, PSNR: 19.314659, PSNR Diff: -0.283693, Changes: 4, Reward: -1.99, 05:11:59\n",
      "Step: 200, MSE: 0.008676, PSNR: 19.053535, PSNR Diff: -0.544817, Changes: 2, Reward: -3.81, 05:12:00\n",
      "Step: 300, MSE: 0.009289, PSNR: 18.756939, PSNR Diff: -0.841413, Changes: 9, Reward: -5.89, 05:12:01\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 404        |\n",
      "|    ep_rew_mean          | -1.57e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16525985 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.033      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.287      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Episode failed: PSNR Diff -1.000227 < -1 at step 394\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:12:11\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:12:11.019888\n",
      "Step: 100, MSE: 0.008178, PSNR: 19.310356, PSNR Diff: -0.287996, Changes: 5, Reward: -2.02, 05:12:11\n",
      "Step: 200, MSE: 0.008606, PSNR: 19.089005, PSNR Diff: -0.509348, Changes: 3, Reward: -3.57, 05:12:12\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 403        |\n",
      "|    ep_rew_mean          | -1.57e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 151        |\n",
      "|    total_timesteps      | 3840       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14390309 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.000306  |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00236   |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.009054, PSNR: 18.868238, PSNR Diff: -0.730114, Changes: 7, Reward: -5.11, 05:12:21\n",
      "Step: 400, MSE: 0.009572, PSNR: 18.626631, PSNR Diff: -0.971722, Changes: 4, Reward: -6.80, 05:12:22\n",
      "Episode failed: PSNR Diff -1.000359 < -1 at step 412\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:12:22\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:12:22.548512\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 404        |\n",
      "|    ep_rew_mean          | -1.57e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 162        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17089939 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0456     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.0821    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.0335     |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008118, PSNR: 19.342075, PSNR Diff: -0.256277, Changes: 6, Reward: -1.79, 05:12:31\n",
      "Step: 200, MSE: 0.008594, PSNR: 19.094751, PSNR Diff: -0.503601, Changes: 7, Reward: -3.53, 05:12:32\n",
      "Step: 300, MSE: 0.009059, PSNR: 18.866032, PSNR Diff: -0.732321, Changes: 5, Reward: -5.13, 05:12:33\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 404       |\n",
      "|    ep_rew_mean          | -1.57e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 25        |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 172       |\n",
      "|    total_timesteps      | 4352      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1443781 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.09e+04 |\n",
      "|    explained_variance   | 0.0542    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | -0.169    |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0434   |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "Step: 400, MSE: 0.009531, PSNR: 18.645323, PSNR Diff: -0.953030, Changes: 4, Reward: -6.67, 05:12:42\n",
      "Episode failed: PSNR Diff -1.003721 < -1 at step 426\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:12:42\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:12:42.862984\n",
      "Step: 100, MSE: 0.008189, PSNR: 19.304426, PSNR Diff: -0.293926, Changes: 3, Reward: -2.06, 05:12:43\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 183        |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10253909 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00106    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.00097    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    value_loss           | 0.0412     |\n",
      "----------------------------------------\n",
      "Step: 200, MSE: 0.008606, PSNR: 19.088789, PSNR Diff: -0.509563, Changes: 1, Reward: -3.57, 05:12:52\n",
      "Step: 300, MSE: 0.009121, PSNR: 18.836586, PSNR Diff: -0.761766, Changes: 3, Reward: -5.33, 05:12:53\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 4864       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13509107 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0341     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.15      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.0791     |\n",
      "----------------------------------------\n",
      "Step: 400, MSE: 0.009560, PSNR: 18.632053, PSNR Diff: -0.966299, Changes: 5, Reward: -6.76, 05:13:03\n",
      "Episode failed: PSNR Diff -1.000622 < -1 at step 414\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:13:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:13:03.182300\n",
      "Step: 100, MSE: 0.008127, PSNR: 19.337423, PSNR Diff: -0.260929, Changes: 5, Reward: -1.83, 05:13:03\n",
      "Step: 200, MSE: 0.008595, PSNR: 19.094404, PSNR Diff: -0.503948, Changes: 3, Reward: -3.53, 05:13:04\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 407        |\n",
      "|    ep_rew_mean          | -1.59e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 204        |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10636118 |\n",
      "|    clip_fraction        | 0.457      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00126    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.143     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.009198, PSNR: 18.799765, PSNR Diff: -0.798588, Changes: 3, Reward: -5.59, 05:13:14\n",
      "Episode failed: PSNR Diff -1.005192 < -1 at step 397\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:13:14\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:13:14.950146\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 5376       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11348553 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0518     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0697     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.013      |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008052, PSNR: 19.377533, PSNR Diff: -0.220819, Changes: 3, Reward: -1.55, 05:13:24\n",
      "Step: 200, MSE: 0.008628, PSNR: 19.077724, PSNR Diff: -0.520628, Changes: 2, Reward: -3.64, 05:13:25\n",
      "Step: 300, MSE: 0.009062, PSNR: 18.864368, PSNR Diff: -0.733984, Changes: 6, Reward: -5.14, 05:13:25\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 225        |\n",
      "|    total_timesteps      | 5632       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13708107 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.04       |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0492     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0615    |\n",
      "|    value_loss           | 0.0905     |\n",
      "----------------------------------------\n",
      "Step: 400, MSE: 0.009555, PSNR: 18.634495, PSNR Diff: -0.963858, Changes: 7, Reward: -6.75, 05:13:35\n",
      "Episode failed: PSNR Diff -1.004751 < -1 at step 431\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:13:35\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:13:35.733151\n",
      "Step: 100, MSE: 0.008134, PSNR: 19.333630, PSNR Diff: -0.264723, Changes: 6, Reward: -1.85, 05:13:36\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | -1.59e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 236        |\n",
      "|    total_timesteps      | 5888       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07982375 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00215    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.127     |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    value_loss           | 0.0507     |\n",
      "----------------------------------------\n",
      "Step: 200, MSE: 0.008721, PSNR: 19.031094, PSNR Diff: -0.567259, Changes: 6, Reward: -3.97, 05:13:45\n",
      "Step: 300, MSE: 0.009232, PSNR: 18.783712, PSNR Diff: -0.814640, Changes: 3, Reward: -5.70, 05:13:46\n",
      "Episode failed: PSNR Diff -1.001404 < -1 at step 389\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:13:46\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:13:46.828263\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 407        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17642494 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0304     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.301      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0728    |\n",
      "|    value_loss           | 0.0431     |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008084, PSNR: 19.360788, PSNR Diff: -0.237564, Changes: 5, Reward: -1.66, 05:13:55\n",
      "Step: 200, MSE: 0.008561, PSNR: 19.111536, PSNR Diff: -0.486816, Changes: 6, Reward: -3.41, 05:13:56\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 407        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 256        |\n",
      "|    total_timesteps      | 6400       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08727275 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0748     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.006      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.038     |\n",
      "|    value_loss           | 0.077      |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.008996, PSNR: 18.896351, PSNR Diff: -0.702002, Changes: 4, Reward: -4.91, 05:14:06\n",
      "Step: 400, MSE: 0.009523, PSNR: 18.648945, PSNR Diff: -0.949408, Changes: 5, Reward: -6.65, 05:14:07\n",
      "Episode failed: PSNR Diff -1.002678 < -1 at step 432\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:14:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:14:07.342851\n",
      "Step: 100, MSE: 0.008102, PSNR: 19.350933, PSNR Diff: -0.247419, Changes: 7, Reward: -1.73, 05:14:08\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | -1.59e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 267        |\n",
      "|    total_timesteps      | 6656       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06991527 |\n",
      "|    clip_fraction        | 0.494      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00418    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.0643    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    value_loss           | 0.0186     |\n",
      "----------------------------------------\n",
      "Step: 200, MSE: 0.008610, PSNR: 19.086630, PSNR Diff: -0.511723, Changes: 6, Reward: -3.58, 05:14:17\n",
      "Step: 300, MSE: 0.009099, PSNR: 18.846863, PSNR Diff: -0.751490, Changes: 4, Reward: -5.26, 05:14:18\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | -1.59e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 278        |\n",
      "|    total_timesteps      | 6912       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07452223 |\n",
      "|    clip_fraction        | 0.488      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0305     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.1       |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    value_loss           | 0.0628     |\n",
      "----------------------------------------\n",
      "Episode failed: PSNR Diff -1.000835 < -1 at step 385\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:14:28\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:14:28.053802\n",
      "Step: 100, MSE: 0.008156, PSNR: 19.321894, PSNR Diff: -0.276459, Changes: 6, Reward: -1.94, 05:14:28\n",
      "Step: 200, MSE: 0.008604, PSNR: 19.089554, PSNR Diff: -0.508799, Changes: 7, Reward: -3.56, 05:14:29\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 407        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 289        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07188049 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.00241    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.0681    |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    value_loss           | 0.0532     |\n",
      "----------------------------------------\n",
      "Step: 300, MSE: 0.009100, PSNR: 18.846300, PSNR Diff: -0.752052, Changes: 5, Reward: -5.26, 05:14:39\n",
      "Step: 400, MSE: 0.009535, PSNR: 18.643639, PSNR Diff: -0.954714, Changes: 7, Reward: -6.68, 05:14:39\n",
      "Episode failed: PSNR Diff -1.000399 < -1 at step 420\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:14:40\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:14:40.048189\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 408         |\n",
      "|    ep_rew_mean          | -1.58e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 7424        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108551696 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.00775     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 0.0182      |\n",
      "-----------------------------------------\n",
      "Step: 100, MSE: 0.008244, PSNR: 19.275442, PSNR Diff: -0.322910, Changes: 4, Reward: -2.26, 05:14:49\n",
      "Step: 200, MSE: 0.008723, PSNR: 19.029907, PSNR Diff: -0.568445, Changes: 5, Reward: -3.98, 05:14:50\n",
      "Step: 300, MSE: 0.009181, PSNR: 18.807762, PSNR Diff: -0.790590, Changes: 4, Reward: -5.53, 05:14:51\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 310        |\n",
      "|    total_timesteps      | 7680       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07364619 |\n",
      "|    clip_fraction        | 0.477      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0436     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.062     |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    value_loss           | 0.0737     |\n",
      "----------------------------------------\n",
      "Step: 400, MSE: 0.009601, PSNR: 18.613815, PSNR Diff: -0.984537, Changes: 5, Reward: -6.89, 05:15:01\n",
      "Episode failed: PSNR Diff -1.000799 < -1 at step 409\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:15:01\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:15:01.171053\n",
      "Step: 100, MSE: 0.008141, PSNR: 19.330204, PSNR Diff: -0.268148, Changes: 7, Reward: -1.88, 05:15:01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 408         |\n",
      "|    ep_rew_mean          | -1.59e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 7936        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114063255 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.002       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 0.0315      |\n",
      "-----------------------------------------\n",
      "Step: 200, MSE: 0.008774, PSNR: 19.005020, PSNR Diff: -0.593332, Changes: 6, Reward: -4.15, 05:15:10\n",
      "Step: 300, MSE: 0.009297, PSNR: 18.753222, PSNR Diff: -0.845131, Changes: 5, Reward: -5.92, 05:15:11\n",
      "Episode failed: PSNR Diff -1.000465 < -1 at step 365\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:15:12\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:15:12.064213\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 331        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08930824 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0103    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.00314    |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0713    |\n",
      "|    value_loss           | 0.0394     |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008135, PSNR: 19.333241, PSNR Diff: -0.265112, Changes: 6, Reward: -1.86, 05:15:21\n",
      "Step: 200, MSE: 0.008614, PSNR: 19.084875, PSNR Diff: -0.513477, Changes: 3, Reward: -3.59, 05:15:21\n",
      "Step: 300, MSE: 0.009146, PSNR: 18.824373, PSNR Diff: -0.773979, Changes: 5, Reward: -5.42, 05:15:22\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 406         |\n",
      "|    ep_rew_mean          | -1.58e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 342         |\n",
      "|    total_timesteps      | 8448        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068096206 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 0.0651      |\n",
      "-----------------------------------------\n",
      "Step: 400, MSE: 0.009557, PSNR: 18.633636, PSNR Diff: -0.964716, Changes: 6, Reward: -6.75, 05:15:32\n",
      "Episode failed: PSNR Diff -1.003584 < -1 at step 417\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:15:32\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:15:32.332086\n",
      "Step: 100, MSE: 0.008157, PSNR: 19.321650, PSNR Diff: -0.276703, Changes: 6, Reward: -1.94, 05:15:33\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 406         |\n",
      "|    ep_rew_mean          | -1.58e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061335023 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.00088     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0224      |\n",
      "-----------------------------------------\n",
      "Step: 200, MSE: 0.008730, PSNR: 19.026434, PSNR Diff: -0.571918, Changes: 7, Reward: -4.00, 05:15:42\n",
      "Step: 300, MSE: 0.009119, PSNR: 18.837326, PSNR Diff: -0.761026, Changes: 9, Reward: -5.33, 05:15:43\n",
      "Step: 400, MSE: 0.009512, PSNR: 18.653936, PSNR Diff: -0.944416, Changes: 2, Reward: -6.61, 05:15:43\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 406        |\n",
      "|    ep_rew_mean          | -1.58e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 363        |\n",
      "|    total_timesteps      | 8960       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06123301 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0071    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | -0.102     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.0482     |\n",
      "----------------------------------------\n",
      "Episode failed: PSNR Diff -1.000937 < -1 at step 426\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:15:52\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:15:52.822791\n",
      "Step: 100, MSE: 0.008115, PSNR: 19.343975, PSNR Diff: -0.254377, Changes: 4, Reward: -1.78, 05:15:53\n",
      "Step: 200, MSE: 0.008565, PSNR: 19.109303, PSNR Diff: -0.489050, Changes: 3, Reward: -3.42, 05:15:54\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 407         |\n",
      "|    ep_rew_mean          | -1.59e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064497754 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | -0.000388   |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.159      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 0.0607      |\n",
      "-----------------------------------------\n",
      "Step: 300, MSE: 0.009065, PSNR: 18.863131, PSNR Diff: -0.735222, Changes: 5, Reward: -5.15, 05:16:04\n",
      "Step: 400, MSE: 0.009440, PSNR: 18.687035, PSNR Diff: -0.911318, Changes: 4, Reward: -6.38, 05:16:04\n",
      "Episode failed: PSNR Diff -1.003159 < -1 at step 451\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:16:05\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:16:05.051444\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 409         |\n",
      "|    ep_rew_mean          | -1.6e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 384         |\n",
      "|    total_timesteps      | 9472        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055185594 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | -0.068      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.022      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.0206      |\n",
      "-----------------------------------------\n",
      "Step: 100, MSE: 0.008168, PSNR: 19.315851, PSNR Diff: -0.282501, Changes: 5, Reward: -1.98, 05:16:14\n",
      "Step: 200, MSE: 0.008619, PSNR: 19.082180, PSNR Diff: -0.516172, Changes: 4, Reward: -3.61, 05:16:15\n",
      "Step: 300, MSE: 0.009050, PSNR: 18.870529, PSNR Diff: -0.727823, Changes: 3, Reward: -5.09, 05:16:16\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 409        |\n",
      "|    ep_rew_mean          | -1.6e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 395        |\n",
      "|    total_timesteps      | 9728       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05712311 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | 0.0358     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0322     |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.0777     |\n",
      "----------------------------------------\n",
      "Episode failed: PSNR Diff -1.002373 < -1 at step 400\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:16:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:16:25.996256\n",
      "Step: 100, MSE: 0.008218, PSNR: 19.289021, PSNR Diff: -0.309332, Changes: 8, Reward: -2.17, 05:16:26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 409         |\n",
      "|    ep_rew_mean          | -1.59e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 9984        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053323667 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.0016      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | -0.113      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    value_loss           | 0.011       |\n",
      "-----------------------------------------\n",
      "Step: 200, MSE: 0.008640, PSNR: 19.071648, PSNR Diff: -0.526705, Changes: 6, Reward: -3.69, 05:16:36\n",
      "Step: 300, MSE: 0.009108, PSNR: 18.842491, PSNR Diff: -0.755861, Changes: 6, Reward: -5.29, 05:16:36\n",
      "Step: 400, MSE: 0.009616, PSNR: 18.606966, PSNR Diff: -0.991386, Changes: 6, Reward: -6.94, 05:16:37\n",
      "Episode failed: PSNR Diff -1.001261 < -1 at step 405\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 05:16:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007653, Initial PSNR: 19.598352, 2024-12-24 05:16:37.662433\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 409        |\n",
      "|    ep_rew_mean          | -1.59e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 417        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06822668 |\n",
      "|    clip_fraction        | 0.406      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.09e+04  |\n",
      "|    explained_variance   | -0.0205    |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.192      |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.0531     |\n",
      "----------------------------------------\n",
      "Step: 100, MSE: 0.008077, PSNR: 19.364210, PSNR Diff: -0.234142, Changes: 4, Reward: -1.64, 05:16:46\n",
      "Step: 200, MSE: 0.008500, PSNR: 19.142548, PSNR Diff: -0.455805, Changes: 3, Reward: -3.19, 05:16:47\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 409         |\n",
      "|    ep_rew_mean          | -1.59e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 10496       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061593182 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.09e+04   |\n",
      "|    explained_variance   | 0.0256      |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.000588    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.0625      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 128, 128).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(128)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((128, 128))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 128 or target.shape[-2] < 128:\n",
    "            target = torchvision.transforms.Resize(128)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, max_allowed_changes=10):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        max_allowed_changes: 한 번에 조작할 수 있는 최대 픽셀 수.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 128, 128)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 128, 128), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.MultiBinary(1 * 8 * 128 * 128)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.max_allowed_changes = max_allowed_changes  # 한 번에 조작할 수 있는 최대 픽셀 수\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        데이터셋에서 새로운 이미지를 가져오고 초기 상태를 설정합니다.\n",
    "        - 데이터셋의 다음 이미지를 불러옵니다. \n",
    "        - BinaryNet을 사용해 초기 관찰값을 생성합니다.\n",
    "        - 초기 상태(state)는 관찰값을 이진화한 결과입니다.\n",
    "        - 초기 PSNR과 MSE를 계산하고 출력합니다.\n",
    "        - 실패 시 이전 데이터를 다시 불러옵니다.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): 랜덤 시드 값. Default는 None.\n",
    "            options (dict, optional): 추가 옵션. Default는 None.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            z (float): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 모델로 초기 관찰값 생성\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # 관찰값을 numpy 배열로 변환\n",
    "\n",
    "        # 초기 상태는 이진화된 값으로 설정\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  # 상태를 Torch 텐서로 변환\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # 메타 정보 추가\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # 초기 MSE와 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 값 출력\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        # 시뮬레이션 결과를 별도로 저장\n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 관찰값(초기 모델 출력)을 반환\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        - 관찰값이 0~0.2인 경우 행동 0으로 고정.\n",
    "        - 관찰값이 0.8~1인 경우 행동 1로 고정.\n",
    "        - 최대 변경 가능 픽셀 수 제한.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): 관찰값.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 행동 마스크.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "\n",
    "        # 허용된 변경 수를 강제 적용\n",
    "        allowed_indices = np.where(mask.flatten() == 1)[0]\n",
    "        if len(allowed_indices) > self.max_allowed_changes:\n",
    "            # 초과 변경을 방지하도록 고정된 수의 픽셀만 선택 가능\n",
    "            selected_indices = np.random.choice(allowed_indices, self.max_allowed_changes, replace=False)\n",
    "            mask = np.zeros_like(mask.flatten())\n",
    "            mask[selected_indices] = 1\n",
    "            mask = mask.reshape(observation.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경의 한 타임스텝을 진행합니다.\n",
    "        - 주어진 행동(action)을 적용하고, 새로운 상태를 계산합니다.\n",
    "        - 관찰값은 원래 관찰값을 유지합니다.\n",
    "        - MSE와 PSNR 계산 후 보상을 반환합니다.\n",
    "        - 실패 시 현재 데이터셋 반복을 활성화합니다.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): 에이전트가 수행한 행동.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 새로운 관찰값.\n",
    "            float: 보상 값.\n",
    "            bool: 종료 여부.\n",
    "            bool: Truncated 여부.\n",
    "            dict: 추가 정보 (MSE, PSNR, 행동 마스크 등).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        action = np.reshape(action, (1, 8, 128, 128)).astype(np.int8)\n",
    "\n",
    "        # 행동에 마스크 강제 적용\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        masked_action = action * mask\n",
    "\n",
    "        # 조작 픽셀 수 확인\n",
    "        num_changes = np.sum(masked_action)\n",
    "        reward = 0\n",
    "\n",
    "        if num_changes > self.max_allowed_changes:\n",
    "            reward -= 50\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, masked_action).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 PSNR과의 차이 계산\n",
    "        psnr_diff = psnr - self.initial_psnr\n",
    "\n",
    "        # 실패 조건 확인\n",
    "        if psnr_diff < -1:\n",
    "            print(f\"Episode failed: PSNR Diff {psnr_diff:.6f} < -1 at step {self.steps}\")\n",
    "            self.retry_current_target = True  # 실패 시 반복 플래그 활성화\n",
    "            return self.observation, -100.0, True, False, {\"mse\": mse, \"psnr\": psnr, \"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        # 보상 계산\n",
    "        reward += psnr_diff * 7\n",
    "        reward -= 0.1 * num_changes if num_changes > self.max_allowed_changes else 0\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, PSNR Diff: {psnr_diff:.6f}, \"\n",
    "                  f\"Changes: {num_changes}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        # 상태 업데이트 (self.state만 업데이트)\n",
    "        self.state = new_state\n",
    "\n",
    "        # 관찰값 유지 (초기 관찰값 유지)\n",
    "        observation = self.observation  # 관찰값은 초기 값 유지\n",
    "\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        mask = self.create_action_mask(observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"psnr_diff\": psnr_diff, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-22 22:14:06.172882_pre_reinforce_8_0.002/2024-12-22 22:14:06.172882_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  # 더 복잡한 네트워크 구조\n",
    "    lstm_hidden_size=128,  # LSTM 크기 유지\n",
    "    shared_lstm=False  # 별도 LSTM 사용\n",
    ")\n",
    "\n",
    "ppo_model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,  # 그라디언트 클리핑 활성화\n",
    "    tensorboard_log=\"./ppo_with_mask/\",\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# 학습\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
