{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 메시지는 콘솔과 파일에 동시에 기록됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "로깅 메시지입니다.\n",
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 256, 256])\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.001146, Initial PSNR: 26.036739, 06:08:44\u001b[0m\n",
      "Logging to ./ppo_with_mask/PPO_200\n",
      "\u001b[92m[Step Start] 06:08:44.802928\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005923 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000024 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001741 seconds\n",
      "[Time Check] NumPy로 변환: 0.001217 seconds\n",
      "[Time Check] combined_observation: 0.001287 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "Step: 0, PSNR Before: 26.036739, PSNR After: 26.036945, PSNR Change: 0.000206, PSNR Diff: 0.000206 (New Max), Reward: 0.16, 06:08:44 Pre-flip Model Output=0.440629, New State Value=1, Flip Count=1\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.001402 seconds\n",
      "\u001b[92m[Step End] 06:08:44.821702 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.019243 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.834794\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001614 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000015 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.003704 seconds\n",
      "[Time Check] NumPy로 변환: 0.000335 seconds\n",
      "[Time Check] combined_observation: 0.000373 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000366 seconds\n",
      "\u001b[92m[Step End] 06:08:44.849102 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014639 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.858767\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004119 seconds\n",
      "[Time Check] Action: 0.000006 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000004 seconds\n",
      "[Time Check] flip: 0.000026 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002597 seconds\n",
      "[Time Check] NumPy로 변환: 0.000554 seconds\n",
      "[Time Check] combined_observation: 0.000543 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 보상 계산: 0.000001 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000004 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000642 seconds\n",
      "\u001b[92m[Step End] 06:08:44.873282 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014867 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.882166\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001568 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000012 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001501 seconds\n",
      "[Time Check] NumPy로 변환: 0.000379 seconds\n",
      "[Time Check] combined_observation: 0.000386 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000498 seconds\n",
      "\u001b[92m[Step End] 06:08:44.896128 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014315 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.904908\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.003781 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000001 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.004134 seconds\n",
      "[Time Check] NumPy로 변환: 0.000450 seconds\n",
      "[Time Check] combined_observation: 0.000346 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000665 seconds\n",
      "\u001b[92m[Step End] 06:08:44.922832 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018288 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.931503\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.003148 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000015 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.009344 seconds\n",
      "[Time Check] NumPy로 변환: 0.000545 seconds\n",
      "[Time Check] combined_observation: 0.000497 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 롤백 수행: 0.000845 seconds\n",
      "\u001b[92m[Step End] 06:08:44.952614 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.021482 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.960917\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.009614 seconds\n",
      "[Time Check] Action: 0.000002 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005342 seconds\n",
      "[Time Check] NumPy로 변환: 0.000487 seconds\n",
      "[Time Check] combined_observation: 0.000438 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000803 seconds\n",
      "\u001b[92m[Step End] 06:08:44.986086 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.025547 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:44.994652\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005531 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001444 seconds\n",
      "[Time Check] NumPy로 변환: 0.000338 seconds\n",
      "[Time Check] combined_observation: 0.000675 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000715 seconds\n",
      "\u001b[92m[Step End] 06:08:45.008656 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014421 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.017249\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001583 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000040 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001499 seconds\n",
      "[Time Check] NumPy로 변환: 0.000479 seconds\n",
      "[Time Check] combined_observation: 0.000410 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 보상 계산: 0.000001 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000340 seconds\n",
      "\u001b[92m[Step End] 06:08:45.027520 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.010628 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.036419\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001894 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000018 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001574 seconds\n",
      "[Time Check] NumPy로 변환: 0.000409 seconds\n",
      "[Time Check] combined_observation: 0.000397 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000861 seconds\n",
      "\u001b[92m[Step End] 06:08:45.044961 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.008894 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.053834\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001713 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001503 seconds\n",
      "[Time Check] NumPy로 변환: 0.000394 seconds\n",
      "[Time Check] combined_observation: 0.000454 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 롤백 수행: 0.000777 seconds\n",
      "\u001b[92m[Step End] 06:08:45.063988 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.010461 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.072190\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001691 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001348 seconds\n",
      "[Time Check] NumPy로 변환: 0.000436 seconds\n",
      "[Time Check] combined_observation: 0.000405 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000566 seconds\n",
      "\u001b[92m[Step End] 06:08:45.081334 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.009523 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.089967\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.006445 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005316 seconds\n",
      "[Time Check] NumPy로 변환: 0.000501 seconds\n",
      "[Time Check] combined_observation: 0.000434 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000514 seconds\n",
      "\u001b[92m[Step End] 06:08:45.112230 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.022579 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.121171\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005574 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005992 seconds\n",
      "[Time Check] NumPy로 변환: 0.000371 seconds\n",
      "[Time Check] combined_observation: 0.000424 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000285 seconds\n",
      "\u001b[92m[Step End] 06:08:45.139541 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018817 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.148314\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005550 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000015 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002249 seconds\n",
      "[Time Check] NumPy로 변환: 0.000438 seconds\n",
      "[Time Check] combined_observation: 0.000392 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 롤백 수행: 0.000670 seconds\n",
      "\u001b[92m[Step End] 06:08:45.165063 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.017104 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.173408\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004575 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000012 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002593 seconds\n",
      "[Time Check] NumPy로 변환: 0.000400 seconds\n",
      "[Time Check] combined_observation: 0.000451 seconds\n",
      "[Time Check] 계산: 0.000039 seconds\n",
      "[Time Check] 롤백 수행: 0.000763 seconds\n",
      "\u001b[92m[Step End] 06:08:45.186991 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014001 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.195457\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005612 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001207 seconds\n",
      "[Time Check] NumPy로 변환: 0.000352 seconds\n",
      "[Time Check] combined_observation: 0.000541 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.001040 seconds\n",
      "\u001b[92m[Step End] 06:08:45.209517 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.014585 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.217912\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001480 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005841 seconds\n",
      "[Time Check] NumPy로 변환: 0.000466 seconds\n",
      "[Time Check] combined_observation: 0.000448 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000796 seconds\n",
      "\u001b[92m[Step End] 06:08:45.235191 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.017624 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.243471\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.008929 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000001 seconds\n",
      "[Time Check] flip: 0.000012 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001303 seconds\n",
      "[Time Check] NumPy로 변환: 0.000587 seconds\n",
      "[Time Check] combined_observation: 0.000430 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000707 seconds\n",
      "\u001b[92m[Step End] 06:08:45.261597 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018689 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.269913\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001378 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001459 seconds\n",
      "[Time Check] NumPy로 변환: 0.000665 seconds\n",
      "[Time Check] combined_observation: 0.000429 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000603 seconds\n",
      "\u001b[92m[Step End] 06:08:45.287755 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018197 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.296079\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005416 seconds\n",
      "[Time Check] Action: 0.000002 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000001 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005387 seconds\n",
      "[Time Check] NumPy로 변환: 0.000413 seconds\n",
      "[Time Check] combined_observation: 0.000429 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000267 seconds\n",
      "\u001b[92m[Step End] 06:08:45.315993 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.020306 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.324747\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004395 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.009576 seconds\n",
      "[Time Check] NumPy로 변환: 0.000688 seconds\n",
      "[Time Check] combined_observation: 0.000487 seconds\n",
      "[Time Check] 계산: 0.000005 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000621 seconds\n",
      "\u001b[92m[Step End] 06:08:45.351126 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.026764 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.359935\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001857 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.003666 seconds\n",
      "[Time Check] NumPy로 변환: 0.000511 seconds\n",
      "[Time Check] combined_observation: 0.000383 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000289 seconds\n",
      "\u001b[92m[Step End] 06:08:45.375963 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.016482 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.384805\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004811 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001456 seconds\n",
      "[Time Check] NumPy로 변환: 0.000452 seconds\n",
      "[Time Check] combined_observation: 0.000486 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000346 seconds\n",
      "\u001b[92m[Step End] 06:08:45.400273 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.015940 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.409146\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005583 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000001 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002572 seconds\n",
      "[Time Check] NumPy로 변환: 0.000429 seconds\n",
      "[Time Check] combined_observation: 0.000405 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000786 seconds\n",
      "\u001b[92m[Step End] 06:08:45.428709 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.020071 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.437350\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005586 seconds\n",
      "[Time Check] Action: 0.000002 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000001 seconds\n",
      "[Time Check] flip: 0.000010 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001296 seconds\n",
      "[Time Check] NumPy로 변환: 0.000563 seconds\n",
      "[Time Check] combined_observation: 0.000411 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000001 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000302 seconds\n",
      "\u001b[92m[Step End] 06:08:45.456016 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.019004 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.464834\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001924 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.009142 seconds\n",
      "[Time Check] NumPy로 변환: 0.000692 seconds\n",
      "[Time Check] combined_observation: 0.000445 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000766 seconds\n",
      "\u001b[92m[Step End] 06:08:45.487809 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.023379 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.496235\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.009658 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.009314 seconds\n",
      "[Time Check] NumPy로 변환: 0.000421 seconds\n",
      "[Time Check] combined_observation: 0.000485 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000001 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000003 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000464 seconds\n",
      "\u001b[92m[Step End] 06:08:45.521393 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.025504 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.530090\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.002364 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005285 seconds\n",
      "[Time Check] NumPy로 변환: 0.000390 seconds\n",
      "[Time Check] combined_observation: 0.000396 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000626 seconds\n",
      "\u001b[92m[Step End] 06:08:45.547133 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.017367 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.555512\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004146 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005160 seconds\n",
      "[Time Check] NumPy로 변환: 0.000427 seconds\n",
      "[Time Check] combined_observation: 0.000388 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000583 seconds\n",
      "\u001b[92m[Step End] 06:08:45.574344 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.019212 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.582802\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005583 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000012 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.004819 seconds\n",
      "[Time Check] NumPy로 변환: 0.000341 seconds\n",
      "[Time Check] combined_observation: 0.000382 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000527 seconds\n",
      "\u001b[92m[Step End] 06:08:45.602177 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.019702 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.610857\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005663 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002345 seconds\n",
      "[Time Check] NumPy로 변환: 0.000612 seconds\n",
      "[Time Check] combined_observation: 0.000495 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000001 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000370 seconds\n",
      "\u001b[92m[Step End] 06:08:45.626182 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.015764 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.634867\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005328 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005398 seconds\n",
      "[Time Check] NumPy로 변환: 0.000461 seconds\n",
      "[Time Check] combined_observation: 0.000494 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 롤백 수행: 0.000849 seconds\n",
      "\u001b[92m[Step End] 06:08:45.658666 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.024166 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.667182\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004148 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005971 seconds\n",
      "[Time Check] NumPy로 변환: 0.000411 seconds\n",
      "[Time Check] combined_observation: 0.000405 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000738 seconds\n",
      "\u001b[92m[Step End] 06:08:45.684646 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018145 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.693612\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004635 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005416 seconds\n",
      "[Time Check] NumPy로 변환: 0.000385 seconds\n",
      "[Time Check] combined_observation: 0.000536 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 롤백 수행: 0.000724 seconds\n",
      "\u001b[92m[Step End] 06:08:45.712157 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.018867 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.720336\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.004439 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000011 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002910 seconds\n",
      "[Time Check] NumPy로 변환: 0.000336 seconds\n",
      "[Time Check] combined_observation: 0.000344 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000001 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000716 seconds\n",
      "\u001b[92m[Step End] 06:08:45.733937 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.013928 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.742841\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.002094 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001505 seconds\n",
      "[Time Check] NumPy로 변환: 0.000447 seconds\n",
      "[Time Check] combined_observation: 0.000470 seconds\n",
      "[Time Check] 계산: 0.000026 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000518 seconds\n",
      "\u001b[92m[Step End] 06:08:45.753007 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.010501 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.762437\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.001833 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000003 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001509 seconds\n",
      "[Time Check] NumPy로 변환: 0.000342 seconds\n",
      "[Time Check] combined_observation: 0.000462 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000551 seconds\n",
      "\u001b[92m[Step End] 06:08:45.774368 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.012374 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.783177\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.005610 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000012 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.002939 seconds\n",
      "[Time Check] NumPy로 변환: 0.000351 seconds\n",
      "[Time Check] combined_observation: 0.000385 seconds\n",
      "[Time Check] 계산: 0.000001 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000464 seconds\n",
      "\u001b[92m[Step End] 06:08:45.801980 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.019294 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.810774\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.007608 seconds\n",
      "[Time Check] Action: 0.000002 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000013 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.009383 seconds\n",
      "[Time Check] NumPy로 변환: 0.000477 seconds\n",
      "[Time Check] combined_observation: 0.000481 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 롤백 수행: 0.000807 seconds\n",
      "\u001b[92m[Step End] 06:08:45.834408 (Failed Action)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.024028 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.842664\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.009566 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000022 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.005516 seconds\n",
      "[Time Check] NumPy로 변환: 0.000513 seconds\n",
      "[Time Check] combined_observation: 0.000461 seconds\n",
      "[Time Check] 계산: 0.000002 seconds\n",
      "[Time Check] 보상 계산: 0.000000 seconds\n",
      "[Time Check] 성공 종료 조건: 0.000002 seconds\n",
      "[Time Check] 관찰값 info 업데이트: 0.000696 seconds\n",
      "\u001b[92m[Step End] 06:08:45.865178 (Success)\u001b[0m\n",
      "\u001b[91m[Time Check] step total: 0.022806 seconds\u001b[0m\n",
      "\u001b[92m[Step Start] 06:08:45.874121\u001b[0m\n",
      "[Time Check] 행동 전 PSNR 계산: 0.003003 seconds\n",
      "[Time Check] Action: 0.000003 seconds\n",
      "[Time Check] 모델 예측값 가져오기: 0.000002 seconds\n",
      "[Time Check] flip: 0.000014 seconds\n",
      "[Time Check] 행동 후후 PSNR 계산: 0.001524 seconds\n",
      "[Time Check] NumPy로 변환: 0.000426 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 623\u001b[0m\n\u001b[1;32m    607\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    609\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m )\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    626\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[1], line 417\u001b[0m, in \u001b[0;36mBinaryHologramEnv.step\u001b[0;34m(self, action, lr, z)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Combined observation 생성 후 출력\u001b[39;00m\n\u001b[1;32m    416\u001b[0m t_combined \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 417\u001b[0m combined_observation \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_np\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Time Check] combined_observation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt_combined\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# PSNR 변화량 계산\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/shape_base.py:433\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    431\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m    432\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 로그를 저장할 디렉토리 설정\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# 현재 파일 이름과 실행 시간 가져오기\n",
    "if '__file__' in globals():\n",
    "    current_file = os.path.splitext(os.path.basename(__file__))[0]  # 현재 파일 이름(확장자 제거)\n",
    "else:\n",
    "    current_file = \"interactive\"  # 인터프리터나 노트북 환경에서 기본 파일 이름 사용\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # 현재 시간\n",
    "log_filename = os.path.join(log_dir, f\"{current_file}_{current_datetime}.log\")  # log 폴더에 파일 저장\n",
    "\n",
    "# 로그 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),  # 동적으로 생성된 파일 이름 사용\n",
    "        logging.StreamHandler()  # 콘솔 출력\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, data):\n",
    "        for file in self.files:\n",
    "            file.write(data)\n",
    "            file.flush()  # 실시간 저장\n",
    "\n",
    "    def flush(self):\n",
    "        for file in self.files:\n",
    "            file.flush()\n",
    "\n",
    "\n",
    "# stdout을 파일과 콘솔로 동시에 출력\n",
    "log_file = open(log_filename, \"a\")\n",
    "sys.stdout = Tee(sys.stdout, log_file)\n",
    "\n",
    "# 테스트 출력\n",
    "print(\"이 메시지는 콘솔과 파일에 동시에 기록됩니다.\")\n",
    "logging.info(\"로깅 메시지입니다.\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "IPS = 256\n",
    "CH = 8\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, IPS, IPS).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(IPS)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((IPS, IPS))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < IPS or target.shape[-2] < IPS:\n",
    "            target = torchvision.transforms.Resize(IPS)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "        # 관찰 공간: (1, 채널, 픽셀, 픽셀)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4, CH, IPS, IPS), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: 픽셀 하나를 선택하는 인덱스 (채널 * 픽셀 *픽셀)\n",
    "        self.num_pixels = CH * IPS * IPS\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        # 타겟 함수와 데이터 로더 설정\n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # 환경 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  # 추가된 속성\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 데이터 로더에서 첫 배치 설정\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "        # 연속 실패 관련 변수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "        self.flip_count = 0\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.retry_current_target:  # 이전 에피소드에서 실패한 경우\n",
    "            self.consecutive_fail_count += 1\n",
    "        else:\n",
    "            self.consecutive_fail_count = 0  # 성공적인 에피소드로 연속 실패 초기화\n",
    "\n",
    "        self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        # 매 에피소드마다 최대 PSNR 차이 초기화\n",
    "        self.max_psnr_diff = float('-inf')\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "\n",
    "        # 타겟 이미지 형식 출력\n",
    "        #print(f\"[DEBUG]Target image shape: {self.target_image.shape}, dtype: {self.target_image.dtype}\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.flip_count = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # Ensure observation shape is (채널, 픽셀, 픽셀)\n",
    "        self.observation = model_output.squeeze(0).cpu().numpy()  # (채널, 픽셀, 픽셀)\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # Binary state\n",
    "\n",
    "        # 시뮬레이션 전 binary 형상을 (1, 채널, 픽셀, 픽셀)로 복원\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()  # (1, 채널, 픽셀, 픽셀)\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        # target_image_np와 result를 채널 차원(CH=8)으로 확장\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "        result_np = np.repeat(result.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "\n",
    "        # 모든 관찰값을 스택으로 결합\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )  # 최종 모양: [4, CH, IPS, IPS]\n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\033[92mInitial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\\033[0m\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "\n",
    "        return combined_observation, {\"state\": self.state}\n",
    "\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\033[92m[Step Start] {datetime.now().strftime('%H:%M:%S.%f')}\\033[0m\")\n",
    "        \n",
    "        # 행동 전 PSNR 계산\n",
    "        t_psnr_before = time.time()\n",
    "        binary_before = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_before = tt.Tensor(binary_before, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9}) #8개\n",
    "        sim_before = tt.simulate(binary_before, z).abs()**2\n",
    "        result_before = torch.mean(sim_before, dim=1, keepdim=True)\n",
    "        psnr_before = tt.relativeLoss(result_before, self.target_image, tm.get_PSNR)\n",
    "        print(f\"[Time Check] 행동 전 PSNR 계산: {time.time() - t_psnr_before:.6f} seconds\")\n",
    "\n",
    "        # 행동을 기반으로 픽셀 좌표 계산Inv\n",
    "        t_action = time.time()\n",
    "        channel = action // (IPS * IPS)\n",
    "        pixel_index = action % (IPS * IPS)\n",
    "        row = pixel_index // IPS\n",
    "        col = pixel_index % IPS\n",
    "        print(f\"[Time Check] Action: {time.time() - t_action:.6f} seconds\")\n",
    "\n",
    "        # 플립 전 모델 예측값 가져오기\n",
    "        t_pre_flip_value = time.time()\n",
    "        pre_flip_value = self.observation[channel, row, col]\n",
    "        print(f\"[Time Check] 모델 예측값 가져오기: {time.time() - t_pre_flip_value:.6f} seconds\")\n",
    "\n",
    "        # 상태 변경\n",
    "        t_flip = time.time()\n",
    "        self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "        self.flip_count += 1  # 플립 증가\n",
    "        print(f\"[Time Check] flip: {time.time() - t_flip:.6f} seconds\")\n",
    "\n",
    "        # 현재 상태로 새로운 시뮬레이션 수행\n",
    "        t_psnr_after = time.time()\n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "        print(f\"[Time Check] 행동 후후 PSNR 계산: {time.time() - t_psnr_after:.6f} seconds\")\n",
    "\n",
    "        # 시뮬레이션 결과를 NumPy로 변환\n",
    "        t_np = time.time()\n",
    "        result_np = np.repeat(result_after.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "        print(f\"[Time Check] NumPy로 변환: {time.time() - t_np:.6f} seconds\")\n",
    "\n",
    "        # Combined observation 생성 후 출력\n",
    "        t_combined = time.time()\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )\n",
    "        print(f\"[Time Check] combined_observation: {time.time() - t_combined:.6f} seconds\")\n",
    "\n",
    "        # PSNR 변화량 계산\n",
    "        t_compute = time.time()\n",
    "        psnr_change = psnr_after - psnr_before\n",
    "\n",
    "        # 기존 PSNR_DIFF 계산\n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "        print(f\"[Time Check] 계산: {time.time() - t_compute:.6f} seconds\")\n",
    "\n",
    "        # psnr_change가 음수인 경우 상태 롤백 수행\n",
    "        if psnr_change < 0:\n",
    "            t_rollback = time.time()\n",
    "\n",
    "            failed_observation = np.stack(\n",
    "                [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "            )  # 최종 모양: [4, CH, IPS, IPS]\n",
    "\n",
    "            failed_action = action\n",
    "            failed_reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "            # 이전 스텝의 누적 보상을 안전하게 초기화\n",
    "            previous_cumulative_reward = 0\n",
    "            if \"info\" in locals() and isinstance(info, dict):\n",
    "                previous_cumulative_reward = info.get(\"cumulative_reward\", 0)\n",
    "\n",
    "            # 실패 정보 생성\n",
    "            info = {\n",
    "                \"psnr_before\": psnr_before,\n",
    "                \"psnr_after\": psnr_after,\n",
    "                \"psnr_change\": psnr_change,\n",
    "                \"psnr_diff\": psnr_diff,\n",
    "                \"pre_flip_value\": pre_flip_value,\n",
    "                \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "                \"state_after\": None,  # 실패한 경우에는 상태를 업데이트하지 않음\n",
    "                \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "                \"observation_after\": None,  # 실패한 경우 관찰값 업데이트 없음\n",
    "                \"failed_action\": failed_action,  # 실패한 행동\n",
    "                \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "                \"reward\": failed_reward,\n",
    "                \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "                \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "                \"step\": self.steps  # 현재 스텝\n",
    "            }\n",
    "\n",
    "            # 플립된 픽셀을 원래대로 복구\n",
    "            self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "            self.flip_count -= 1\n",
    "\n",
    "            # 출력 추가 (100 스텝마다 출력)\n",
    "            if self.steps % 100 == 0:\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                print(\n",
    "                    f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                    f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                    f\"Reward: {failed_reward:.2f}, {current_time} \"\n",
    "                    f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                    f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                    f\"Flip Count={self.flip_count}\"\n",
    "                )\n",
    "\n",
    "            # 스텝 증가\n",
    "            self.steps += 1\n",
    "            print(f\"[Time Check] 롤백 수행: {time.time() - t_rollback:.6f} seconds\")\n",
    "\n",
    "            print(f\"\\033[92m[Step End] {datetime.now().strftime('%H:%M:%S.%f')} (Failed Action)\\033[0m\")\n",
    "            print(f\"\\033[91m[Time Check] step total: {time.time() - start_time:.6f} seconds\\033[0m\")\n",
    "\n",
    "            return failed_observation, failed_reward, False, False, info\n",
    "\n",
    "        # 보상 계산\n",
    "        t_rewardcompute = time.time()\n",
    "        reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "        print(f\"[Time Check] 보상 계산: {time.time() - t_rewardcompute:.6f} seconds\")\n",
    "\n",
    "        # 최고 PSNR_DIFF일 때 출력\n",
    "        #if is_max_psnr_diff:\n",
    "        #    current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        #    print(\n",
    "        #        f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "        #        f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "        #        f\"Reward: {reward:.2f}, {current_time} \"\n",
    "        #        f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "        #        f\"New State Value={self.state[channel, row, col]}, \"\n",
    "        #        f\"Flip Count={self.flip_count}\\033[0m\"\n",
    "        #    )\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\"\n",
    "            )\n",
    "\n",
    "        t_terminated = time.time()\n",
    "        # 성공 종료 조건: PSNR >= T_PSNR 또는 PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\\033[0m\"\n",
    "            )\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  # 성공 에피소드 조건\n",
    "                reward += 100  # 에피소드 성공 시 추가 보상\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "        print(f\"[Time Check] 성공 종료 조건: {time.time() - t_terminated:.6f} seconds\")\n",
    "\n",
    "        t_info = time.time()\n",
    "        # 관찰값 업데이트\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"pre_flip_value\": pre_flip_value,\n",
    "            \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "            \"state_after\": self.state.copy() if psnr_change >= 0 else None,  # 행동 성공 시 상태\n",
    "            \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "            \"observation_after\": combined_observation if psnr_change >= 0 else None,  # 행동 성공 시 관찰값\n",
    "            \"failed_action\": action if psnr_change < 0 else None,  # 실패한 행동\n",
    "            \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "            \"reward\": reward,\n",
    "            \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "            \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "            \"action_coords\": (channel, row, col),  # 행동한 좌표\n",
    "            \"step\": self.steps  # 현재 스텝\n",
    "        }\n",
    "        print(f\"[Time Check] 관찰값 info 업데이트: {time.time() - t_info:.6f} seconds\")\n",
    "\n",
    "        #del binary_before, binary_after, sim_before, sim_after, result_before, result_after\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        print(f\"\\033[92m[Step End] {datetime.now().strftime('%H:%M:%S.%f')} (Success)\\033[0m\")\n",
    "        print(f\"\\033[91m[Time Check] step total: {time.time() - start_time:.6f} seconds\\033[0m\")\n",
    "\n",
    "        return combined_observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    #max_steps=10000,\n",
    "    #T_PSNR=30,\n",
    "    #T_steps=10\n",
    ")\n",
    "\n",
    "# PPO 모델 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-4,  # 학습률 감소\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping 추가\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
