{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 메시지는 콘솔과 파일에 동시에 기록됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 256, 256])\n",
      "Warning: PPO model not found at ./ppo_CnnPolicy_models/ppo_CnnPolicy_latest.zip. Starting training from scratch.\n",
      "Starting training from scratch.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\u001b[93m[Episode Start] Currently using dataset file: ('/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/0001.png',)\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.001372, Initial PSNR: 25.839237, 10:45:34\u001b[0m\n",
      "Logging to ./ppo_CnnPolicy/PPO_3\n",
      "Step: 0, PSNR Before: 25.839237, PSNR After: 25.839188, PSNR Change: -0.000050, PSNR Diff: -0.000050 (New Max), Reward: -0.04, 10:45:34 Pre-flip Model Output=0.037808, New State Value=0, Flip Count=0\n",
      "---------------------------\n",
      "| time/              |    |\n",
      "|    fps             | 33 |\n",
      "|    iterations      | 1  |\n",
      "|    time_elapsed    | 1  |\n",
      "|    total_timesteps | 64 |\n",
      "---------------------------\n",
      "Step: 100, PSNR Before: 25.843985, PSNR After: 25.843866, PSNR Change: -0.000118, PSNR Diff: 0.004629 (New Max), Reward: -0.09, 10:45:44 Pre-flip Model Output=0.050410, New State Value=0, Flip Count=38\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 128          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021993201 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -13.2        |\n",
      "|    explained_variance   | -0.000733    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0685       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    value_loss           | 0.427        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 192         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006761049 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.531       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n",
      "Step: 200, PSNR Before: 25.850483, PSNR After: 25.850153, PSNR Change: -0.000330, PSNR Diff: 0.010916 (New Max), Reward: -0.26, 10:46:02 Pre-flip Model Output=0.143513, New State Value=0, Flip Count=73\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011542569 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 0.481       |\n",
      "-----------------------------------------\n",
      "Step: 300, PSNR Before: 25.854622, PSNR After: 25.854572, PSNR Change: -0.000050, PSNR Diff: 0.015335 (New Max), Reward: -0.04, 10:46:12 Pre-flip Model Output=0.657817, New State Value=1, Flip Count=101\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 8          |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 320        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01703035 |\n",
      "|    clip_fraction        | 0.0734     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0661     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0723    |\n",
      "|    value_loss           | 0.994      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 384         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025820933 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0833     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0919     |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n",
      "Step: 400, PSNR Before: 25.861074, PSNR After: 25.861275, PSNR Change: 0.000200, PSNR Diff: 0.022038 (New Max), Reward: 0.16, 10:46:30 Pre-flip Model Output=0.420027, New State Value=1, Flip Count=137\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 57         |\n",
      "|    total_timesteps      | 448        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02522584 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0866     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0918    |\n",
      "|    value_loss           | 1.03       |\n",
      "----------------------------------------\n",
      "Step: 500, PSNR Before: 25.865685, PSNR After: 25.865641, PSNR Change: -0.000044, PSNR Diff: 0.026403 (New Max), Reward: -0.04, 10:46:40 Pre-flip Model Output=0.301569, New State Value=0, Flip Count=169\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 512         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029931376 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00497    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0974     |\n",
      "|    value_loss           | 0.607       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 576         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035412982 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.113      |\n",
      "|    value_loss           | 0.334       |\n",
      "-----------------------------------------\n",
      "Step: 600, PSNR Before: 25.872995, PSNR After: 25.871962, PSNR Change: -0.001034, PSNR Diff: 0.032724 (New Max), Reward: -0.83, 10:46:58 Pre-flip Model Output=0.114238, New State Value=0, Flip Count=219\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 85         |\n",
      "|    total_timesteps      | 640        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03670567 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.109     |\n",
      "|    value_loss           | 0.331      |\n",
      "----------------------------------------\n",
      "Step: 700, PSNR Before: 25.877247, PSNR After: 25.877235, PSNR Change: -0.000011, PSNR Diff: 0.037998 (New Max), Reward: -0.01, 10:47:08 Pre-flip Model Output=0.148376, New State Value=0, Flip Count=251\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 704        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03260075 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0934    |\n",
      "|    value_loss           | 0.306      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039078973 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0714     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.109      |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "Step: 800, PSNR Before: 25.884605, PSNR After: 25.884064, PSNR Change: -0.000542, PSNR Diff: 0.044827 (New Max), Reward: -0.43, 10:47:26 Pre-flip Model Output=0.092877, New State Value=0, Flip Count=292\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 112        |\n",
      "|    total_timesteps      | 832        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03876175 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.265      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.108     |\n",
      "|    value_loss           | 0.777      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 896         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040042885 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.267       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "Step: 900, PSNR Before: 25.889627, PSNR After: 25.889385, PSNR Change: -0.000242, PSNR Diff: 0.050148 (New Max), Reward: -0.19, 10:47:44 Pre-flip Model Output=0.868156, New State Value=1, Flip Count=317\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 960         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041268464 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0695     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Step: 1000, PSNR Before: 25.893730, PSNR After: 25.893766, PSNR Change: 0.000036, PSNR Diff: 0.054529 (New Max), Reward: 0.03, 10:47:53 Pre-flip Model Output=0.422332, New State Value=1, Flip Count=351\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038548157 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0431     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 0.265       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 7         |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 149       |\n",
      "|    total_timesteps      | 1088      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0393778 |\n",
      "|    clip_fraction        | 0.4       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.2     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0629   |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    value_loss           | 0.376     |\n",
      "---------------------------------------\n",
      "Step: 1100, PSNR Before: 25.898035, PSNR After: 25.897768, PSNR Change: -0.000267, PSNR Diff: 0.058531 (New Max), Reward: -0.21, 10:48:11 Pre-flip Model Output=0.138671, New State Value=0, Flip Count=377\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 1152        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043058723 |\n",
      "|    clip_fraction        | 0.473       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 0.583       |\n",
      "-----------------------------------------\n",
      "Step: 1200, PSNR Before: 25.902542, PSNR After: 25.902519, PSNR Change: -0.000023, PSNR Diff: 0.063282 (New Max), Reward: -0.02, 10:48:21 Pre-flip Model Output=0.325364, New State Value=0, Flip Count=403\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 1216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042120766 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.115      |\n",
      "|    value_loss           | 0.673       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 1280        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047826964 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.115      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.112      |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Step: 1300, PSNR Before: 25.906763, PSNR After: 25.906250, PSNR Change: -0.000513, PSNR Diff: 0.067013 (New Max), Reward: -0.41, 10:48:39 Pre-flip Model Output=0.415810, New State Value=0, Flip Count=436\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 1344        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043335155 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.119      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Step: 1400, PSNR Before: 25.910717, PSNR After: 25.910755, PSNR Change: 0.000038, PSNR Diff: 0.071518 (New Max), Reward: 0.03, 10:48:49 Pre-flip Model Output=0.289585, New State Value=1, Flip Count=461\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 1408        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047828812 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0775     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 1472        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041995447 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.104      |\n",
      "|    value_loss           | 0.614       |\n",
      "-----------------------------------------\n",
      "Step: 1500, PSNR Before: 25.918011, PSNR After: 25.917850, PSNR Change: -0.000160, PSNR Diff: 0.078613 (New Max), Reward: -0.13, 10:49:07 Pre-flip Model Output=0.314790, New State Value=0, Flip Count=491\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052656204 |\n",
      "|    clip_fraction        | 0.561       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.128      |\n",
      "|    value_loss           | 0.668       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 223        |\n",
      "|    total_timesteps      | 1600       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05053455 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.194      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.121     |\n",
      "|    value_loss           | 0.7        |\n",
      "----------------------------------------\n",
      "Step: 1600, PSNR Before: 25.922964, PSNR After: 25.922806, PSNR Change: -0.000158, PSNR Diff: 0.083569 (New Max), Reward: -0.13, 10:49:25 Pre-flip Model Output=0.977325, New State Value=1, Flip Count=523\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 1664        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050064534 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.245       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.121      |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "Step: 1700, PSNR Before: 25.927332, PSNR After: 25.928146, PSNR Change: 0.000814, PSNR Diff: 0.088909 (New Max), Reward: 0.65, 10:49:34 Pre-flip Model Output=0.524130, New State Value=0, Flip Count=556\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 1728        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050281953 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.126      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.119      |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 1792        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053537775 |\n",
      "|    clip_fraction        | 0.555       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0627     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.126      |\n",
      "|    value_loss           | 0.301       |\n",
      "-----------------------------------------\n",
      "Step: 1800, PSNR Before: 25.932190, PSNR After: 25.932238, PSNR Change: 0.000048, PSNR Diff: 0.093000 (New Max), Reward: 0.04, 10:49:52 Pre-flip Model Output=0.550174, New State Value=0, Flip Count=586\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 1856        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052721947 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0873      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.124      |\n",
      "|    value_loss           | 0.53        |\n",
      "-----------------------------------------\n",
      "\u001b[94mStep: 1894, PSNR Before: 25.938967, PSNR After: 25.939245, PSNR Change: 0.000278, PSNR Diff: 0.100008 (New Max), Reward: 0.22, 10:50:02 Pre-flip Model Output=0.786009, New State Value=0, Flip Count=621\u001b[0m\n",
      "\u001b[94mStep: 1898, PSNR Before: 25.939245, PSNR After: 25.939381, PSNR Change: 0.000135, PSNR Diff: 0.100143 (New Max), Reward: 0.11, 10:50:02 Pre-flip Model Output=0.034744, New State Value=1, Flip Count=622\u001b[0m\n",
      "Step: 1900, PSNR Before: 25.939381, PSNR After: 25.939384, PSNR Change: 0.000004, PSNR Diff: 0.100147 (New Max), Reward: 0.00, 10:50:02 Pre-flip Model Output=0.000001, New State Value=1, Flip Count=623\n",
      "\u001b[94mStep: 1900, PSNR Before: 25.939381, PSNR After: 25.939384, PSNR Change: 0.000004, PSNR Diff: 0.100147 (New Max), Reward: 0.00, 10:50:02 Pre-flip Model Output=0.000001, New State Value=1, Flip Count=623\u001b[0m\n",
      "\u001b[94mStep: 1902, PSNR Before: 25.939384, PSNR After: 25.939735, PSNR Change: 0.000351, PSNR Diff: 0.100498 (New Max), Reward: 0.28, 10:50:02 Pre-flip Model Output=0.100270, New State Value=1, Flip Count=624\u001b[0m\n",
      "\u001b[94mStep: 1903, PSNR Before: 25.939735, PSNR After: 25.939787, PSNR Change: 0.000051, PSNR Diff: 0.100550 (New Max), Reward: 0.04, 10:50:02 Pre-flip Model Output=0.544520, New State Value=0, Flip Count=625\u001b[0m\n",
      "\u001b[94mStep: 1909, PSNR Before: 25.939787, PSNR After: 25.939816, PSNR Change: 0.000029, PSNR Diff: 0.100578 (New Max), Reward: 0.02, 10:50:02 Pre-flip Model Output=0.392639, New State Value=1, Flip Count=626\u001b[0m\n",
      "\u001b[94mStep: 1912, PSNR Before: 25.939816, PSNR After: 25.939838, PSNR Change: 0.000023, PSNR Diff: 0.100601 (New Max), Reward: 0.02, 10:50:02 Pre-flip Model Output=0.832040, New State Value=0, Flip Count=627\u001b[0m\n",
      "\u001b[94mStep: 1914, PSNR Before: 25.939838, PSNR After: 25.940029, PSNR Change: 0.000191, PSNR Diff: 0.100792 (New Max), Reward: 0.15, 10:50:02 Pre-flip Model Output=0.318456, New State Value=1, Flip Count=628\u001b[0m\n",
      "\u001b[94mStep: 1917, PSNR Before: 25.940029, PSNR After: 25.940029, PSNR Change: 0.000000, PSNR Diff: 0.100792 (New Max), Reward: 0.00, 10:50:03 Pre-flip Model Output=0.256103, New State Value=1, Flip Count=629\u001b[0m\n",
      "\u001b[94mStep: 1919, PSNR Before: 25.940029, PSNR After: 25.940338, PSNR Change: 0.000309, PSNR Diff: 0.101101 (New Max), Reward: 0.25, 10:50:03 Pre-flip Model Output=0.007510, New State Value=1, Flip Count=630\u001b[0m\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 1920        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056767132 |\n",
      "|    clip_fraction        | 0.545       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0767      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.128      |\n",
      "|    value_loss           | 0.506       |\n",
      "-----------------------------------------\n",
      "\u001b[94mStep: 1924, PSNR Before: 25.940338, PSNR After: 25.940506, PSNR Change: 0.000168, PSNR Diff: 0.101269 (New Max), Reward: 0.13, 10:50:11 Pre-flip Model Output=0.255922, New State Value=1, Flip Count=631\u001b[0m\n",
      "\u001b[93m[Episode Start] Currently using dataset file: ('/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/0002.png',)\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.003654, Initial PSNR: 24.371769, 10:50:11\u001b[0m\n",
      "Step: 0, PSNR Before: 24.371769, PSNR After: 24.371775, PSNR Change: 0.000006, PSNR Diff: 0.000006 (New Max), Reward: 0.00, 10:50:11 Pre-flip Model Output=0.185029, New State Value=1, Flip Count=1\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 1984        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011290975 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 535         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 1.34e+03    |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 24.376122, PSNR After: 24.376427, PSNR Change: 0.000305, PSNR Diff: 0.004658 (New Max), Reward: 0.24, 10:50:21 Pre-flip Model Output=0.448712, New State Value=1, Flip Count=29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025578493 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 576         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 728         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 2112        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051491514 |\n",
      "|    clip_fraction        | 0.516       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0889     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.113      |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Step: 200, PSNR Before: 24.379835, PSNR After: 24.379704, PSNR Change: -0.000132, PSNR Diff: 0.007935 (New Max), Reward: -0.11, 10:50:39 Pre-flip Model Output=0.907328, New State Value=1, Flip Count=52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 2176        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051744767 |\n",
      "|    clip_fraction        | 0.509       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.312       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "Step: 300, PSNR Before: 24.384796, PSNR After: 24.384781, PSNR Change: -0.000015, PSNR Diff: 0.013012 (New Max), Reward: -0.01, 10:50:49 Pre-flip Model Output=0.429862, New State Value=0, Flip Count=88\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 315        |\n",
      "|    total_timesteps      | 2240       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05275827 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.114     |\n",
      "|    value_loss           | 0.334      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 2304        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055956528 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.126      |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Step: 400, PSNR Before: 24.388802, PSNR After: 24.388550, PSNR Change: -0.000252, PSNR Diff: 0.016781 (New Max), Reward: -0.20, 10:51:07 Pre-flip Model Output=0.356360, New State Value=0, Flip Count=110\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 2368        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055117816 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.123      |\n",
      "|    value_loss           | 0.39        |\n",
      "-----------------------------------------\n",
      "Step: 500, PSNR Before: 24.391911, PSNR After: 24.391670, PSNR Change: -0.000240, PSNR Diff: 0.019901 (New Max), Reward: -0.19, 10:51:17 Pre-flip Model Output=0.725669, New State Value=1, Flip Count=131\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 2432        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055676814 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0949     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.113      |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 352        |\n",
      "|    total_timesteps      | 2496       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05645109 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.345      |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.13      |\n",
      "|    value_loss           | 1.22       |\n",
      "----------------------------------------\n",
      "Step: 600, PSNR Before: 24.396162, PSNR After: 24.396061, PSNR Change: -0.000101, PSNR Diff: 0.024292 (New Max), Reward: -0.08, 10:51:35 Pre-flip Model Output=0.725483, New State Value=1, Flip Count=161\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051959656 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.033      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.114      |\n",
      "|    value_loss           | 0.431       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 2624        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055621807 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0497     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "Step: 700, PSNR Before: 24.400059, PSNR After: 24.399811, PSNR Change: -0.000248, PSNR Diff: 0.028042 (New Max), Reward: -0.20, 10:51:53 Pre-flip Model Output=0.821412, New State Value=1, Flip Count=189\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 380         |\n",
      "|    total_timesteps      | 2688        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059325233 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.131      |\n",
      "|    value_loss           | 0.415       |\n",
      "-----------------------------------------\n",
      "Step: 800, PSNR Before: 24.402832, PSNR After: 24.402523, PSNR Change: -0.000309, PSNR Diff: 0.030754 (New Max), Reward: -0.25, 10:52:03 Pre-flip Model Output=0.161120, New State Value=0, Flip Count=210\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 389         |\n",
      "|    total_timesteps      | 2752        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053394187 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.119      |\n",
      "|    value_loss           | 0.752       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 399        |\n",
      "|    total_timesteps      | 2816       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05433336 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.115     |\n",
      "|    value_loss           | 0.552      |\n",
      "----------------------------------------\n",
      "Step: 900, PSNR Before: 24.405350, PSNR After: 24.405128, PSNR Change: -0.000221, PSNR Diff: 0.033360 (New Max), Reward: -0.18, 10:52:21 Pre-flip Model Output=0.348371, New State Value=0, Flip Count=235\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 408        |\n",
      "|    total_timesteps      | 2880       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05464508 |\n",
      "|    clip_fraction        | 0.556      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.334      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.121     |\n",
      "|    value_loss           | 0.904      |\n",
      "----------------------------------------\n",
      "Step: 1000, PSNR Before: 24.409197, PSNR After: 24.409306, PSNR Change: 0.000109, PSNR Diff: 0.037537 (New Max), Reward: 0.09, 10:52:31 Pre-flip Model Output=0.801095, New State Value=0, Flip Count=269\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 418        |\n",
      "|    total_timesteps      | 2944       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05962023 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.121     |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 427        |\n",
      "|    total_timesteps      | 3008       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05765889 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0115     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.124     |\n",
      "|    value_loss           | 0.354      |\n",
      "----------------------------------------\n",
      "Step: 1100, PSNR Before: 24.413355, PSNR After: 24.413071, PSNR Change: -0.000284, PSNR Diff: 0.041302 (New Max), Reward: -0.23, 10:52:49 Pre-flip Model Output=0.999584, New State Value=1, Flip Count=296\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053608604 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0854     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.111      |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Step: 1200, PSNR Before: 24.417492, PSNR After: 24.417429, PSNR Change: -0.000063, PSNR Diff: 0.045660 (New Max), Reward: -0.05, 10:52:59 Pre-flip Model Output=0.227200, New State Value=0, Flip Count=326\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.92e+03  |\n",
      "|    ep_rew_mean          | 6.34      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 7         |\n",
      "|    iterations           | 49        |\n",
      "|    time_elapsed         | 446       |\n",
      "|    total_timesteps      | 3136      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0595968 |\n",
      "|    clip_fraction        | 0.584     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.2     |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0495   |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.126    |\n",
      "|    value_loss           | 0.263     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 3200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055026524 |\n",
      "|    clip_fraction        | 0.541       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.115      |\n",
      "|    value_loss           | 0.36        |\n",
      "-----------------------------------------\n",
      "Step: 1300, PSNR Before: 24.420481, PSNR After: 24.420776, PSNR Change: 0.000296, PSNR Diff: 0.049007 (New Max), Reward: 0.24, 10:53:18 Pre-flip Model Output=0.319087, New State Value=1, Flip Count=349\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 3264        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056911573 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.106      |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Step: 1400, PSNR Before: 24.423258, PSNR After: 24.423237, PSNR Change: -0.000021, PSNR Diff: 0.051468 (New Max), Reward: -0.02, 10:53:27 Pre-flip Model Output=0.839044, New State Value=1, Flip Count=369\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 474         |\n",
      "|    total_timesteps      | 3328        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057139307 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.123      |\n",
      "|    value_loss           | 0.948       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 3392        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060289055 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0905     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.125      |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "Step: 1500, PSNR Before: 24.426729, PSNR After: 24.426247, PSNR Change: -0.000483, PSNR Diff: 0.054478 (New Max), Reward: -0.39, 10:53:46 Pre-flip Model Output=0.512207, New State Value=1, Flip Count=394\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 3456        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064331576 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0856     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.131      |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 3520        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062636465 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0784     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.129      |\n",
      "|    value_loss           | 0.273       |\n",
      "-----------------------------------------\n",
      "Step: 1600, PSNR Before: 24.431698, PSNR After: 24.431568, PSNR Change: -0.000130, PSNR Diff: 0.059799 (New Max), Reward: -0.10, 10:54:04 Pre-flip Model Output=0.036980, New State Value=0, Flip Count=432\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 511         |\n",
      "|    total_timesteps      | 3584        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056889683 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0636     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.264       |\n",
      "-----------------------------------------\n",
      "Step: 1700, PSNR Before: 24.435980, PSNR After: 24.435465, PSNR Change: -0.000515, PSNR Diff: 0.063696 (New Max), Reward: -0.41, 10:54:14 Pre-flip Model Output=0.945838, New State Value=1, Flip Count=462\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 3648        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058875363 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.116      |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 530         |\n",
      "|    total_timesteps      | 3712        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057988457 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.292       |\n",
      "-----------------------------------------\n",
      "Step: 1800, PSNR Before: 24.438143, PSNR After: 24.437668, PSNR Change: -0.000475, PSNR Diff: 0.065899 (New Max), Reward: -0.38, 10:54:32 Pre-flip Model Output=0.195608, New State Value=0, Flip Count=482\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 539        |\n",
      "|    total_timesteps      | 3776       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06406009 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0828     |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.131     |\n",
      "|    value_loss           | 0.777      |\n",
      "----------------------------------------\n",
      "Step: 1900, PSNR Before: 24.442751, PSNR After: 24.442585, PSNR Change: -0.000166, PSNR Diff: 0.070816 (New Max), Reward: -0.13, 10:54:42 Pre-flip Model Output=0.169562, New State Value=0, Flip Count=506\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.92e+03   |\n",
      "|    ep_rew_mean          | 6.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 548        |\n",
      "|    total_timesteps      | 3840       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06230537 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00581   |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.123     |\n",
      "|    value_loss           | 0.387      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 3904        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058054086 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0671     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Step: 2000, PSNR Before: 24.447128, PSNR After: 24.447042, PSNR Change: -0.000086, PSNR Diff: 0.075274 (New Max), Reward: -0.07, 10:55:00 Pre-flip Model Output=0.100997, New State Value=0, Flip Count=535\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.92e+03  |\n",
      "|    ep_rew_mean          | 6.34      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 6         |\n",
      "|    iterations           | 62        |\n",
      "|    time_elapsed         | 567       |\n",
      "|    total_timesteps      | 3968      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0643878 |\n",
      "|    clip_fraction        | 0.581     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.2     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0367   |\n",
      "|    n_updates            | 610       |\n",
      "|    policy_gradient_loss | -0.122    |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Step: 2100, PSNR Before: 24.451063, PSNR After: 24.451162, PSNR Change: 0.000099, PSNR Diff: 0.079393 (New Max), Reward: 0.08, 10:55:10 Pre-flip Model Output=0.815904, New State Value=0, Flip Count=566\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 4032        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057232987 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00272    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.12       |\n",
      "|    value_loss           | 0.502       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.92e+03    |\n",
      "|    ep_rew_mean          | 6.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057414528 |\n",
      "|    clip_fraction        | 0.544       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0625     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.123      |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 로그를 저장할 디렉토리 설정\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# 현재 파일 이름과 실행 시간 가져오기\n",
    "if '__file__' in globals():\n",
    "    current_file = os.path.splitext(os.path.basename(__file__))[0]  # 현재 파일 이름(확장자 제거)\n",
    "else:\n",
    "    current_file = \"interactive\"  # 인터프리터나 노트북 환경에서 기본 파일 이름 사용\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # 현재 시간\n",
    "log_filename = os.path.join(log_dir, f\"{current_file}_{current_datetime}.log\")  # log 폴더에 파일 저장\n",
    "\n",
    "# 로그 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),  # 동적으로 생성된 파일 이름 사용\n",
    "        logging.StreamHandler()  # 콘솔 출력\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, data):\n",
    "        for file in self.files:\n",
    "            file.write(data)\n",
    "            file.flush()  # 실시간 저장\n",
    "\n",
    "    def flush(self):\n",
    "        for file in self.files:\n",
    "            file.flush()\n",
    "\n",
    "\n",
    "# stdout을 파일과 콘솔로 동시에 출력\n",
    "log_file = open(log_filename, \"a\")\n",
    "sys.stdout = Tee(sys.stdout, log_file)\n",
    "\n",
    "# 테스트 출력\n",
    "print(\"이 메시지는 콘솔과 파일에 동시에 기록됩니다.\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "IPS = 256\n",
    "CH = 8\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, IPS, IPS).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(IPS)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((IPS, IPS))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=self.meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < IPS or target.shape[-2] < IPS:\n",
    "            target = torchvision.transforms.Resize(IPS)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        # 데이터와 파일 경로를 함께 반환\n",
    "        return target, self.target_list[idx]\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "        # 관찰 공간: (1, 채널, 픽셀, 픽셀)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4, CH, IPS, IPS), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: 픽셀 하나를 선택하는 인덱스 (채널 * 픽셀 *픽셀)\n",
    "        self.num_pixels = CH * IPS * IPS\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        # 타겟 함수와 데이터 로더 설정\n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # 환경 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  # 추가된 속성\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 데이터 로더에서 첫 배치 설정\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "        # 연속 실패 관련 변수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "        self.flip_count = 0\n",
    "\n",
    "        # PSNR 저장 변수\n",
    "        self.previous_psnr = None\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 이터레이터에서 다음 데이터를 가져옴\n",
    "        try:\n",
    "            self.target_image, self.current_file = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            # 데이터셋 끝에 도달하면 이터레이터를 다시 생성하고 처음부터 다시 시작\n",
    "            print(\"\\033[93m[INFO] Reached the end of dataset. Restarting from the beginning.\\033[0m\")\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image, self.current_file = next(self.data_iter)\n",
    "\n",
    "        print(f\"\\033[93m[Episode Start] Currently using dataset file: {self.current_file}\\033[0m\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "\n",
    "        # 연속 실패 처리\n",
    "        #if self.retry_current_target:  # 이전 에피소드에서 실패한 경우\n",
    "        #    self.consecutive_fail_count += 1\n",
    "        #else:\n",
    "        #    self.consecutive_fail_count = 0  # 성공적인 에피소드로 연속 실패 초기화\n",
    "\n",
    "        # 최대 연속 실패 기록 갱신\n",
    "        #self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        # 실패 플래그에 따라 데이터 유지 또는 새 데이터 로드\n",
    "        #if not self.retry_current_target:\n",
    "        #    try:\n",
    "        #        self.target_image = next(self.data_iter)\n",
    "        #    except StopIteration:\n",
    "        #        print(\"\\033[93m[INFO] Reached the end of dataset. Restarting from the beginning.\\033[0m\")\n",
    "        #        self.data_iter = iter(self.trainloader)\n",
    "        #        self.target_image = next(self.data_iter)\n",
    "\n",
    "        # 타겟 이미지 형식 출력\n",
    "        #print(f\"[DEBUG]Target image shape: {self.target_image.shape}, dtype: {self.target_image.dtype}\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        # 매 에피소드마다 초기화\n",
    "        self.max_psnr_diff = float('-inf')\n",
    "        self.steps = 0\n",
    "        self.flip_count = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # Ensure observation shape is (채널, 픽셀, 픽셀)\n",
    "        self.observation = model_output.squeeze(0).cpu().numpy()  # (채널, 픽셀, 픽셀)\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # Binary state\n",
    "\n",
    "        # 시뮬레이션 전 binary 형상을 (1, 채널, 픽셀, 픽셀)로 복원\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()  # (1, 채널, 픽셀, 픽셀)\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "        self.previous_psnr = self.initial_psnr # 초기 PSNR 저장\n",
    "\n",
    "        # target_image_np와 result를 채널 차원(CH=8)으로 확장\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "        result_np = np.repeat(result.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "\n",
    "        # 모든 관찰값을 스택으로 결합\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )  # 최종 모양: [4, CH, IPS, IPS]\n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\033[92mInitial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\\033[0m\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "\n",
    "        return combined_observation, {\"state\": self.state}\n",
    "\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 행동 전 PSNR 계산\n",
    "        psnr_before = self.previous_psnr\n",
    "\n",
    "        # 기존 로직\n",
    "        channel = action // (IPS * IPS)\n",
    "        pixel_index = action % (IPS * IPS)\n",
    "        row = pixel_index // IPS\n",
    "        col = pixel_index % IPS\n",
    "\n",
    "        # 플립 전 모델 예측값 가져오기\n",
    "        pre_flip_value = self.observation[channel, row, col]\n",
    "\n",
    "        # 상태 변경\n",
    "        self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "        self.flip_count += 1  # 플립 증가\n",
    "\n",
    "        # 현재 상태로 새로운 시뮬레이션 수행\n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 시뮬레이션 결과를 NumPy로 변환\n",
    "        result_np = np.repeat(result_after.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        # Combined observation 생성 후 출력\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )\n",
    "\n",
    "        # PSNR 변화량 계산\n",
    "        psnr_change = psnr_after - psnr_before\n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "\n",
    "        # psnr_change가 음수인 경우 상태 롤백 수행\n",
    "        if psnr_change < 0:\n",
    "\n",
    "            failed_observation = combined_observation\n",
    "\n",
    "            failed_action = action\n",
    "            failed_reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "            # 이전 스텝의 누적 보상을 안전하게 초기화\n",
    "            previous_cumulative_reward = 0\n",
    "            if \"info\" in locals() and isinstance(info, dict):\n",
    "                previous_cumulative_reward = info.get(\"cumulative_reward\", 0)\n",
    "\n",
    "            # 플립된 픽셀을 원래대로 복구\n",
    "            self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "            self.flip_count -= 1\n",
    "\n",
    "            # 출력 추가 (100 스텝마다 출력)\n",
    "            if self.steps % 100 == 0:\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                print(\n",
    "                    f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                    f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                    f\"Reward: {failed_reward:.2f}, {current_time} \"\n",
    "                    f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                    f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                    f\"Flip Count={self.flip_count}\"\n",
    "                )\n",
    "\n",
    "            # 스텝 증가\n",
    "            self.steps += 1\n",
    "\n",
    "            # 실패 정보 생성\n",
    "            info = {\n",
    "                \"psnr_before\": psnr_before,\n",
    "                \"psnr_after\": psnr_after,\n",
    "                \"psnr_change\": psnr_change,\n",
    "                \"psnr_diff\": psnr_diff,\n",
    "                \"pre_flip_value\": pre_flip_value,\n",
    "                \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "                \"state_after\": None,  # 실패한 경우에는 상태를 업데이트하지 않음\n",
    "                \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "                \"observation_after\": None,  # 실패한 경우 관찰값 업데이트 없음\n",
    "                \"failed_action\": failed_action,  # 실패한 행동\n",
    "                \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "                \"reward\": failed_reward,\n",
    "                \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "                \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "                \"step\": self.steps  # 현재 스텝\n",
    "            }\n",
    "            return failed_observation, failed_reward, False, False, info\n",
    "\n",
    "        # 보상 계산\n",
    "        reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\"\n",
    "            )\n",
    "\n",
    "        self.previous_psnr = psnr_after\n",
    "\n",
    "        # 성공 종료 조건: PSNR >= T_PSNR 또는 PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\\033[0m\"\n",
    "            )\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  # 성공 에피소드 조건\n",
    "                reward += 100  # 에피소드 성공 시 추가 보상\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"pre_flip_value\": pre_flip_value,\n",
    "            \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "            \"state_after\": self.state.copy() if psnr_change >= 0 else None,  # 행동 성공 시 상태\n",
    "            \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "            \"observation_after\": combined_observation if psnr_change >= 0 else None,  # 행동 성공 시 관찰값\n",
    "            \"failed_action\": action if psnr_change < 0 else None,  # 실패한 행동\n",
    "            \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "            \"reward\": reward,\n",
    "            \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "            \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "            \"action_coords\": (channel, row, col),  # 행동한 좌표\n",
    "            \"step\": self.steps  # 현재 스텝\n",
    "        }\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        return combined_observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=128):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "\n",
    "        # 입력 채널 계산: 4 * CH = 32\n",
    "        n_input_channels = observation_space.shape[0] * observation_space.shape[1]\n",
    "\n",
    "        # CNN 정의\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # CNN 출력 크기 계산\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, n_input_channels, IPS, IPS)  # (배치 크기, 채널, 높이, 너비)\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "\n",
    "        # FC 레이어 정의\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # 입력 형태: (배치 크기, 4, CH, IPS, IPS) -> (배치 크기, 4 * CH, IPS, IPS)\n",
    "        batch_size = observations.size(0)\n",
    "        observations = observations.view(batch_size, -1, IPS, IPS)\n",
    "\n",
    "        # CNN 처리 후 결과 반환\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    #max_steps=10000,\n",
    "    #T_PSNR=30,\n",
    "    #T_steps=10\n",
    ")\n",
    "\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class StopOnEpisodeCallback(BaseCallback):\n",
    "    def __init__(self, max_episodes, verbose=1):\n",
    "        super(StopOnEpisodeCallback, self).__init__(verbose)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.episode_count = 0  # 에피소드 수를 추적\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # `done`이 True일 때마다 에피소드 증가\n",
    "        if self.locals.get(\"dones\") is not None:\n",
    "            self.episode_count += np.sum(self.locals[\"dones\"])  # 에피소드 완료 횟수 추가\n",
    "\n",
    "        #if self.verbose > 0 and self.episode_count % 10 == 0:\n",
    "            #print(f\"Episode count: {self.episode_count}/{self.max_episodes}\")\n",
    "\n",
    "        # 최대 에피소드 도달 시 학습 종료\n",
    "        if self.episode_count >= self.max_episodes:\n",
    "            print(f\"Stopping training at episode {self.episode_count}\")\n",
    "            return False  # 학습 중단\n",
    "\n",
    "        return True  # 학습 계속\n",
    "\n",
    "\n",
    "# 저장할 폴더 경로 설정\n",
    "save_dir = \"./ppo_CnnPolicy_models/\"  # 모델 저장 디렉토리\n",
    "os.makedirs(save_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# PPO 모델 저장 경로 설정\n",
    "ppo_model_path = os.path.join(save_dir, \"ppo_CnnPolicy_latest.zip\")  # 최신 모델 저장 경로\n",
    "resume_training = True  # True로 설정하면 이전 모델에서 학습 재개\n",
    "\n",
    "# PPO 모델 로드 또는 새로 생성\n",
    "if resume_training and os.path.exists(ppo_model_path):\n",
    "    print(f\"Loading trained PPO model from {ppo_model_path}\")\n",
    "    ppo_model = PPO.load(ppo_model_path, env=env)\n",
    "else:\n",
    "    if resume_training:\n",
    "        print(f\"Warning: PPO model not found at {ppo_model_path}. Starting training from scratch.\")\n",
    "    print(\"Starting training from scratch.\")\n",
    "    ppo_model = PPO(\n",
    "        policy=\"CnnPolicy\",\n",
    "        env=env,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=2,\n",
    "        n_steps=64,\n",
    "        batch_size=16,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        learning_rate=1e-4,\n",
    "        clip_range=0.2,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.2,\n",
    "        tensorboard_log=\"./ppo_CnnPolicy/\"\n",
    "    )\n",
    "\n",
    "# 학습을 종료할 최대 에피소드 설정\n",
    "max_episodes = 800  # 원하는 에피소드 수\n",
    "stop_callback = StopOnEpisodeCallback(max_episodes=max_episodes)\n",
    "\n",
    "# 학습 시작 (max_episodes 이후 자동 종료)\n",
    "ppo_model.learn(total_timesteps=100000000, callback=stop_callback)\n",
    "\n",
    "# 모델 저장\n",
    "print(f\"Start model saving at {save_dir}\")\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "ppo_model_save_path = os.path.join(save_dir, f\"ppo_CnnPolicy_{current_date}.zip\")\n",
    "ppo_model.save(ppo_model_save_path)\n",
    "print(f\"PPO Model saved at {ppo_model_save_path}\")\n",
    "\n",
    "# 최신 모델 업데이트\n",
    "print(f\"Start model updating at {save_dir}\")\n",
    "ppo_model_latest_path = os.path.join(save_dir, \"ppo_CnnPolicy_latest.zip\")\n",
    "ppo_model.save(ppo_model_latest_path)\n",
    "print(f\"Latest PPO Model updated at {ppo_model_latest_path}\")\n",
    "\n",
    "#얘는 오래 걸리니깐 리셋 함수에 진행한 데이터셋 번호를 기억하던지 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
