{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:39:09\n",
      "Logging to ./ppo_with_mask/PPO_119\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:39:10.196012\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633627, PSNR Change: -0.000071, PSNR Diff: -0.000071 (New Max), Reward: -5.65, 08:39:11\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631062, PSNR After: 27.631058, PSNR Change: -0.000004, PSNR Diff: -0.002640, Reward: -0.31, 08:41:12\n",
      "Step: 200, PSNR Before: 27.627619, PSNR After: 27.627668, PSNR Change: 0.000050, PSNR Diff: -0.006029, Reward: 3.97, 08:43:14\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 0   |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 312 |\n",
      "|    total_timesteps | 256 |\n",
      "----------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010082 < -0.01 at step 300\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 1, Max consecutive episode failures: 1\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:45:24\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:45:24.869915\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 08:45:26\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629232, PSNR After: 27.629147, PSNR Change: -0.000086, PSNR Diff: -0.004551, Reward: -6.87, 08:47:28\n",
      "Step: 200, PSNR Before: 27.626316, PSNR After: 27.626459, PSNR Change: 0.000143, PSNR Diff: -0.007238, Reward: 11.44, 08:49:32\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 301           |\n",
      "|    ep_rew_mean          | -882          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 636           |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6763806e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000289      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.206         |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -7.21e-05     |\n",
      "|    value_loss           | 0.84          |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 251\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 2, Max consecutive episode failures: 2\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:50:43\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:50:44.026132\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633430, PSNR Change: -0.000267, PSNR Diff: -0.000267 (New Max), Reward: -21.36, 08:50:45\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633430, PSNR After: 27.633453, PSNR Change: 0.000023, PSNR Diff: -0.000244 (New Max), Reward: 1.83, 08:50:46\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629452, PSNR After: 27.629499, PSNR Change: 0.000048, PSNR Diff: -0.004198, Reward: 3.81, 08:52:47\n",
      "Step: 200, PSNR Before: 27.626345, PSNR After: 27.626259, PSNR Change: -0.000086, PSNR Diff: -0.007439, Reward: -6.87, 08:54:52\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 276           |\n",
      "|    ep_rew_mean          | -888          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 961           |\n",
      "|    total_timesteps      | 768           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8184226e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.144         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.038         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000105     |\n",
      "|    value_loss           | 0.148         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 277\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 3, Max consecutive episode failures: 3\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 08:56:35\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 08:56:36.374402\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633526, PSNR Change: -0.000172, PSNR Diff: -0.000172 (New Max), Reward: -13.73, 08:56:37\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629129, PSNR After: 27.629129, PSNR Change: 0.000000, PSNR Diff: -0.004568, Reward: 0.00, 08:58:41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 277          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1290         |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.098991e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.174        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00012     |\n",
      "|    value_loss           | 0.225        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624681, PSNR After: 27.624666, PSNR Change: -0.000015, PSNR Diff: -0.009031, Reward: -1.22, 09:00:55\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010063 < -0.01 at step 220\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 4, Max consecutive episode failures: 4\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:01:21\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:01:22.280500\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633671, PSNR Change: -0.000027, PSNR Diff: -0.000027 (New Max), Reward: -2.14, 09:01:23\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631344, PSNR After: 27.631342, PSNR Change: -0.000002, PSNR Diff: -0.002356, Reward: -0.15, 09:03:30\n",
      "Step: 200, PSNR Before: 27.627567, PSNR After: 27.627552, PSNR Change: -0.000015, PSNR Diff: -0.006145, Reward: -1.22, 09:05:38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 263          |\n",
      "|    ep_rew_mean          | -891         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1623         |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.613562e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.00521      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.215        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    value_loss           | 0.457        |\n",
      "------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 288\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 5, Max consecutive episode failures: 5\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:07:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:07:37.780428\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633669, PSNR Change: -0.000029, PSNR Diff: -0.000029 (New Max), Reward: -2.29, 09:07:38\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630955, PSNR After: 27.630930, PSNR Change: -0.000025, PSNR Diff: -0.002768, Reward: -1.98, 09:09:46\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 268          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1958         |\n",
      "|    total_timesteps      | 1536         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.476207e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0168      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0727       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000161    |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.625391, PSNR After: 27.625238, PSNR Change: -0.000153, PSNR Diff: -0.008459, Reward: -12.21, 09:12:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010080 < -0.01 at step 270\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 6, Max consecutive episode failures: 6\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:13:30\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:13:31.544093\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633694, PSNR Change: -0.000004, PSNR Diff: -0.000004 (New Max), Reward: -0.31, 09:13:32\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633688, PSNR After: 27.633709, PSNR Change: 0.000021, PSNR Diff: 0.000011 (New Max), Reward: 1.68, 09:13:39\u001b[0m\n",
      "\u001b[94mStep: 8, PSNR Before: 27.633682, PSNR After: 27.633728, PSNR Change: 0.000046, PSNR Diff: 0.000031 (New Max), Reward: 3.66, 09:13:41\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630022, PSNR After: 27.630033, PSNR Change: 0.000011, PSNR Diff: -0.003664, Reward: 0.92, 09:15:38\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 2290          |\n",
      "|    total_timesteps      | 1792          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0221265e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0108       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.16          |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000178     |\n",
      "|    value_loss           | 0.383         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627369, PSNR After: 27.627262, PSNR Change: -0.000107, PSNR Diff: -0.006435, Reward: -8.54, 09:17:52\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010014 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 7, Max consecutive episode failures: 7\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:19:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:19:49.211097\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.634045, PSNR Change: 0.000347, PSNR Diff: 0.000347 (New Max), Reward: 27.77, 09:19:50\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631104, PSNR After: 27.631069, PSNR Change: -0.000034, PSNR Diff: -0.002628, Reward: -2.75, 09:21:56\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 2622          |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1664815e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0332        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0406        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000197     |\n",
      "|    value_loss           | 0.108         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.628286, PSNR After: 27.628162, PSNR Change: -0.000124, PSNR Diff: -0.005535, Reward: -9.92, 09:24:10\n",
      "Step: 300, PSNR Before: 27.624249, PSNR After: 27.624205, PSNR Change: -0.000044, PSNR Diff: -0.009493, Reward: -3.51, 09:26:19\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010073 < -0.01 at step 312\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 8, Max consecutive episode failures: 8\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:26:35\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:26:36.286397\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633619, PSNR Change: -0.000078, PSNR Diff: -0.000078 (New Max), Reward: -6.26, 09:26:37\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 277           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2957          |\n",
      "|    total_timesteps      | 2304          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4039688e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.104         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0633        |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000215     |\n",
      "|    value_loss           | 0.124         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630299, PSNR After: 27.630318, PSNR Change: 0.000019, PSNR Diff: -0.003380, Reward: 1.53, 09:28:51\n",
      "Step: 200, PSNR Before: 27.625862, PSNR After: 27.625835, PSNR Change: -0.000027, PSNR Diff: -0.007862, Reward: -2.14, 09:30:59\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010078 < -0.01 at step 251\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 9, Max consecutive episode failures: 9\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:32:05\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:32:06.024651\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633724, PSNR Change: 0.000027, PSNR Diff: 0.000027 (New Max), Reward: 2.14, 09:32:07\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 274           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 3293          |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4854595e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0726        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.047         |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000222     |\n",
      "|    value_loss           | 0.142         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629721, PSNR After: 27.629650, PSNR Change: -0.000071, PSNR Diff: -0.004047, Reward: -5.65, 09:34:22\n",
      "Step: 200, PSNR Before: 27.626022, PSNR After: 27.626028, PSNR Change: 0.000006, PSNR Diff: -0.007669, Reward: 0.46, 09:36:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 260\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 10, Max consecutive episode failures: 10\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:37:47\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:37:48.156766\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633535, PSNR Change: -0.000162, PSNR Diff: -0.000162 (New Max), Reward: -12.97, 09:37:49\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633535, PSNR After: 27.633537, PSNR Change: 0.000002, PSNR Diff: -0.000160 (New Max), Reward: 0.15, 09:37:50\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633537, PSNR After: 27.633608, PSNR Change: 0.000071, PSNR Diff: -0.000090 (New Max), Reward: 5.65, 09:37:51\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633574, PSNR After: 27.633663, PSNR Change: 0.000090, PSNR Diff: -0.000034 (New Max), Reward: 7.17, 09:37:54\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633663, PSNR After: 27.633698, PSNR Change: 0.000034, PSNR Diff: 0.000000 (New Max), Reward: 2.75, 09:37:55\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633698, PSNR After: 27.633751, PSNR Change: 0.000053, PSNR Diff: 0.000053 (New Max), Reward: 4.27, 09:37:56\u001b[0m\n",
      "\u001b[94mStep: 8, PSNR Before: 27.633751, PSNR After: 27.633785, PSNR Change: 0.000034, PSNR Diff: 0.000088 (New Max), Reward: 2.75, 09:37:58\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 273          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 3628         |\n",
      "|    total_timesteps      | 2816         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.557637e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0804       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0975       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000231    |\n",
      "|    value_loss           | 0.222        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629406, PSNR After: 27.629456, PSNR Change: 0.000050, PSNR Diff: -0.004242, Reward: 3.97, 09:40:03\n",
      "Step: 200, PSNR Before: 27.625372, PSNR After: 27.625322, PSNR Change: -0.000050, PSNR Diff: -0.008375, Reward: -3.97, 09:42:12\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010042 < -0.01 at step 229\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 11, Max consecutive episode failures: 11\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:42:49\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:42:49.765071\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633625, PSNR Change: -0.000072, PSNR Diff: -0.000072 (New Max), Reward: -5.80, 09:42:50\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633625, PSNR After: 27.633675, PSNR Change: 0.000050, PSNR Diff: -0.000023 (New Max), Reward: 3.97, 09:42:52\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633640, PSNR After: 27.633715, PSNR Change: 0.000074, PSNR Diff: 0.000017 (New Max), Reward: 5.95, 09:42:54\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630293, PSNR After: 27.630299, PSNR Change: 0.000006, PSNR Diff: -0.003399, Reward: 0.46, 09:44:57\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 3962          |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8277206e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00226       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0944        |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000245     |\n",
      "|    value_loss           | 0.19          |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626860, PSNR After: 27.626831, PSNR Change: -0.000029, PSNR Diff: -0.006866, Reward: -2.29, 09:47:12\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010048 < -0.01 at step 289\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 12, Max consecutive episode failures: 12\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:49:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:49:07.740027\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633665, PSNR Change: -0.000032, PSNR Diff: -0.000032 (New Max), Reward: -2.59, 09:49:08\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 271          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 4297         |\n",
      "|    total_timesteps      | 3328         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.876615e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0396       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0822       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000256    |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630798, PSNR After: 27.630730, PSNR Change: -0.000069, PSNR Diff: -0.002968, Reward: -5.49, 09:51:23\n",
      "Step: 200, PSNR Before: 27.627804, PSNR After: 27.627792, PSNR Change: -0.000011, PSNR Diff: -0.005905, Reward: -0.92, 09:53:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010023 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 13, Max consecutive episode failures: 13\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 09:55:27\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 09:55:28.002670\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633802, PSNR Change: 0.000105, PSNR Diff: 0.000105 (New Max), Reward: 8.39, 09:55:29\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633734, PSNR After: 27.633818, PSNR Change: 0.000084, PSNR Diff: 0.000120 (New Max), Reward: 6.71, 09:55:32\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 272          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 4630         |\n",
      "|    total_timesteps      | 3584         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.188608e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.0582       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00028     |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630262, PSNR After: 27.630157, PSNR Change: -0.000105, PSNR Diff: -0.003540, Reward: -8.39, 09:57:41\n",
      "Step: 200, PSNR Before: 27.626638, PSNR After: 27.626534, PSNR Change: -0.000105, PSNR Diff: -0.007164, Reward: -8.39, 09:59:48\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010029 < -0.01 at step 272\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 14, Max consecutive episode failures: 14\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:01:19\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:01:20.438316\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 10:01:21\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633677, PSNR After: 27.633684, PSNR Change: 0.000008, PSNR Diff: -0.000013 (New Max), Reward: 0.61, 10:01:24\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633677, PSNR After: 27.633705, PSNR Change: 0.000029, PSNR Diff: 0.000008 (New Max), Reward: 2.29, 10:01:26\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633705, PSNR After: 27.633745, PSNR Change: 0.000040, PSNR Diff: 0.000048 (New Max), Reward: 3.20, 10:01:28\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 4962          |\n",
      "|    total_timesteps      | 3840          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1606684e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0186        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0663        |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000278     |\n",
      "|    value_loss           | 0.147         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.631290, PSNR After: 27.631165, PSNR Change: -0.000126, PSNR Diff: -0.002533, Reward: -10.07, 10:03:34\n",
      "Step: 200, PSNR Before: 27.626322, PSNR After: 27.626322, PSNR Change: 0.000000, PSNR Diff: -0.007376, Reward: 0.00, 10:05:41\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010017 < -0.01 at step 255\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 15, Max consecutive episode failures: 15\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:06:51\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:06:51.849668\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633554, PSNR Change: -0.000143, PSNR Diff: -0.000143 (New Max), Reward: -11.44, 10:06:53\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 5293          |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2328459e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.000799     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0596        |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000275     |\n",
      "|    value_loss           | 0.133         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.628977, PSNR After: 27.629005, PSNR Change: 0.000029, PSNR Diff: -0.004692, Reward: 2.29, 10:09:06\n",
      "Step: 200, PSNR Before: 27.626842, PSNR After: 27.626808, PSNR Change: -0.000034, PSNR Diff: -0.006889, Reward: -2.75, 10:11:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010166 < -0.01 at step 280\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 16, Max consecutive episode failures: 16\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:12:56\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 5627          |\n",
      "|    total_timesteps      | 4352          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3259781e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000441      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.101         |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000284     |\n",
      "|    value_loss           | 0.219         |\n",
      "-------------------------------------------\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:13:04.129921\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633696, PSNR Change: -0.000002, PSNR Diff: -0.000002 (New Max), Reward: -0.15, 10:13:05\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630795, PSNR After: 27.630775, PSNR Change: -0.000019, PSNR Diff: -0.002922, Reward: -1.53, 10:15:12\n",
      "Step: 200, PSNR Before: 27.626589, PSNR After: 27.626034, PSNR Change: -0.000555, PSNR Diff: -0.007664, Reward: -44.40, 10:17:19\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -895          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 5960          |\n",
      "|    total_timesteps      | 4608          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6868656e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0155        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0655        |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000305     |\n",
      "|    value_loss           | 0.151         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010233 < -0.01 at step 286\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 17, Max consecutive episode failures: 17\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:19:14\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:19:15.156829\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633657, PSNR Change: -0.000040, PSNR Diff: -0.000040 (New Max), Reward: -3.20, 10:19:16\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629692, PSNR After: 27.629679, PSNR Change: -0.000013, PSNR Diff: -0.004019, Reward: -1.07, 10:21:19\n",
      "Step: 200, PSNR Before: 27.627211, PSNR After: 27.627066, PSNR Change: -0.000145, PSNR Diff: -0.006632, Reward: -11.60, 10:23:23\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 273           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 6284          |\n",
      "|    total_timesteps      | 4864          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7869828e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00162       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0424        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000296     |\n",
      "|    value_loss           | 0.151         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.623873, PSNR After: 27.623974, PSNR Change: 0.000101, PSNR Diff: -0.009724, Reward: 8.09, 10:25:36\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010025 < -0.01 at step 319\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 18, Max consecutive episode failures: 18\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:26:00\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:26:01.597234\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633625, PSNR Change: -0.000072, PSNR Diff: -0.000072 (New Max), Reward: -5.80, 10:26:02\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629200, PSNR After: 27.629101, PSNR Change: -0.000099, PSNR Diff: -0.004597, Reward: -7.93, 10:28:08\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 276           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 6615          |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2107346e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0405        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0424        |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00033      |\n",
      "|    value_loss           | 0.0951        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624018, PSNR After: 27.623941, PSNR Change: -0.000076, PSNR Diff: -0.009756, Reward: -6.10, 10:30:21\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010059 < -0.01 at step 205\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 19, Max consecutive episode failures: 19\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:30:28\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:30:29.089712\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633636, PSNR Change: -0.000061, PSNR Diff: -0.000061 (New Max), Reward: -4.88, 10:30:30\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629753, PSNR After: 27.629700, PSNR Change: -0.000053, PSNR Diff: -0.003998, Reward: -4.27, 10:32:35\n",
      "Step: 200, PSNR Before: 27.625778, PSNR After: 27.625587, PSNR Change: -0.000191, PSNR Diff: -0.008110, Reward: -15.26, 10:34:42\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 6946          |\n",
      "|    total_timesteps      | 5376          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8847717e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0225       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0902        |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.000322     |\n",
      "|    value_loss           | 0.213         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010042 < -0.01 at step 235\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 20, Max consecutive episode failures: 20\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:35:33\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:35:34.508316\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633783, PSNR Change: 0.000086, PSNR Diff: 0.000086 (New Max), Reward: 6.87, 10:35:35\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633747, PSNR After: 27.633850, PSNR Change: 0.000103, PSNR Diff: 0.000153 (New Max), Reward: 8.24, 10:35:40\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630051, PSNR After: 27.629833, PSNR Change: -0.000217, PSNR Diff: -0.003864, Reward: -17.40, 10:37:40\n",
      "Step: 200, PSNR Before: 27.625856, PSNR After: 27.625807, PSNR Change: -0.000050, PSNR Diff: -0.007891, Reward: -3.97, 10:39:48\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 7277          |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3038668e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000925      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0927        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000354     |\n",
      "|    value_loss           | 0.193         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010014 < -0.01 at step 234\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 21, Max consecutive episode failures: 21\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:40:38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:40:38.987547\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633575, PSNR Change: -0.000122, PSNR Diff: -0.000122 (New Max), Reward: -9.77, 10:40:40\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629776, PSNR After: 27.629698, PSNR Change: -0.000078, PSNR Diff: -0.004000, Reward: -6.26, 10:42:45\n",
      "Step: 200, PSNR Before: 27.627342, PSNR After: 27.627254, PSNR Change: -0.000088, PSNR Diff: -0.006443, Reward: -7.02, 10:44:52\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 7607          |\n",
      "|    total_timesteps      | 5888          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2340176e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0188       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0941        |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000338     |\n",
      "|    value_loss           | 0.234         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010078 < -0.01 at step 269\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 22, Max consecutive episode failures: 22\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:46:26\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:46:27.143285\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633717, PSNR Change: 0.000019, PSNR Diff: 0.000019 (New Max), Reward: 1.53, 10:46:28\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633717, PSNR After: 27.633739, PSNR Change: 0.000023, PSNR Diff: 0.000042 (New Max), Reward: 1.83, 10:46:29\u001b[0m\n",
      "Step: 100, PSNR Before: 27.632118, PSNR After: 27.632113, PSNR Change: -0.000006, PSNR Diff: -0.001585, Reward: -0.46, 10:48:34\n",
      "Step: 200, PSNR Before: 27.629353, PSNR After: 27.629337, PSNR Change: -0.000015, PSNR Diff: -0.004360, Reward: -1.22, 10:50:41\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 7939          |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6344863e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0117       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0996        |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000363     |\n",
      "|    value_loss           | 0.187         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.625692, PSNR After: 27.625648, PSNR Change: -0.000044, PSNR Diff: -0.008049, Reward: -3.51, 10:52:55\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010006 < -0.01 at step 352\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 23, Max consecutive episode failures: 23\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:54:01\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:54:02.448001\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633570, PSNR Change: -0.000128, PSNR Diff: -0.000128 (New Max), Reward: -10.22, 10:54:03\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630060, PSNR After: 27.629976, PSNR Change: -0.000084, PSNR Diff: -0.003721, Reward: -6.71, 10:56:09\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 8271          |\n",
      "|    total_timesteps      | 6400          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7416037e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0422       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0138        |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000456     |\n",
      "|    value_loss           | 0.0408        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624435, PSNR After: 27.624226, PSNR Change: -0.000210, PSNR Diff: -0.009472, Reward: -16.78, 10:58:24\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010193 < -0.01 at step 220\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 24, Max consecutive episode failures: 24\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 10:58:49\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 10:58:50.230278\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633774, PSNR Change: 0.000076, PSNR Diff: 0.000076 (New Max), Reward: 6.10, 10:58:51\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633774, PSNR After: 27.633791, PSNR Change: 0.000017, PSNR Diff: 0.000093 (New Max), Reward: 1.37, 10:58:52\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633759, PSNR After: 27.634029, PSNR Change: 0.000271, PSNR Diff: 0.000332 (New Max), Reward: 21.67, 10:58:55\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629910, PSNR After: 27.629833, PSNR Change: -0.000076, PSNR Diff: -0.003864, Reward: -6.10, 11:00:57\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 8603          |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0698797e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0278       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0859        |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00039      |\n",
      "|    value_loss           | 0.179         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627260, PSNR After: 27.627220, PSNR Change: -0.000040, PSNR Diff: -0.006477, Reward: -3.20, 11:03:10\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010010 < -0.01 at step 282\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 25, Max consecutive episode failures: 25\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:04:53\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:04:54.125706\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633703, PSNR Change: 0.000006, PSNR Diff: 0.000006 (New Max), Reward: 0.46, 11:04:55\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630327, PSNR After: 27.630318, PSNR Change: -0.000010, PSNR Diff: -0.003380, Reward: -0.76, 11:07:00\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 8933          |\n",
      "|    total_timesteps      | 6912          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2724423e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.00178       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.082         |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.000398     |\n",
      "|    value_loss           | 0.169         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626186, PSNR After: 27.625984, PSNR Change: -0.000202, PSNR Diff: -0.007713, Reward: -16.17, 11:09:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010002 < -0.01 at step 240\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 26, Max consecutive episode failures: 26\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:10:05\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:10:05.821525\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633677, PSNR Change: -0.000021, PSNR Diff: -0.000021 (New Max), Reward: -1.68, 11:10:07\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630272, PSNR After: 27.630522, PSNR Change: 0.000250, PSNR Diff: -0.003176, Reward: 19.99, 11:12:13\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -894         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 9264         |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.206093e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.00748     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000416    |\n",
      "|    value_loss           | 0.0863       |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.624197, PSNR After: 27.624159, PSNR Change: -0.000038, PSNR Diff: -0.009539, Reward: -3.05, 11:14:26\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010201 < -0.01 at step 209\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 27, Max consecutive episode failures: 27\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:14:38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:14:38.685296\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633644, PSNR Change: -0.000053, PSNR Diff: -0.000053 (New Max), Reward: -4.27, 11:14:39\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633644, PSNR After: 27.633665, PSNR Change: 0.000021, PSNR Diff: -0.000032 (New Max), Reward: 1.68, 11:14:41\u001b[0m\n",
      "Step: 100, PSNR Before: 27.628887, PSNR After: 27.628895, PSNR Change: 0.000008, PSNR Diff: -0.004803, Reward: 0.61, 11:16:44\n",
      "Step: 200, PSNR Before: 27.626301, PSNR After: 27.626276, PSNR Change: -0.000025, PSNR Diff: -0.007421, Reward: -1.98, 11:18:51\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 267           |\n",
      "|    ep_rew_mean          | -894          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 9594          |\n",
      "|    total_timesteps      | 7424          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0105155e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0139        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0926        |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000441     |\n",
      "|    value_loss           | 0.223         |\n",
      "-------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010130 < -0.01 at step 292\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 28, Max consecutive episode failures: 28\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:20:54\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:20:55.582822\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633656, PSNR Change: -0.000042, PSNR Diff: -0.000042 (New Max), Reward: -3.36, 11:20:56\u001b[0m\n",
      "\u001b[94mStep: 29, PSNR Before: 27.633644, PSNR After: 27.633694, PSNR Change: 0.000050, PSNR Diff: -0.000004 (New Max), Reward: 3.97, 11:21:32\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630753, PSNR After: 27.630751, PSNR Change: -0.000002, PSNR Diff: -0.002947, Reward: -0.15, 11:23:02\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 9924          |\n",
      "|    total_timesteps      | 7680          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6216883e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0295        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.102         |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000412     |\n",
      "|    value_loss           | 0.178         |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626627, PSNR After: 27.626600, PSNR Change: -0.000027, PSNR Diff: -0.007097, Reward: -2.14, 11:25:15\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010136 < -0.01 at step 264\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 29, Max consecutive episode failures: 29\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:26:36\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:26:37.057146\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633638, PSNR Change: -0.000059, PSNR Diff: -0.000059 (New Max), Reward: -4.73, 11:26:38\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633638, PSNR After: 27.633646, PSNR Change: 0.000008, PSNR Diff: -0.000051 (New Max), Reward: 0.61, 11:26:39\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630236, PSNR After: 27.630312, PSNR Change: 0.000076, PSNR Diff: -0.003386, Reward: 6.10, 11:28:43\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 268           |\n",
      "|    ep_rew_mean          | -893          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 10254         |\n",
      "|    total_timesteps      | 7936          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5146014e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0225       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.021         |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000497     |\n",
      "|    value_loss           | 0.0681        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626129, PSNR After: 27.625788, PSNR Change: -0.000341, PSNR Diff: -0.007910, Reward: -27.31, 11:30:56\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010216 < -0.01 at step 261\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 30, Max consecutive episode failures: 30\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:32:11\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:32:12.531057\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633652, PSNR Change: -0.000046, PSNR Diff: -0.000046 (New Max), Reward: -3.66, 11:32:13\u001b[0m\n",
      "\u001b[94mStep: 11, PSNR Before: 27.633648, PSNR After: 27.633675, PSNR Change: 0.000027, PSNR Diff: -0.000023 (New Max), Reward: 2.14, 11:32:25\u001b[0m\n",
      "Step: 100, PSNR Before: 27.631357, PSNR After: 27.631369, PSNR Change: 0.000011, PSNR Diff: -0.002329, Reward: 0.92, 11:34:15\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 268          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 10577        |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.455222e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.012        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0627       |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000448    |\n",
      "|    value_loss           | 0.141        |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.627153, PSNR After: 27.627159, PSNR Change: 0.000006, PSNR Diff: -0.006538, Reward: 0.46, 11:36:27\n",
      "Step: 300, PSNR Before: 27.623951, PSNR After: 27.623949, PSNR Change: -0.000002, PSNR Diff: -0.009748, Reward: -0.15, 11:38:34\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010380 < -0.01 at step 302\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 31, Max consecutive episode failures: 31\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:38:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:38:38.128443\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633726, PSNR Change: 0.000029, PSNR Diff: 0.000029 (New Max), Reward: 2.29, 11:38:39\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633726, PSNR After: 27.633762, PSNR Change: 0.000036, PSNR Diff: 0.000065 (New Max), Reward: 2.90, 11:38:40\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633762, PSNR After: 27.633770, PSNR Change: 0.000008, PSNR Diff: 0.000072 (New Max), Reward: 0.61, 11:38:41\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633770, PSNR After: 27.633797, PSNR Change: 0.000027, PSNR Diff: 0.000099 (New Max), Reward: 2.14, 11:38:43\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630634, PSNR After: 27.630617, PSNR Change: -0.000017, PSNR Diff: -0.003080, Reward: -1.37, 11:40:44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -893         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 10908        |\n",
      "|    total_timesteps      | 8448         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.565824e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.0191      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0353       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    value_loss           | 0.0817       |\n",
      "------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626656, PSNR After: 27.626659, PSNR Change: 0.000004, PSNR Diff: -0.007038, Reward: 0.31, 11:42:58\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010023 < -0.01 at step 271\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 32, Max consecutive episode failures: 32\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:44:29\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:44:29.756410\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633638, PSNR Change: -0.000059, PSNR Diff: -0.000059 (New Max), Reward: -4.73, 11:44:30\u001b[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 269         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 11239       |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.48899e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.0293     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.000484   |\n",
      "|    value_loss           | 0.091       |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 27.629238, PSNR After: 27.629429, PSNR Change: 0.000191, PSNR Diff: -0.004269, Reward: 15.26, 11:46:42\n",
      "Step: 200, PSNR Before: 27.625864, PSNR After: 27.625877, PSNR Change: 0.000013, PSNR Diff: -0.007820, Reward: 1.07, 11:48:49\n",
      "Step: 300, PSNR Before: 27.624676, PSNR After: 27.624586, PSNR Change: -0.000090, PSNR Diff: -0.009111, Reward: -7.17, 11:50:56\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010250 < -0.01 at step 335\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 33, Max consecutive episode failures: 33\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:51:41\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:51:41.741123\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633644, PSNR Change: -0.000053, PSNR Diff: -0.000053 (New Max), Reward: -4.27, 11:51:42\u001b[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 271         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 11570       |\n",
      "|    total_timesteps      | 8960        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.91507e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.00516    |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000501   |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 27.629280, PSNR After: 27.629271, PSNR Change: -0.000010, PSNR Diff: -0.004427, Reward: -0.76, 11:53:55\n",
      "Step: 200, PSNR Before: 27.624996, PSNR After: 27.624985, PSNR Change: -0.000011, PSNR Diff: -0.008713, Reward: -0.92, 11:56:02\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010084 < -0.01 at step 227\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 34, Max consecutive episode failures: 34\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 11:56:36\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 11:56:36.853010\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633636, PSNR Change: -0.000061, PSNR Diff: -0.000061 (New Max), Reward: -4.88, 11:56:38\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633636, PSNR After: 27.633726, PSNR Change: 0.000090, PSNR Diff: 0.000029 (New Max), Reward: 7.17, 11:56:39\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633726, PSNR After: 27.633762, PSNR Change: 0.000036, PSNR Diff: 0.000065 (New Max), Reward: 2.90, 11:56:40\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 270           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 11901         |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1858065e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.0179        |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.031         |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00066      |\n",
      "|    value_loss           | 0.063         |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629835, PSNR After: 27.629805, PSNR Change: -0.000031, PSNR Diff: -0.003893, Reward: -2.44, 11:58:50\n",
      "Step: 200, PSNR Before: 27.626228, PSNR After: 27.626188, PSNR Change: -0.000040, PSNR Diff: -0.007509, Reward: -3.20, 12:00:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010073 < -0.01 at step 255\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 35, Max consecutive episode failures: 35\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:02:07\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:02:08.261542\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633781, PSNR Change: 0.000084, PSNR Diff: 0.000084 (New Max), Reward: 6.71, 12:02:09\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633781, PSNR After: 27.633823, PSNR Change: 0.000042, PSNR Diff: 0.000126 (New Max), Reward: 3.36, 12:02:10\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 12232        |\n",
      "|    total_timesteps      | 9472         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.399358e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.019        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000523    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630777, PSNR After: 27.630486, PSNR Change: -0.000292, PSNR Diff: -0.003212, Reward: -23.35, 12:04:22\n",
      "Step: 200, PSNR Before: 27.626263, PSNR After: 27.626259, PSNR Change: -0.000004, PSNR Diff: -0.007439, Reward: -0.31, 12:06:30\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010101 < -0.01 at step 259\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 36, Max consecutive episode failures: 36\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:07:46\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:07:47.394767\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633440, PSNR Change: -0.000257, PSNR Diff: -0.000257 (New Max), Reward: -20.60, 12:07:48\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633440, PSNR After: 27.633480, PSNR Change: 0.000040, PSNR Diff: -0.000217 (New Max), Reward: 3.20, 12:07:49\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633480, PSNR After: 27.633484, PSNR Change: 0.000004, PSNR Diff: -0.000214 (New Max), Reward: 0.31, 12:07:51\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633404, PSNR After: 27.633535, PSNR Change: 0.000132, PSNR Diff: -0.000162 (New Max), Reward: 10.53, 12:07:53\u001b[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 12567        |\n",
      "|    total_timesteps      | 9728         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.923227e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | 0.035        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000534    |\n",
      "|    value_loss           | 0.0986       |\n",
      "------------------------------------------\n",
      "Step: 100, PSNR Before: 27.629971, PSNR After: 27.629612, PSNR Change: -0.000359, PSNR Diff: -0.004086, Reward: -28.69, 12:10:01\n",
      "Step: 200, PSNR Before: 27.626476, PSNR After: 27.626446, PSNR Change: -0.000031, PSNR Diff: -0.007252, Reward: -2.44, 12:12:09\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010056 < -0.01 at step 272\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 37, Max consecutive episode failures: 37\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:13:41\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:13:42.371349\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633690, PSNR Change: -0.000008, PSNR Diff: -0.000008 (New Max), Reward: -0.61, 12:13:43\u001b[0m\n",
      "\u001b[94mStep: 3, PSNR Before: 27.633598, PSNR After: 27.633772, PSNR Change: 0.000174, PSNR Diff: 0.000074 (New Max), Reward: 13.89, 12:13:46\u001b[0m\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 269        |\n",
      "|    ep_rew_mean          | -892       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 12900      |\n",
      "|    total_timesteps      | 9984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 7.6578e-07 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -14.6      |\n",
      "|    explained_variance   | 0.0178     |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 0.0669     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.000533  |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Step: 100, PSNR Before: 27.632347, PSNR After: 27.632355, PSNR Change: 0.000008, PSNR Diff: -0.001343, Reward: 0.61, 12:15:57\n",
      "Step: 200, PSNR Before: 27.627407, PSNR After: 27.627375, PSNR Change: -0.000032, PSNR Diff: -0.006323, Reward: -2.59, 12:18:04\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 269           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 13233         |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1085067e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.015         |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0556        |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000664     |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Step: 300, PSNR Before: 27.624771, PSNR After: 27.624828, PSNR Change: 0.000057, PSNR Diff: -0.008869, Reward: 4.58, 12:20:18\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010122 < -0.01 at step 329\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 38, Max consecutive episode failures: 38\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:20:55\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:20:56.356121\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633652, PSNR Change: -0.000046, PSNR Diff: -0.000046 (New Max), Reward: -3.66, 12:20:57\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629644, PSNR After: 27.629654, PSNR Change: 0.000010, PSNR Diff: -0.004044, Reward: 0.76, 12:23:03\n",
      "Step: 200, PSNR Before: 27.626110, PSNR After: 27.626459, PSNR Change: 0.000349, PSNR Diff: -0.007238, Reward: 27.92, 12:25:09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 271          |\n",
      "|    ep_rew_mean          | -892         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 13564        |\n",
      "|    total_timesteps      | 10496        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.969808e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.6        |\n",
      "|    explained_variance   | -0.00762     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 0.0325       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000621    |\n",
      "|    value_loss           | 0.109        |\n",
      "------------------------------------------\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010027 < -0.01 at step 283\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 39, Max consecutive episode failures: 39\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:27:03\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:27:03.984741\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633646, PSNR Change: -0.000051, PSNR Diff: -0.000051 (New Max), Reward: -4.12, 12:27:05\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630825, PSNR After: 27.630833, PSNR Change: 0.000008, PSNR Diff: -0.002865, Reward: 0.61, 12:29:10\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 13896         |\n",
      "|    total_timesteps      | 10752         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1820812e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | 0.000957      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0416        |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000674     |\n",
      "|    value_loss           | 0.0963        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626955, PSNR After: 27.626869, PSNR Change: -0.000086, PSNR Diff: -0.006828, Reward: -6.87, 12:31:24\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010021 < -0.01 at step 291\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 40, Max consecutive episode failures: 40\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:33:20\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:33:21.265995\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633713, PSNR Change: 0.000015, PSNR Diff: 0.000015 (New Max), Reward: 1.22, 12:33:22\u001b[0m\n",
      "Step: 100, PSNR Before: 27.629879, PSNR After: 27.629835, PSNR Change: -0.000044, PSNR Diff: -0.003862, Reward: -3.51, 12:35:27\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 14229         |\n",
      "|    total_timesteps      | 11008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2239907e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.0115       |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0307        |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.000664     |\n",
      "|    value_loss           | 0.0568        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.625216, PSNR After: 27.625237, PSNR Change: 0.000021, PSNR Diff: -0.008461, Reward: 1.68, 12:37:41\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010035 < -0.01 at step 244\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 41, Max consecutive episode failures: 41\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:38:37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:38:37.870606\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633665, PSNR Change: -0.000032, PSNR Diff: -0.000032 (New Max), Reward: -2.59, 12:38:39\u001b[0m\n",
      "\u001b[94mStep: 4, PSNR Before: 27.633629, PSNR After: 27.633701, PSNR Change: 0.000072, PSNR Diff: 0.000004 (New Max), Reward: 5.80, 12:38:42\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633600, PSNR After: 27.633705, PSNR Change: 0.000105, PSNR Diff: 0.000008 (New Max), Reward: 8.39, 12:38:45\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633705, PSNR After: 27.633722, PSNR Change: 0.000017, PSNR Diff: 0.000025 (New Max), Reward: 1.37, 12:38:46\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630796, PSNR After: 27.630798, PSNR Change: 0.000002, PSNR Diff: -0.002899, Reward: 0.15, 12:40:44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 271         |\n",
      "|    ep_rew_mean          | -892        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 14558       |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.80217e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | -0.0178     |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.000597   |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Step: 200, PSNR Before: 27.627762, PSNR After: 27.627737, PSNR Change: -0.000025, PSNR Diff: -0.005960, Reward: -1.98, 12:42:57\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010094 < -0.01 at step 287\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 42, Max consecutive episode failures: 42\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:44:48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:44:49.277709\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633648, PSNR Change: -0.000050, PSNR Diff: -0.000050 (New Max), Reward: -3.97, 12:44:50\u001b[0m\n",
      "\u001b[94mStep: 5, PSNR Before: 27.633631, PSNR After: 27.633663, PSNR Change: 0.000032, PSNR Diff: -0.000034 (New Max), Reward: 2.59, 12:44:55\u001b[0m\n",
      "\u001b[94mStep: 6, PSNR Before: 27.633663, PSNR After: 27.633675, PSNR Change: 0.000011, PSNR Diff: -0.000023 (New Max), Reward: 0.92, 12:44:56\u001b[0m\n",
      "\u001b[94mStep: 7, PSNR Before: 27.633675, PSNR After: 27.633690, PSNR Change: 0.000015, PSNR Diff: -0.000008 (New Max), Reward: 1.22, 12:44:58\u001b[0m\n",
      "Step: 100, PSNR Before: 27.630178, PSNR After: 27.630213, PSNR Change: 0.000034, PSNR Diff: -0.003485, Reward: 2.75, 12:46:55\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 271           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 14890         |\n",
      "|    total_timesteps      | 11520         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3469253e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00727      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0198        |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.000681     |\n",
      "|    value_loss           | 0.0553        |\n",
      "-------------------------------------------\n",
      "Step: 200, PSNR Before: 27.626574, PSNR After: 27.626448, PSNR Change: -0.000126, PSNR Diff: -0.007250, Reward: -10.07, 12:49:08\n",
      "Step: 300, PSNR Before: 27.624083, PSNR After: 27.624088, PSNR Change: 0.000006, PSNR Diff: -0.009609, Reward: 0.46, 12:51:14\n",
      "\u001b[91mEpisode failed: PSNR Diff -0.010094 < -0.01 at step 309\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 43, Max consecutive episode failures: 43\u001b[0m\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 12:51:25\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.001724, Initial PSNR: 27.633698, 2024-12-24 12:51:26.478288\n",
      "\u001b[94mStep: 1, PSNR Before: 27.633698, PSNR After: 27.633663, PSNR Change: -0.000034, PSNR Diff: -0.000034 (New Max), Reward: -2.75, 12:51:27\u001b[0m\n",
      "\u001b[94mStep: 2, PSNR Before: 27.633663, PSNR After: 27.633734, PSNR Change: 0.000071, PSNR Diff: 0.000036 (New Max), Reward: 5.65, 12:51:28\u001b[0m\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 272           |\n",
      "|    ep_rew_mean          | -892          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 15218         |\n",
      "|    total_timesteps      | 11776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1418015e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.6         |\n",
      "|    explained_variance   | -0.00105      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 0.0316        |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000626     |\n",
      "|    value_loss           | 0.0799        |\n",
      "-------------------------------------------\n",
      "Step: 100, PSNR Before: 27.630823, PSNR After: 27.630447, PSNR Change: -0.000376, PSNR Diff: -0.003250, Reward: -30.06, 12:53:40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 576\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# PPO \u001b[39;00m\n\u001b[1;32m    561\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 576\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[1;32m    579\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_normalize.py:181\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 413\u001b[0m, in \u001b[0;36mBinaryHologramEnv.step\u001b[0;34m(self, action, lr, z)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mflatten()[action] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m#       \u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m#print(f\"Invalid action taken at step {self.steps}, action: {action}\")\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m: mask}\n",
      "Cell \u001b[0;32mIn[2], line 381\u001b[0m, in \u001b[0;36mBinaryHologramEnv.create_action_mask\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(obs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    380\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((obs[channel] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (obs[channel] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindices):\n\u001b[1;32m    382\u001b[0m         pixel_idx \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m+\u001b[39m row \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;241m+\u001b[39m col\n\u001b[1;32m    383\u001b[0m         mask[pixel_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m#   \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#      \n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 512, 512).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(512)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((512, 512))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 512 or target.shape[-2] < 512:\n",
    "            target = torchvision.transforms.Resize(512)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv \n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        \"\"\"\n",
    "          .\n",
    "        target_function:   (MSE  PSNR)  .\n",
    "        trainloader:   .\n",
    "        max_steps:   .\n",
    "        T_PSNR:  PSNR .\n",
    "        T_steps: PSNR     .\n",
    "        T_PSNR_DIFF: PSNR  .\n",
    "        max_allowed_changes:        (: 1).\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        #  : (1, 8, 512, 512)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.float32)\n",
    "\n",
    "        #  :     (512 * 512 * 8)\n",
    "        self.num_pixels = 8 * 512 * 512\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        #     \n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        #  \n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  #  \n",
    "\n",
    "        #   \n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        #     \n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        #    \n",
    "        self.retry_current_target = False  #    \n",
    "\n",
    "        #    \n",
    "        self.consecutive_fail_count = 0  #   \n",
    "        self.max_consecutive_failures = 0  #     \n",
    "\n",
    "        #  PSNR_DIFF  \n",
    "        self.max_psnr_diff = float('-inf')  #   PSNR_DIFF \n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        \"\"\"\n",
    "          .\n",
    "              .\n",
    "        -    . \n",
    "        - BinaryNet    .\n",
    "        -  (state)   .\n",
    "        -  PSNR MSE  .\n",
    "        -      .\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional):   . Default None.\n",
    "            options (dict, optional):  . Default None.\n",
    "            lr (float, optional): . Default 1e-4.\n",
    "            z (float, optional):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            dict:    .\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.retry_current_target:  #    \n",
    "            self.consecutive_fail_count += 1\n",
    "        else:\n",
    "            self.consecutive_fail_count = 0  #     \n",
    "\n",
    "        self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        if not self.retry_current_target:  #      \n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        #    PSNR  \n",
    "        self.max_psnr_diff = float('-inf')\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  #  \n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta  \n",
    "\n",
    "        # \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE  PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  #  PSNR \n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\")\n",
    "\n",
    "        self.retry_current_target = False  #      \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "          ,     .\n",
    "\n",
    "        Args:\n",
    "            z (float):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            dict:    .\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            #    \n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  #  numpy  \n",
    "\n",
    "        #     \n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  #  Torch  \n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  #   \n",
    "\n",
    "        #  \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        #  MSE PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        #   \n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        #    \n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        #  \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # (  ) \n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "            .\n",
    "        -  0.2 ~ 0.8     .\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): .\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:    1,    0.\n",
    "        \"\"\"\n",
    "        #     \n",
    "        mask = np.zeros(self.num_pixels, dtype=np.int8)\n",
    "\n",
    "        # (1, 8, 512, 512) -> (8, 512, 512) \n",
    "        obs = observation.squeeze()\n",
    "\n",
    "        #      \n",
    "        for channel in range(obs.shape[0]):\n",
    "            indices = np.where((obs[channel] > 0) & (obs[channel] < 1))\n",
    "            for row, col in zip(*indices):\n",
    "                pixel_idx = channel * 512 * 512 + row * 512 + col\n",
    "                mask[pixel_idx] = 1  #   \n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "           .\n",
    "        -  (action) ,   .\n",
    "        -    PSNR (psnr_change)  .\n",
    "        - psnr_change 0     .\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray):   .\n",
    "            lr (float, optional): . Default 1e-4.\n",
    "            z (float, optional):  . Default 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray):  .\n",
    "            float:  .\n",
    "            bool:  .\n",
    "            bool: Truncated .\n",
    "            dict:   (MSE, PSNR, PSNR_DIFF,   ).\n",
    "        \"\"\"\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        #   \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        if mask.flatten()[action] == 0:\n",
    "            #       \n",
    "            #print(f\"Invalid action taken at step {self.steps}, action: {action}\")\n",
    "            return self.observation, -10.0, False, False, {\"mask\": mask}\n",
    "\n",
    "        #   PSNR \n",
    "        binary_before = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_before = tt.Tensor(binary_before, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_before = tt.simulate(binary_before, z).abs()**2\n",
    "        result_before = torch.mean(sim_before, dim=1, keepdim=True)\n",
    "        psnr_before = tt.relativeLoss(result_before, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        #     \n",
    "        channel = action // (512 * 512)\n",
    "        pixel_index = action % (512 * 512)\n",
    "        row = pixel_index // 512\n",
    "        col = pixel_index % 512\n",
    "\n",
    "        #   \n",
    "        self.state[0, channel, row, col] = 1 - self.state[0, channel, row, col]\n",
    "\n",
    "        #     \n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # PSNR  \n",
    "        psnr_change = psnr_after - psnr_before\n",
    "\n",
    "        # PSNR_CHANGE 0     \n",
    "        #if psnr_change < 0:\n",
    "            #print(f\"Invalid action: PSNR Change {psnr_change:.6f} < 0 at step {self.steps}\")\n",
    "        #    return self.observation, -10.0, False, False, {\"psnr_before\": psnr_before, \"psnr_after\": psnr_after, \"psnr_change\": psnr_change, \"mask\": mask}\n",
    "\n",
    "        #  PSNR_DIFF \n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  #  PSNR_DIFF \n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  #  PSNR_DIFF \n",
    "\n",
    "        #  \n",
    "        reward = psnr_change * 80000  # PSNR (psnr_change)  \n",
    "\n",
    "        #   \n",
    "        if psnr_diff < -0.01:\n",
    "            print(f\"\\033[91mEpisode failed: PSNR Diff {psnr_diff:.6f} < -0.01 at step {self.steps}\\033[0m\")\n",
    "            self.retry_current_target = True  #     \n",
    "            return self.observation, -100.0, True, False, {\"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        #  PSNR_DIFF  \n",
    "        if is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time}\\033[0m\"\n",
    "            )\n",
    "\n",
    "        #   (100  )\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                  f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f}, Reward: {reward:.2f}, {current_time}\")\n",
    "\n",
    "        #   : PSNR >= T_PSNR  PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  #   \n",
    "                reward += 100  #     \n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        #  \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "\n",
    "        del binary_before, binary_after, sim_before, sim_after, result_before, result_after\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  #  \n",
    "padding = 0\n",
    "\n",
    "# Dataset512  \n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet  \n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#   \n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "#      \n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  #  train_loader \n",
    "    max_steps=10000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker  \n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized  \n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO \n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-5,  #  \n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping \n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#   \n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO   \n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])]  #    \n",
    "#)\n",
    "\n",
    "# PPO  \n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",  # LSTM    MLP \n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  #   \n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "# \n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#  \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# PPO \n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#   \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "#from sb3_contrib import RecurrentPPO\n",
    "\n",
    "#policy_kwargs = dict(\n",
    "#    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  #    \n",
    "#    lstm_hidden_size=128,  # LSTM  \n",
    "#    shared_lstm=False  #  LSTM \n",
    "#)\n",
    "\n",
    "#ppo_model = RecurrentPPO(\n",
    "#    \"MlpLstmPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=256,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    gae_lambda=0.95,\n",
    "#    learning_rate=1e-5,\n",
    "#    clip_range=0.2,\n",
    "#    vf_coef=0.5,\n",
    "#    max_grad_norm=0.5,  #   \n",
    "#    tensorboard_log=\"./ppo_with_mask/\",\n",
    "#    policy_kwargs=policy_kwargs\n",
    "#)\n",
    "\n",
    "\n",
    "# \n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#  \n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "#   \n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback \n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  #   ( )\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#\n",
    "\n",
    "#   ( )\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
