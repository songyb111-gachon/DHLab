{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "Initial MSE: 0.002123, Initial PSNR: 26.730062, 05:39:22\n",
      "Logging to ./ppo_with_mask/RecurrentPPO_2\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002123, Initial PSNR: 26.730062, 2024-12-23_05-38-47\n",
      "Step: 1, MSE: 0.060387, PSNR: 12.190535, 05:39:22\n",
      "Step: 2, MSE: 0.060127, PSNR: 12.209311, 05:39:22\n",
      "Step: 3, MSE: 0.060253, PSNR: 12.200228, 05:39:22\n",
      "Step: 4, MSE: 0.060222, PSNR: 12.202459, 05:39:22\n",
      "Step: 5, MSE: 0.060315, PSNR: 12.195770, 05:39:23\n",
      "Step: 6, MSE: 0.060212, PSNR: 12.203205, 05:39:23\n",
      "Step: 7, MSE: 0.060100, PSNR: 12.211226, 05:39:23\n",
      "Step: 8, MSE: 0.060208, PSNR: 12.203482, 05:39:23\n",
      "Step: 9, MSE: 0.060275, PSNR: 12.198639, 05:39:23\n",
      "Step: 10, MSE: 0.060221, PSNR: 12.202497, 05:39:23\n",
      "Step: 11, MSE: 0.060464, PSNR: 12.185025, 05:39:23\n",
      "Step: 12, MSE: 0.060231, PSNR: 12.201806, 05:39:24\n",
      "Step: 13, MSE: 0.060419, PSNR: 12.188269, 05:39:24\n",
      "Step: 14, MSE: 0.060243, PSNR: 12.200901, 05:39:24\n",
      "Step: 15, MSE: 0.060382, PSNR: 12.190953, 05:39:24\n",
      "Step: 16, MSE: 0.060231, PSNR: 12.201772, 05:39:24\n",
      "Step: 17, MSE: 0.060157, PSNR: 12.207125, 05:39:24\n",
      "Step: 18, MSE: 0.060138, PSNR: 12.208504, 05:39:24\n",
      "Step: 19, MSE: 0.060330, PSNR: 12.194643, 05:39:24\n",
      "Step: 20, MSE: 0.060266, PSNR: 12.199247, 05:39:25\n",
      "Step: 21, MSE: 0.060203, PSNR: 12.203807, 05:39:25\n",
      "Step: 22, MSE: 0.060202, PSNR: 12.203911, 05:39:25\n",
      "Step: 23, MSE: 0.060309, PSNR: 12.196198, 05:39:25\n",
      "Step: 24, MSE: 0.060388, PSNR: 12.190476, 05:39:25\n",
      "Step: 25, MSE: 0.060144, PSNR: 12.208051, 05:39:25\n",
      "Step: 26, MSE: 0.060317, PSNR: 12.195623, 05:39:25\n",
      "Step: 27, MSE: 0.060338, PSNR: 12.194057, 05:39:26\n",
      "Step: 28, MSE: 0.060137, PSNR: 12.208607, 05:39:26\n",
      "Step: 29, MSE: 0.060387, PSNR: 12.190550, 05:39:26\n",
      "Step: 30, MSE: 0.060385, PSNR: 12.190706, 05:39:26\n",
      "Step: 31, MSE: 0.060108, PSNR: 12.210656, 05:39:26\n",
      "Step: 32, MSE: 0.060202, PSNR: 12.203912, 05:39:26\n",
      "Step: 33, MSE: 0.060268, PSNR: 12.199125, 05:39:27\n",
      "Step: 34, MSE: 0.060226, PSNR: 12.202188, 05:39:27\n",
      "Step: 35, MSE: 0.060502, PSNR: 12.182281, 05:39:27\n",
      "Step: 36, MSE: 0.060181, PSNR: 12.205439, 05:39:27\n",
      "Step: 37, MSE: 0.060266, PSNR: 12.199302, 05:39:27\n",
      "Step: 38, MSE: 0.060466, PSNR: 12.184888, 05:39:27\n",
      "Step: 39, MSE: 0.060392, PSNR: 12.190172, 05:39:27\n",
      "Step: 40, MSE: 0.060297, PSNR: 12.197079, 05:39:28\n",
      "Step: 41, MSE: 0.060260, PSNR: 12.199732, 05:39:28\n",
      "Step: 42, MSE: 0.060350, PSNR: 12.193231, 05:39:28\n",
      "Step: 43, MSE: 0.060260, PSNR: 12.199690, 05:39:28\n",
      "Step: 44, MSE: 0.060190, PSNR: 12.204732, 05:39:28\n",
      "Step: 45, MSE: 0.060266, PSNR: 12.199295, 05:39:28\n",
      "Step: 46, MSE: 0.060295, PSNR: 12.197156, 05:39:28\n",
      "Step: 47, MSE: 0.060229, PSNR: 12.201910, 05:39:29\n",
      "Step: 48, MSE: 0.060414, PSNR: 12.188618, 05:39:29\n",
      "Step: 49, MSE: 0.060242, PSNR: 12.201027, 05:39:29\n",
      "Step: 50, MSE: 0.060585, PSNR: 12.176372, 05:39:29\n",
      "Step: 51, MSE: 0.060184, PSNR: 12.205185, 05:39:29\n",
      "Step: 52, MSE: 0.060438, PSNR: 12.186892, 05:39:29\n",
      "Step: 53, MSE: 0.059996, PSNR: 12.218765, 05:39:29\n",
      "Step: 54, MSE: 0.060221, PSNR: 12.202549, 05:39:30\n",
      "Step: 55, MSE: 0.060208, PSNR: 12.203476, 05:39:30\n",
      "Step: 56, MSE: 0.060115, PSNR: 12.210192, 05:39:30\n",
      "Step: 57, MSE: 0.060263, PSNR: 12.199528, 05:39:30\n",
      "Step: 58, MSE: 0.060260, PSNR: 12.199696, 05:39:30\n",
      "Step: 59, MSE: 0.060386, PSNR: 12.190614, 05:39:30\n",
      "Step: 60, MSE: 0.060149, PSNR: 12.207746, 05:39:31\n",
      "Step: 61, MSE: 0.060268, PSNR: 12.199142, 05:39:31\n",
      "Step: 62, MSE: 0.060210, PSNR: 12.203322, 05:39:31\n",
      "Step: 63, MSE: 0.060416, PSNR: 12.188510, 05:39:31\n",
      "Step: 64, MSE: 0.060159, PSNR: 12.206963, 05:39:31\n",
      "Step: 65, MSE: 0.060297, PSNR: 12.197039, 05:39:31\n",
      "Step: 66, MSE: 0.060423, PSNR: 12.188002, 05:39:31\n",
      "Step: 67, MSE: 0.060222, PSNR: 12.202473, 05:39:32\n",
      "Step: 68, MSE: 0.060175, PSNR: 12.205828, 05:39:32\n",
      "Step: 69, MSE: 0.060212, PSNR: 12.203139, 05:39:32\n",
      "Step: 70, MSE: 0.060267, PSNR: 12.199178, 05:39:32\n",
      "Step: 71, MSE: 0.060538, PSNR: 12.179749, 05:39:32\n",
      "Step: 72, MSE: 0.060590, PSNR: 12.176007, 05:39:32\n",
      "Step: 73, MSE: 0.060407, PSNR: 12.189122, 05:39:32\n",
      "Step: 74, MSE: 0.060137, PSNR: 12.208596, 05:39:33\n",
      "Step: 75, MSE: 0.060316, PSNR: 12.195683, 05:39:33\n",
      "Step: 76, MSE: 0.060353, PSNR: 12.193003, 05:39:33\n",
      "Step: 77, MSE: 0.060239, PSNR: 12.201258, 05:39:33\n",
      "Step: 78, MSE: 0.060185, PSNR: 12.205094, 05:39:33\n",
      "Step: 79, MSE: 0.060390, PSNR: 12.190346, 05:39:33\n",
      "Step: 80, MSE: 0.060334, PSNR: 12.194408, 05:39:33\n",
      "Step: 81, MSE: 0.060320, PSNR: 12.195395, 05:39:34\n",
      "Step: 82, MSE: 0.060223, PSNR: 12.202392, 05:39:34\n",
      "Step: 83, MSE: 0.060339, PSNR: 12.193987, 05:39:34\n",
      "Step: 84, MSE: 0.060348, PSNR: 12.193337, 05:39:34\n",
      "Step: 85, MSE: 0.060331, PSNR: 12.194567, 05:39:34\n",
      "Step: 86, MSE: 0.060043, PSNR: 12.215361, 05:39:34\n",
      "Step: 87, MSE: 0.060314, PSNR: 12.195827, 05:39:35\n",
      "Step: 88, MSE: 0.060243, PSNR: 12.200903, 05:39:35\n",
      "Step: 89, MSE: 0.060386, PSNR: 12.190622, 05:39:35\n",
      "Step: 90, MSE: 0.060344, PSNR: 12.193630, 05:39:35\n",
      "Step: 91, MSE: 0.060306, PSNR: 12.196384, 05:39:35\n",
      "Step: 92, MSE: 0.060335, PSNR: 12.194338, 05:39:35\n",
      "Step: 93, MSE: 0.060361, PSNR: 12.192469, 05:39:35\n",
      "Step: 94, MSE: 0.060132, PSNR: 12.208966, 05:39:35\n",
      "Step: 95, MSE: 0.060201, PSNR: 12.203981, 05:39:36\n",
      "Step: 96, MSE: 0.060178, PSNR: 12.205624, 05:39:36\n",
      "Step: 97, MSE: 0.060289, PSNR: 12.197637, 05:39:36\n",
      "Step: 98, MSE: 0.060240, PSNR: 12.201130, 05:39:36\n",
      "Step: 99, MSE: 0.060222, PSNR: 12.202423, 05:39:36\n",
      "Step: 100, MSE: 0.060264, PSNR: 12.199419, 05:39:36\n",
      "Step: 101, MSE: 0.060317, PSNR: 12.195583, 05:39:36\n",
      "Step: 102, MSE: 0.060113, PSNR: 12.210306, 05:39:37\n",
      "Step: 103, MSE: 0.060348, PSNR: 12.193374, 05:39:37\n",
      "Step: 104, MSE: 0.060272, PSNR: 12.198854, 05:39:37\n",
      "Step: 105, MSE: 0.060158, PSNR: 12.207087, 05:39:37\n",
      "Step: 106, MSE: 0.060331, PSNR: 12.194625, 05:39:37\n",
      "Step: 107, MSE: 0.060279, PSNR: 12.198376, 05:39:37\n",
      "Step: 108, MSE: 0.060369, PSNR: 12.191891, 05:39:37\n",
      "Step: 109, MSE: 0.060076, PSNR: 12.213011, 05:39:37\n",
      "Step: 110, MSE: 0.060414, PSNR: 12.188640, 05:39:38\n",
      "Step: 111, MSE: 0.060205, PSNR: 12.203699, 05:39:38\n",
      "Step: 112, MSE: 0.060150, PSNR: 12.207612, 05:39:38\n",
      "Step: 113, MSE: 0.060286, PSNR: 12.197844, 05:39:38\n",
      "Step: 114, MSE: 0.060146, PSNR: 12.207956, 05:39:38\n",
      "Step: 115, MSE: 0.060192, PSNR: 12.204592, 05:39:38\n",
      "Step: 116, MSE: 0.060296, PSNR: 12.197096, 05:39:38\n",
      "Step: 117, MSE: 0.060226, PSNR: 12.202190, 05:39:39\n",
      "Step: 118, MSE: 0.060318, PSNR: 12.195534, 05:39:39\n",
      "Step: 119, MSE: 0.060428, PSNR: 12.187609, 05:39:39\n",
      "Step: 120, MSE: 0.060152, PSNR: 12.207506, 05:39:39\n",
      "Step: 121, MSE: 0.060318, PSNR: 12.195548, 05:39:39\n",
      "Step: 122, MSE: 0.060287, PSNR: 12.197762, 05:39:39\n",
      "Step: 123, MSE: 0.060081, PSNR: 12.212605, 05:39:40\n",
      "Step: 124, MSE: 0.060272, PSNR: 12.198826, 05:39:40\n",
      "Step: 125, MSE: 0.060199, PSNR: 12.204123, 05:39:40\n",
      "Step: 126, MSE: 0.060346, PSNR: 12.193501, 05:39:40\n",
      "Step: 127, MSE: 0.060323, PSNR: 12.195201, 05:39:40\n",
      "Step: 128, MSE: 0.060117, PSNR: 12.210020, 05:39:40\n",
      "Step: 129, MSE: 0.060178, PSNR: 12.205648, 05:39:41\n",
      "Step: 130, MSE: 0.060199, PSNR: 12.204143, 05:39:41\n",
      "Step: 131, MSE: 0.060332, PSNR: 12.194520, 05:39:41\n",
      "Step: 132, MSE: 0.060145, PSNR: 12.208017, 05:39:41\n",
      "Step: 133, MSE: 0.060260, PSNR: 12.199718, 05:39:41\n",
      "Step: 134, MSE: 0.060188, PSNR: 12.204916, 05:39:41\n",
      "Step: 135, MSE: 0.060235, PSNR: 12.201539, 05:39:42\n",
      "Step: 136, MSE: 0.060321, PSNR: 12.195335, 05:39:42\n",
      "Step: 137, MSE: 0.060507, PSNR: 12.181927, 05:39:42\n",
      "Step: 138, MSE: 0.060220, PSNR: 12.202612, 05:39:42\n",
      "Step: 139, MSE: 0.060401, PSNR: 12.189569, 05:39:42\n",
      "Step: 140, MSE: 0.060075, PSNR: 12.213051, 05:39:42\n",
      "Step: 141, MSE: 0.060185, PSNR: 12.205126, 05:39:43\n",
      "Step: 142, MSE: 0.060406, PSNR: 12.189205, 05:39:43\n",
      "Step: 143, MSE: 0.060218, PSNR: 12.202702, 05:39:43\n",
      "Step: 144, MSE: 0.060317, PSNR: 12.195627, 05:39:43\n",
      "Step: 145, MSE: 0.060311, PSNR: 12.196007, 05:39:43\n",
      "Step: 146, MSE: 0.060312, PSNR: 12.195973, 05:39:43\n",
      "Step: 147, MSE: 0.060280, PSNR: 12.198244, 05:39:44\n",
      "Step: 148, MSE: 0.060359, PSNR: 12.192606, 05:39:44\n",
      "Step: 149, MSE: 0.060233, PSNR: 12.201639, 05:39:44\n",
      "Step: 150, MSE: 0.060422, PSNR: 12.188077, 05:39:44\n",
      "Step: 151, MSE: 0.060201, PSNR: 12.203937, 05:39:44\n",
      "Step: 152, MSE: 0.060095, PSNR: 12.211614, 05:39:44\n",
      "Step: 153, MSE: 0.060161, PSNR: 12.206818, 05:39:44\n",
      "Step: 154, MSE: 0.060133, PSNR: 12.208896, 05:39:44\n",
      "Step: 155, MSE: 0.060508, PSNR: 12.181849, 05:39:45\n",
      "Step: 156, MSE: 0.060419, PSNR: 12.188259, 05:39:45\n",
      "Step: 157, MSE: 0.060240, PSNR: 12.201117, 05:39:45\n",
      "Step: 158, MSE: 0.060292, PSNR: 12.197418, 05:39:45\n",
      "Step: 159, MSE: 0.060266, PSNR: 12.199295, 05:39:45\n",
      "Step: 160, MSE: 0.060159, PSNR: 12.207001, 05:39:45\n",
      "Step: 161, MSE: 0.060126, PSNR: 12.209373, 05:39:45\n",
      "Step: 162, MSE: 0.060247, PSNR: 12.200672, 05:39:46\n",
      "Step: 163, MSE: 0.060381, PSNR: 12.190996, 05:39:46\n",
      "Step: 164, MSE: 0.060186, PSNR: 12.205047, 05:39:46\n",
      "Step: 165, MSE: 0.060325, PSNR: 12.195007, 05:39:46\n",
      "Step: 166, MSE: 0.060271, PSNR: 12.198948, 05:39:46\n",
      "Step: 167, MSE: 0.060332, PSNR: 12.194502, 05:39:46\n",
      "Step: 168, MSE: 0.060359, PSNR: 12.192570, 05:39:46\n",
      "Step: 169, MSE: 0.060169, PSNR: 12.206248, 05:39:47\n",
      "Step: 170, MSE: 0.060250, PSNR: 12.200401, 05:39:47\n",
      "Step: 171, MSE: 0.060099, PSNR: 12.211353, 05:39:47\n",
      "Step: 172, MSE: 0.060214, PSNR: 12.203030, 05:39:47\n",
      "Step: 173, MSE: 0.060244, PSNR: 12.200876, 05:39:47\n",
      "Step: 174, MSE: 0.060309, PSNR: 12.196164, 05:39:48\n",
      "Step: 175, MSE: 0.060335, PSNR: 12.194320, 05:39:48\n",
      "Step: 176, MSE: 0.060128, PSNR: 12.209209, 05:39:48\n",
      "Step: 177, MSE: 0.060360, PSNR: 12.192526, 05:39:48\n",
      "Step: 178, MSE: 0.060193, PSNR: 12.204552, 05:39:48\n",
      "Step: 179, MSE: 0.060455, PSNR: 12.185655, 05:39:48\n",
      "Step: 180, MSE: 0.060315, PSNR: 12.195724, 05:39:48\n",
      "Step: 181, MSE: 0.060168, PSNR: 12.206341, 05:39:49\n",
      "Step: 182, MSE: 0.060250, PSNR: 12.200411, 05:39:49\n",
      "Step: 183, MSE: 0.060180, PSNR: 12.205506, 05:39:49\n",
      "Step: 184, MSE: 0.060138, PSNR: 12.208506, 05:39:49\n",
      "Step: 185, MSE: 0.060246, PSNR: 12.200752, 05:39:49\n",
      "Step: 186, MSE: 0.060332, PSNR: 12.194527, 05:39:49\n",
      "Step: 187, MSE: 0.060209, PSNR: 12.203396, 05:39:49\n",
      "Step: 188, MSE: 0.060225, PSNR: 12.202198, 05:39:50\n",
      "Step: 189, MSE: 0.060249, PSNR: 12.200476, 05:39:50\n",
      "Step: 190, MSE: 0.060255, PSNR: 12.200052, 05:39:50\n",
      "Step: 191, MSE: 0.060248, PSNR: 12.200600, 05:39:50\n",
      "Step: 192, MSE: 0.060200, PSNR: 12.204066, 05:39:50\n",
      "Step: 193, MSE: 0.060437, PSNR: 12.186995, 05:39:50\n",
      "Step: 194, MSE: 0.060418, PSNR: 12.188322, 05:39:50\n",
      "Step: 195, MSE: 0.060042, PSNR: 12.215425, 05:39:51\n",
      "Step: 196, MSE: 0.060227, PSNR: 12.202108, 05:39:51\n",
      "Step: 197, MSE: 0.060309, PSNR: 12.196213, 05:39:51\n",
      "Step: 198, MSE: 0.060327, PSNR: 12.194899, 05:39:51\n",
      "Step: 199, MSE: 0.060259, PSNR: 12.199791, 05:39:51\n",
      "Step: 200, MSE: 0.060014, PSNR: 12.217511, 05:39:51\n",
      "Step: 201, MSE: 0.060323, PSNR: 12.195179, 05:39:51\n",
      "Step: 202, MSE: 0.060363, PSNR: 12.192322, 05:39:52\n",
      "Step: 203, MSE: 0.060461, PSNR: 12.185276, 05:39:52\n",
      "Step: 204, MSE: 0.060258, PSNR: 12.199869, 05:39:52\n",
      "Step: 205, MSE: 0.060267, PSNR: 12.199216, 05:39:52\n",
      "Step: 206, MSE: 0.060236, PSNR: 12.201473, 05:39:52\n",
      "Step: 207, MSE: 0.060299, PSNR: 12.196890, 05:39:52\n",
      "Step: 208, MSE: 0.060353, PSNR: 12.193005, 05:39:52\n",
      "Step: 209, MSE: 0.060240, PSNR: 12.201138, 05:39:53\n",
      "Step: 210, MSE: 0.060243, PSNR: 12.200926, 05:39:53\n",
      "Step: 211, MSE: 0.060176, PSNR: 12.205753, 05:39:53\n",
      "Step: 212, MSE: 0.060456, PSNR: 12.185598, 05:39:53\n",
      "Step: 213, MSE: 0.060103, PSNR: 12.211066, 05:39:53\n",
      "Step: 214, MSE: 0.060132, PSNR: 12.208953, 05:39:53\n",
      "Step: 215, MSE: 0.060301, PSNR: 12.196764, 05:39:54\n",
      "Step: 216, MSE: 0.060208, PSNR: 12.203459, 05:39:54\n",
      "Step: 217, MSE: 0.060309, PSNR: 12.196215, 05:39:54\n",
      "Step: 218, MSE: 0.060487, PSNR: 12.183394, 05:39:54\n",
      "Step: 219, MSE: 0.060263, PSNR: 12.199460, 05:39:54\n",
      "Step: 220, MSE: 0.060199, PSNR: 12.204090, 05:39:54\n",
      "Step: 221, MSE: 0.060353, PSNR: 12.192993, 05:39:55\n",
      "Step: 222, MSE: 0.060289, PSNR: 12.197615, 05:39:55\n",
      "Step: 223, MSE: 0.060096, PSNR: 12.211565, 05:39:55\n",
      "Step: 224, MSE: 0.060290, PSNR: 12.197567, 05:39:55\n",
      "Step: 225, MSE: 0.060195, PSNR: 12.204380, 05:39:55\n",
      "Step: 226, MSE: 0.060125, PSNR: 12.209423, 05:39:55\n",
      "Step: 227, MSE: 0.060093, PSNR: 12.211754, 05:39:55\n",
      "Step: 228, MSE: 0.060350, PSNR: 12.193203, 05:39:56\n",
      "Step: 229, MSE: 0.060214, PSNR: 12.203005, 05:39:56\n",
      "Step: 230, MSE: 0.060228, PSNR: 12.202048, 05:39:56\n",
      "Step: 231, MSE: 0.060085, PSNR: 12.212360, 05:39:56\n",
      "Step: 232, MSE: 0.060112, PSNR: 12.210397, 05:39:56\n",
      "Step: 233, MSE: 0.060187, PSNR: 12.204996, 05:39:57\n",
      "Step: 234, MSE: 0.060369, PSNR: 12.191845, 05:39:57\n",
      "Step: 235, MSE: 0.060314, PSNR: 12.195808, 05:39:57\n",
      "Step: 236, MSE: 0.060308, PSNR: 12.196238, 05:39:57\n",
      "Step: 237, MSE: 0.060426, PSNR: 12.187773, 05:39:57\n",
      "Step: 238, MSE: 0.060455, PSNR: 12.185690, 05:39:57\n",
      "Step: 239, MSE: 0.060121, PSNR: 12.209744, 05:39:57\n",
      "Step: 240, MSE: 0.060152, PSNR: 12.207509, 05:39:57\n",
      "Step: 241, MSE: 0.060259, PSNR: 12.199797, 05:39:58\n",
      "Step: 242, MSE: 0.060402, PSNR: 12.189483, 05:39:58\n",
      "Step: 243, MSE: 0.060268, PSNR: 12.199167, 05:39:58\n",
      "Step: 244, MSE: 0.060198, PSNR: 12.204165, 05:39:58\n",
      "Step: 245, MSE: 0.060320, PSNR: 12.195385, 05:39:58\n",
      "Step: 246, MSE: 0.060267, PSNR: 12.199205, 05:39:58\n",
      "Step: 247, MSE: 0.060417, PSNR: 12.188398, 05:39:59\n",
      "Step: 248, MSE: 0.060089, PSNR: 12.212040, 05:39:59\n",
      "Step: 249, MSE: 0.060308, PSNR: 12.196227, 05:39:59\n",
      "Step: 250, MSE: 0.060376, PSNR: 12.191387, 05:39:59\n",
      "Step: 251, MSE: 0.060331, PSNR: 12.194584, 05:39:59\n",
      "Step: 252, MSE: 0.060274, PSNR: 12.198694, 05:39:59\n",
      "Step: 253, MSE: 0.060251, PSNR: 12.200385, 05:39:59\n",
      "Step: 254, MSE: 0.060295, PSNR: 12.197223, 05:40:00\n",
      "Step: 255, MSE: 0.060200, PSNR: 12.204036, 05:40:00\n",
      "Step: 256, MSE: 0.060243, PSNR: 12.200928, 05:40:00\n",
      "Step: 257, MSE: 0.060436, PSNR: 12.187075, 05:40:00\n",
      "Step: 258, MSE: 0.060198, PSNR: 12.204189, 05:40:00\n",
      "Step: 259, MSE: 0.060375, PSNR: 12.191419, 05:40:00\n",
      "Step: 260, MSE: 0.060234, PSNR: 12.201599, 05:40:01\n",
      "Step: 261, MSE: 0.060262, PSNR: 12.199544, 05:40:01\n",
      "Step: 262, MSE: 0.060249, PSNR: 12.200470, 05:40:01\n",
      "Step: 263, MSE: 0.060483, PSNR: 12.183681, 05:40:01\n",
      "Step: 264, MSE: 0.060117, PSNR: 12.210060, 05:40:01\n",
      "Step: 265, MSE: 0.060247, PSNR: 12.200623, 05:40:01\n",
      "Step: 266, MSE: 0.060259, PSNR: 12.199812, 05:40:01\n",
      "Step: 267, MSE: 0.060299, PSNR: 12.196873, 05:40:02\n",
      "Step: 268, MSE: 0.060279, PSNR: 12.198336, 05:40:02\n",
      "Step: 269, MSE: 0.060186, PSNR: 12.205065, 05:40:02\n",
      "Step: 270, MSE: 0.060469, PSNR: 12.184702, 05:40:02\n",
      "Step: 271, MSE: 0.060292, PSNR: 12.197400, 05:40:02\n",
      "Step: 272, MSE: 0.060441, PSNR: 12.186663, 05:40:02\n",
      "Step: 273, MSE: 0.060427, PSNR: 12.187706, 05:40:02\n",
      "Step: 274, MSE: 0.060222, PSNR: 12.202453, 05:40:03\n",
      "Step: 275, MSE: 0.060357, PSNR: 12.192749, 05:40:03\n",
      "Step: 276, MSE: 0.060295, PSNR: 12.197178, 05:40:03\n",
      "Step: 277, MSE: 0.060171, PSNR: 12.206120, 05:40:03\n",
      "Step: 278, MSE: 0.060221, PSNR: 12.202487, 05:40:03\n",
      "Step: 279, MSE: 0.060174, PSNR: 12.205934, 05:40:03\n",
      "Step: 280, MSE: 0.060117, PSNR: 12.209995, 05:40:03\n",
      "Step: 281, MSE: 0.060320, PSNR: 12.195395, 05:40:03\n",
      "Step: 282, MSE: 0.060316, PSNR: 12.195705, 05:40:04\n",
      "Step: 283, MSE: 0.060243, PSNR: 12.200958, 05:40:04\n",
      "Step: 284, MSE: 0.060058, PSNR: 12.214268, 05:40:04\n",
      "Step: 285, MSE: 0.060368, PSNR: 12.191958, 05:40:04\n",
      "Step: 286, MSE: 0.060310, PSNR: 12.196095, 05:40:04\n",
      "Step: 287, MSE: 0.060299, PSNR: 12.196917, 05:40:04\n",
      "Step: 288, MSE: 0.060373, PSNR: 12.191606, 05:40:05\n",
      "Step: 289, MSE: 0.060319, PSNR: 12.195490, 05:40:05\n",
      "Step: 290, MSE: 0.060424, PSNR: 12.187940, 05:40:05\n",
      "Step: 291, MSE: 0.060284, PSNR: 12.197989, 05:40:05\n",
      "Step: 292, MSE: 0.060175, PSNR: 12.205832, 05:40:05\n",
      "Step: 293, MSE: 0.060258, PSNR: 12.199820, 05:40:05\n",
      "Step: 294, MSE: 0.060275, PSNR: 12.198654, 05:40:06\n",
      "Step: 295, MSE: 0.060305, PSNR: 12.196487, 05:40:06\n",
      "Step: 296, MSE: 0.060212, PSNR: 12.203201, 05:40:06\n",
      "Step: 297, MSE: 0.060247, PSNR: 12.200680, 05:40:06\n",
      "Step: 298, MSE: 0.060146, PSNR: 12.207943, 05:40:06\n",
      "Step: 299, MSE: 0.060396, PSNR: 12.189918, 05:40:06\n",
      "Step: 300, MSE: 0.060289, PSNR: 12.197633, 05:40:06\n",
      "Step: 301, MSE: 0.060012, PSNR: 12.217606, 05:40:07\n",
      "Step: 302, MSE: 0.060150, PSNR: 12.207644, 05:40:07\n",
      "Step: 303, MSE: 0.060476, PSNR: 12.184157, 05:40:07\n",
      "Step: 304, MSE: 0.060100, PSNR: 12.211239, 05:40:07\n",
      "Step: 305, MSE: 0.060227, PSNR: 12.202064, 05:40:07\n",
      "Step: 306, MSE: 0.060294, PSNR: 12.197226, 05:40:07\n",
      "Step: 307, MSE: 0.060344, PSNR: 12.193637, 05:40:07\n",
      "Step: 308, MSE: 0.060250, PSNR: 12.200440, 05:40:08\n",
      "Step: 309, MSE: 0.060296, PSNR: 12.197104, 05:40:08\n",
      "Step: 310, MSE: 0.060102, PSNR: 12.211088, 05:40:08\n",
      "Step: 311, MSE: 0.060289, PSNR: 12.197632, 05:40:08\n",
      "Step: 312, MSE: 0.060289, PSNR: 12.197603, 05:40:08\n",
      "Step: 313, MSE: 0.060272, PSNR: 12.198853, 05:40:08\n",
      "Step: 314, MSE: 0.060120, PSNR: 12.209805, 05:40:08\n",
      "Step: 315, MSE: 0.060250, PSNR: 12.200397, 05:40:09\n",
      "Step: 316, MSE: 0.060153, PSNR: 12.207433, 05:40:09\n",
      "Step: 317, MSE: 0.060287, PSNR: 12.197754, 05:40:09\n",
      "Step: 318, MSE: 0.060066, PSNR: 12.213717, 05:40:09\n",
      "Step: 319, MSE: 0.060285, PSNR: 12.197885, 05:40:09\n",
      "Step: 320, MSE: 0.060269, PSNR: 12.199062, 05:40:09\n",
      "Step: 321, MSE: 0.060271, PSNR: 12.198924, 05:40:10\n",
      "Step: 322, MSE: 0.060139, PSNR: 12.208443, 05:40:10\n",
      "Step: 323, MSE: 0.060499, PSNR: 12.182508, 05:40:10\n",
      "Step: 324, MSE: 0.060256, PSNR: 12.200010, 05:40:10\n",
      "Step: 325, MSE: 0.060307, PSNR: 12.196289, 05:40:10\n",
      "Step: 326, MSE: 0.060240, PSNR: 12.201161, 05:40:10\n",
      "Step: 327, MSE: 0.060376, PSNR: 12.191383, 05:40:10\n",
      "Step: 328, MSE: 0.060164, PSNR: 12.206613, 05:40:11\n",
      "Step: 329, MSE: 0.060294, PSNR: 12.197258, 05:40:11\n",
      "Step: 330, MSE: 0.060240, PSNR: 12.201151, 05:40:11\n",
      "Step: 331, MSE: 0.060358, PSNR: 12.192664, 05:40:11\n",
      "Step: 332, MSE: 0.060173, PSNR: 12.205952, 05:40:11\n",
      "Step: 333, MSE: 0.060284, PSNR: 12.197962, 05:40:11\n",
      "Step: 334, MSE: 0.060205, PSNR: 12.203653, 05:40:11\n",
      "Step: 335, MSE: 0.060141, PSNR: 12.208301, 05:40:12\n",
      "Step: 336, MSE: 0.060289, PSNR: 12.197624, 05:40:12\n",
      "Step: 337, MSE: 0.060411, PSNR: 12.188850, 05:40:12\n",
      "Step: 338, MSE: 0.060250, PSNR: 12.200461, 05:40:12\n",
      "Step: 339, MSE: 0.060353, PSNR: 12.193002, 05:40:12\n",
      "Step: 340, MSE: 0.060210, PSNR: 12.203344, 05:40:12\n",
      "Step: 341, MSE: 0.060292, PSNR: 12.197430, 05:40:12\n",
      "Step: 342, MSE: 0.060218, PSNR: 12.202769, 05:40:13\n",
      "Step: 343, MSE: 0.060265, PSNR: 12.199330, 05:40:13\n",
      "Step: 344, MSE: 0.060351, PSNR: 12.193185, 05:40:13\n",
      "Step: 345, MSE: 0.060036, PSNR: 12.215899, 05:40:13\n",
      "Step: 346, MSE: 0.060429, PSNR: 12.187513, 05:40:13\n",
      "Step: 347, MSE: 0.060218, PSNR: 12.202754, 05:40:13\n",
      "Step: 348, MSE: 0.060420, PSNR: 12.188201, 05:40:14\n",
      "Step: 349, MSE: 0.060185, PSNR: 12.205097, 05:40:14\n",
      "Step: 350, MSE: 0.060616, PSNR: 12.174154, 05:40:14\n",
      "Step: 351, MSE: 0.060226, PSNR: 12.202147, 05:40:14\n",
      "Step: 352, MSE: 0.060340, PSNR: 12.193979, 05:40:14\n",
      "Step: 353, MSE: 0.060197, PSNR: 12.204268, 05:40:14\n",
      "Step: 354, MSE: 0.060281, PSNR: 12.198160, 05:40:15\n",
      "Step: 355, MSE: 0.060312, PSNR: 12.195957, 05:40:15\n",
      "Step: 356, MSE: 0.060129, PSNR: 12.209183, 05:40:15\n",
      "Step: 357, MSE: 0.060330, PSNR: 12.194672, 05:40:15\n",
      "Step: 358, MSE: 0.060249, PSNR: 12.200484, 05:40:15\n",
      "Step: 359, MSE: 0.060375, PSNR: 12.191442, 05:40:15\n",
      "Step: 360, MSE: 0.060337, PSNR: 12.194141, 05:40:15\n",
      "Step: 361, MSE: 0.060234, PSNR: 12.201616, 05:40:16\n",
      "Step: 362, MSE: 0.060218, PSNR: 12.202712, 05:40:16\n",
      "Step: 363, MSE: 0.060267, PSNR: 12.199187, 05:40:16\n",
      "Step: 364, MSE: 0.060208, PSNR: 12.203455, 05:40:16\n",
      "Step: 365, MSE: 0.060232, PSNR: 12.201720, 05:40:16\n",
      "Step: 366, MSE: 0.060339, PSNR: 12.194006, 05:40:16\n",
      "Step: 367, MSE: 0.060144, PSNR: 12.208099, 05:40:16\n",
      "Step: 368, MSE: 0.060323, PSNR: 12.195181, 05:40:17\n",
      "Step: 369, MSE: 0.060216, PSNR: 12.202872, 05:40:17\n",
      "Step: 370, MSE: 0.060234, PSNR: 12.201606, 05:40:17\n",
      "Step: 371, MSE: 0.060244, PSNR: 12.200871, 05:40:17\n",
      "Step: 372, MSE: 0.060462, PSNR: 12.185206, 05:40:17\n",
      "Step: 373, MSE: 0.060326, PSNR: 12.194975, 05:40:17\n",
      "Step: 374, MSE: 0.060340, PSNR: 12.193958, 05:40:18\n",
      "Step: 375, MSE: 0.060273, PSNR: 12.198748, 05:40:18\n",
      "Step: 376, MSE: 0.060336, PSNR: 12.194224, 05:40:18\n",
      "Step: 377, MSE: 0.060088, PSNR: 12.212122, 05:40:18\n",
      "Step: 378, MSE: 0.060218, PSNR: 12.202725, 05:40:18\n",
      "Step: 379, MSE: 0.060239, PSNR: 12.201238, 05:40:18\n",
      "Step: 380, MSE: 0.060387, PSNR: 12.190538, 05:40:18\n",
      "Step: 381, MSE: 0.060370, PSNR: 12.191765, 05:40:19\n",
      "Step: 382, MSE: 0.060228, PSNR: 12.201991, 05:40:19\n",
      "Step: 383, MSE: 0.060252, PSNR: 12.200294, 05:40:19\n",
      "Step: 384, MSE: 0.060084, PSNR: 12.212440, 05:40:19\n",
      "Step: 385, MSE: 0.060167, PSNR: 12.206407, 05:40:19\n",
      "Step: 386, MSE: 0.060214, PSNR: 12.202999, 05:40:19\n",
      "Step: 387, MSE: 0.060048, PSNR: 12.215005, 05:40:20\n",
      "Step: 388, MSE: 0.060393, PSNR: 12.190099, 05:40:20\n",
      "Step: 389, MSE: 0.060192, PSNR: 12.204589, 05:40:20\n",
      "Step: 390, MSE: 0.060148, PSNR: 12.207783, 05:40:20\n",
      "Step: 391, MSE: 0.060224, PSNR: 12.202293, 05:40:20\n",
      "Step: 392, MSE: 0.060319, PSNR: 12.195454, 05:40:20\n",
      "Step: 393, MSE: 0.060192, PSNR: 12.204590, 05:40:21\n",
      "Step: 394, MSE: 0.060186, PSNR: 12.205013, 05:40:21\n",
      "Step: 395, MSE: 0.060496, PSNR: 12.182742, 05:40:21\n",
      "Step: 396, MSE: 0.060296, PSNR: 12.197113, 05:40:21\n",
      "Step: 397, MSE: 0.060264, PSNR: 12.199412, 05:40:21\n",
      "Step: 398, MSE: 0.060220, PSNR: 12.202614, 05:40:21\n",
      "Step: 399, MSE: 0.060253, PSNR: 12.200233, 05:40:21\n",
      "Step: 400, MSE: 0.060182, PSNR: 12.205338, 05:40:22\n",
      "Step: 401, MSE: 0.060197, PSNR: 12.204226, 05:40:22\n",
      "Step: 402, MSE: 0.060210, PSNR: 12.203332, 05:40:22\n",
      "Step: 403, MSE: 0.060170, PSNR: 12.206167, 05:40:22\n",
      "Step: 404, MSE: 0.060281, PSNR: 12.198210, 05:40:22\n",
      "Step: 405, MSE: 0.060371, PSNR: 12.191740, 05:40:22\n",
      "Step: 406, MSE: 0.060245, PSNR: 12.200792, 05:40:23\n",
      "Step: 407, MSE: 0.060291, PSNR: 12.197447, 05:40:23\n",
      "Step: 408, MSE: 0.060393, PSNR: 12.190099, 05:40:23\n",
      "Step: 409, MSE: 0.060224, PSNR: 12.202338, 05:40:23\n",
      "Step: 410, MSE: 0.060304, PSNR: 12.196560, 05:40:23\n",
      "Step: 411, MSE: 0.060370, PSNR: 12.191763, 05:40:23\n",
      "Step: 412, MSE: 0.060238, PSNR: 12.201265, 05:40:23\n",
      "Step: 413, MSE: 0.060320, PSNR: 12.195369, 05:40:24\n",
      "Step: 414, MSE: 0.060253, PSNR: 12.200209, 05:40:24\n",
      "Step: 415, MSE: 0.060345, PSNR: 12.193562, 05:40:24\n",
      "Step: 416, MSE: 0.060279, PSNR: 12.198326, 05:40:24\n",
      "Step: 417, MSE: 0.060251, PSNR: 12.200355, 05:40:24\n",
      "Step: 418, MSE: 0.060083, PSNR: 12.212452, 05:40:24\n",
      "Step: 419, MSE: 0.060285, PSNR: 12.197894, 05:40:25\n",
      "Step: 420, MSE: 0.060138, PSNR: 12.208480, 05:40:25\n",
      "Step: 421, MSE: 0.060207, PSNR: 12.203550, 05:40:25\n",
      "Step: 422, MSE: 0.060176, PSNR: 12.205795, 05:40:25\n",
      "Step: 423, MSE: 0.060205, PSNR: 12.203644, 05:40:25\n",
      "Step: 424, MSE: 0.060334, PSNR: 12.194367, 05:40:25\n",
      "Step: 425, MSE: 0.060126, PSNR: 12.209341, 05:40:25\n",
      "Step: 426, MSE: 0.060285, PSNR: 12.197908, 05:40:25\n",
      "Step: 427, MSE: 0.060182, PSNR: 12.205360, 05:40:26\n",
      "Step: 428, MSE: 0.060217, PSNR: 12.202812, 05:40:26\n",
      "Step: 429, MSE: 0.060115, PSNR: 12.210184, 05:40:26\n",
      "Step: 430, MSE: 0.060520, PSNR: 12.181016, 05:40:26\n",
      "Step: 431, MSE: 0.060199, PSNR: 12.204141, 05:40:26\n",
      "Step: 432, MSE: 0.060243, PSNR: 12.200922, 05:40:26\n",
      "Step: 433, MSE: 0.060114, PSNR: 12.210230, 05:40:27\n",
      "Step: 434, MSE: 0.060195, PSNR: 12.204372, 05:40:27\n",
      "Step: 435, MSE: 0.060283, PSNR: 12.198050, 05:40:27\n",
      "Step: 436, MSE: 0.060142, PSNR: 12.208213, 05:40:27\n",
      "Step: 437, MSE: 0.060090, PSNR: 12.212005, 05:40:27\n",
      "Step: 438, MSE: 0.060209, PSNR: 12.203416, 05:40:27\n",
      "Step: 439, MSE: 0.060292, PSNR: 12.197392, 05:40:28\n",
      "Step: 440, MSE: 0.060256, PSNR: 12.199988, 05:40:28\n",
      "Step: 441, MSE: 0.060332, PSNR: 12.194489, 05:40:28\n",
      "Step: 442, MSE: 0.060284, PSNR: 12.198002, 05:40:28\n",
      "Step: 443, MSE: 0.060300, PSNR: 12.196807, 05:40:28\n",
      "Step: 444, MSE: 0.060149, PSNR: 12.207694, 05:40:28\n",
      "Step: 445, MSE: 0.060308, PSNR: 12.196274, 05:40:28\n",
      "Step: 446, MSE: 0.060290, PSNR: 12.197560, 05:40:29\n",
      "Step: 447, MSE: 0.060270, PSNR: 12.199016, 05:40:29\n",
      "Step: 448, MSE: 0.060235, PSNR: 12.201538, 05:40:29\n",
      "Step: 449, MSE: 0.060272, PSNR: 12.198830, 05:40:29\n",
      "Step: 450, MSE: 0.060305, PSNR: 12.196438, 05:40:29\n",
      "Step: 451, MSE: 0.060256, PSNR: 12.199988, 05:40:29\n",
      "Step: 452, MSE: 0.060462, PSNR: 12.185165, 05:40:30\n",
      "Step: 453, MSE: 0.060223, PSNR: 12.202348, 05:40:30\n",
      "Step: 454, MSE: 0.060230, PSNR: 12.201904, 05:40:30\n",
      "Step: 455, MSE: 0.060290, PSNR: 12.197582, 05:40:30\n",
      "Step: 456, MSE: 0.060159, PSNR: 12.207008, 05:40:30\n",
      "Step: 457, MSE: 0.060242, PSNR: 12.201036, 05:40:30\n",
      "Step: 458, MSE: 0.060170, PSNR: 12.206188, 05:40:30\n",
      "Step: 459, MSE: 0.060528, PSNR: 12.180428, 05:40:31\n",
      "Step: 460, MSE: 0.060225, PSNR: 12.202229, 05:40:31\n",
      "Step: 461, MSE: 0.060095, PSNR: 12.211581, 05:40:31\n",
      "Step: 462, MSE: 0.060207, PSNR: 12.203526, 05:40:31\n",
      "Step: 463, MSE: 0.060382, PSNR: 12.190891, 05:40:31\n",
      "Step: 464, MSE: 0.060372, PSNR: 12.191654, 05:40:31\n",
      "Step: 465, MSE: 0.060479, PSNR: 12.183934, 05:40:32\n",
      "Step: 466, MSE: 0.060183, PSNR: 12.205253, 05:40:32\n",
      "Step: 467, MSE: 0.060132, PSNR: 12.208939, 05:40:32\n",
      "Step: 468, MSE: 0.060185, PSNR: 12.205105, 05:40:32\n",
      "Step: 469, MSE: 0.060271, PSNR: 12.198901, 05:40:32\n",
      "Step: 470, MSE: 0.060165, PSNR: 12.206573, 05:40:32\n",
      "Step: 471, MSE: 0.060106, PSNR: 12.210845, 05:40:32\n",
      "Step: 472, MSE: 0.060361, PSNR: 12.192431, 05:40:33\n",
      "Step: 473, MSE: 0.060153, PSNR: 12.207430, 05:40:33\n",
      "Step: 474, MSE: 0.060364, PSNR: 12.192187, 05:40:33\n",
      "Step: 475, MSE: 0.060300, PSNR: 12.196818, 05:40:33\n",
      "Step: 476, MSE: 0.060246, PSNR: 12.200745, 05:40:33\n",
      "Step: 477, MSE: 0.060309, PSNR: 12.196178, 05:40:33\n",
      "Step: 478, MSE: 0.060463, PSNR: 12.185104, 05:40:34\n",
      "Step: 479, MSE: 0.060384, PSNR: 12.190790, 05:40:34\n",
      "Step: 480, MSE: 0.060089, PSNR: 12.212036, 05:40:34\n",
      "Step: 481, MSE: 0.060181, PSNR: 12.205400, 05:40:34\n",
      "Step: 482, MSE: 0.060341, PSNR: 12.193878, 05:40:34\n",
      "Step: 483, MSE: 0.060278, PSNR: 12.198430, 05:40:34\n",
      "Step: 484, MSE: 0.060345, PSNR: 12.193609, 05:40:34\n",
      "Step: 485, MSE: 0.060246, PSNR: 12.200700, 05:40:35\n",
      "Step: 486, MSE: 0.060218, PSNR: 12.202707, 05:40:35\n",
      "Step: 487, MSE: 0.060338, PSNR: 12.194078, 05:40:35\n",
      "Step: 488, MSE: 0.060225, PSNR: 12.202229, 05:40:35\n",
      "Step: 489, MSE: 0.060113, PSNR: 12.210346, 05:40:35\n",
      "Step: 490, MSE: 0.060197, PSNR: 12.204280, 05:40:35\n",
      "Step: 491, MSE: 0.060218, PSNR: 12.202712, 05:40:35\n",
      "Step: 492, MSE: 0.060365, PSNR: 12.192156, 05:40:36\n",
      "Step: 493, MSE: 0.060351, PSNR: 12.193148, 05:40:36\n",
      "Step: 494, MSE: 0.060263, PSNR: 12.199501, 05:40:36\n",
      "Step: 495, MSE: 0.060215, PSNR: 12.202989, 05:40:36\n",
      "Step: 496, MSE: 0.060277, PSNR: 12.198500, 05:40:36\n",
      "Step: 497, MSE: 0.060210, PSNR: 12.203335, 05:40:36\n",
      "Step: 498, MSE: 0.060227, PSNR: 12.202076, 05:40:37\n",
      "Step: 499, MSE: 0.060273, PSNR: 12.198780, 05:40:37\n",
      "Step: 500, MSE: 0.060291, PSNR: 12.197451, 05:40:37\n",
      "Step: 501, MSE: 0.060467, PSNR: 12.184840, 05:40:37\n",
      "Step: 502, MSE: 0.060345, PSNR: 12.193608, 05:40:37\n",
      "Step: 503, MSE: 0.060396, PSNR: 12.189953, 05:40:37\n",
      "Step: 504, MSE: 0.060143, PSNR: 12.208165, 05:40:37\n",
      "Step: 505, MSE: 0.060317, PSNR: 12.195569, 05:40:38\n",
      "Step: 506, MSE: 0.060357, PSNR: 12.192707, 05:40:38\n",
      "Step: 507, MSE: 0.060072, PSNR: 12.213264, 05:40:38\n",
      "Step: 508, MSE: 0.060441, PSNR: 12.186713, 05:40:38\n",
      "Step: 509, MSE: 0.060144, PSNR: 12.208104, 05:40:38\n",
      "Step: 510, MSE: 0.060326, PSNR: 12.194971, 05:40:38\n",
      "Step: 511, MSE: 0.060366, PSNR: 12.192085, 05:40:38\n",
      "Step: 512, MSE: 0.060275, PSNR: 12.198609, 05:40:39\n",
      "Step: 513, MSE: 0.060173, PSNR: 12.205990, 05:40:39\n",
      "Step: 514, MSE: 0.060180, PSNR: 12.205446, 05:40:39\n",
      "Step: 515, MSE: 0.060273, PSNR: 12.198738, 05:40:39\n",
      "Step: 516, MSE: 0.060097, PSNR: 12.211495, 05:40:39\n",
      "Step: 517, MSE: 0.060306, PSNR: 12.196392, 05:40:39\n",
      "Step: 518, MSE: 0.060214, PSNR: 12.203033, 05:40:40\n",
      "Step: 519, MSE: 0.060366, PSNR: 12.192081, 05:40:40\n",
      "Step: 520, MSE: 0.060338, PSNR: 12.194094, 05:40:40\n",
      "Step: 521, MSE: 0.060323, PSNR: 12.195159, 05:40:40\n",
      "Step: 522, MSE: 0.060307, PSNR: 12.196340, 05:40:40\n",
      "Step: 523, MSE: 0.060244, PSNR: 12.200881, 05:40:40\n",
      "Step: 524, MSE: 0.060193, PSNR: 12.204519, 05:40:40\n",
      "Step: 525, MSE: 0.060190, PSNR: 12.204721, 05:40:41\n",
      "Step: 526, MSE: 0.060257, PSNR: 12.199953, 05:40:41\n",
      "Step: 527, MSE: 0.060369, PSNR: 12.191847, 05:40:41\n",
      "Step: 528, MSE: 0.060183, PSNR: 12.205296, 05:40:41\n",
      "Step: 529, MSE: 0.060484, PSNR: 12.183576, 05:40:41\n",
      "Step: 530, MSE: 0.060382, PSNR: 12.190932, 05:40:41\n",
      "Step: 531, MSE: 0.060273, PSNR: 12.198806, 05:40:42\n",
      "Step: 532, MSE: 0.060296, PSNR: 12.197084, 05:40:42\n",
      "Step: 533, MSE: 0.060316, PSNR: 12.195686, 05:40:42\n",
      "Step: 534, MSE: 0.060349, PSNR: 12.193322, 05:40:42\n",
      "Step: 535, MSE: 0.060182, PSNR: 12.205348, 05:40:42\n",
      "Step: 536, MSE: 0.060127, PSNR: 12.209316, 05:40:42\n",
      "Step: 537, MSE: 0.060353, PSNR: 12.193023, 05:40:42\n",
      "Step: 538, MSE: 0.060386, PSNR: 12.190610, 05:40:43\n",
      "Step: 539, MSE: 0.060127, PSNR: 12.209339, 05:40:43\n",
      "Step: 540, MSE: 0.060302, PSNR: 12.196655, 05:40:43\n",
      "Step: 541, MSE: 0.060195, PSNR: 12.204432, 05:40:43\n",
      "Step: 542, MSE: 0.060350, PSNR: 12.193213, 05:40:43\n",
      "Step: 543, MSE: 0.060221, PSNR: 12.202549, 05:40:43\n",
      "Step: 544, MSE: 0.060256, PSNR: 12.200030, 05:40:44\n",
      "Step: 545, MSE: 0.060243, PSNR: 12.200909, 05:40:44\n",
      "Step: 546, MSE: 0.060269, PSNR: 12.199089, 05:40:44\n",
      "Step: 547, MSE: 0.060314, PSNR: 12.195818, 05:40:44\n",
      "Step: 548, MSE: 0.060286, PSNR: 12.197820, 05:40:44\n",
      "Step: 549, MSE: 0.060210, PSNR: 12.203316, 05:40:44\n",
      "Step: 550, MSE: 0.060294, PSNR: 12.197277, 05:40:44\n",
      "Step: 551, MSE: 0.060121, PSNR: 12.209705, 05:40:44\n",
      "Step: 552, MSE: 0.060297, PSNR: 12.197019, 05:40:45\n",
      "Step: 553, MSE: 0.060218, PSNR: 12.202763, 05:40:45\n",
      "Step: 554, MSE: 0.060069, PSNR: 12.213488, 05:40:45\n",
      "Step: 555, MSE: 0.060243, PSNR: 12.200919, 05:40:45\n",
      "Step: 556, MSE: 0.060134, PSNR: 12.208818, 05:40:45\n",
      "Step: 557, MSE: 0.060234, PSNR: 12.201567, 05:40:45\n",
      "Step: 558, MSE: 0.060289, PSNR: 12.197585, 05:40:45\n",
      "Step: 559, MSE: 0.060170, PSNR: 12.206212, 05:40:46\n",
      "Step: 560, MSE: 0.060079, PSNR: 12.212762, 05:40:46\n",
      "Step: 561, MSE: 0.060265, PSNR: 12.199316, 05:40:46\n",
      "Step: 562, MSE: 0.060257, PSNR: 12.199936, 05:40:46\n",
      "Step: 563, MSE: 0.060296, PSNR: 12.197104, 05:40:46\n",
      "Step: 564, MSE: 0.060278, PSNR: 12.198396, 05:40:47\n",
      "Step: 565, MSE: 0.060179, PSNR: 12.205519, 05:40:47\n",
      "Step: 566, MSE: 0.060340, PSNR: 12.193953, 05:40:47\n",
      "Step: 567, MSE: 0.060169, PSNR: 12.206282, 05:40:47\n",
      "Step: 568, MSE: 0.060224, PSNR: 12.202300, 05:40:47\n",
      "Step: 569, MSE: 0.059916, PSNR: 12.224573, 05:40:47\n",
      "Step: 570, MSE: 0.060095, PSNR: 12.211610, 05:40:47\n",
      "Step: 571, MSE: 0.060294, PSNR: 12.197279, 05:40:48\n",
      "Step: 572, MSE: 0.060160, PSNR: 12.206956, 05:40:48\n",
      "Step: 573, MSE: 0.060136, PSNR: 12.208632, 05:40:48\n",
      "Step: 574, MSE: 0.060210, PSNR: 12.203350, 05:40:48\n",
      "Step: 575, MSE: 0.060234, PSNR: 12.201603, 05:40:48\n",
      "Step: 576, MSE: 0.060399, PSNR: 12.189712, 05:40:48\n",
      "Step: 577, MSE: 0.060348, PSNR: 12.193384, 05:40:48\n",
      "Step: 578, MSE: 0.060284, PSNR: 12.197948, 05:40:49\n",
      "Step: 579, MSE: 0.060280, PSNR: 12.198249, 05:40:49\n",
      "Step: 580, MSE: 0.060222, PSNR: 12.202444, 05:40:49\n",
      "Step: 581, MSE: 0.060283, PSNR: 12.198039, 05:40:49\n",
      "Step: 582, MSE: 0.060271, PSNR: 12.198933, 05:40:49\n",
      "Step: 583, MSE: 0.060230, PSNR: 12.201899, 05:40:49\n",
      "Step: 584, MSE: 0.060365, PSNR: 12.192163, 05:40:49\n",
      "Step: 585, MSE: 0.060363, PSNR: 12.192312, 05:40:50\n",
      "Step: 586, MSE: 0.060292, PSNR: 12.197390, 05:40:50\n",
      "Step: 587, MSE: 0.060341, PSNR: 12.193856, 05:40:50\n",
      "Step: 588, MSE: 0.060188, PSNR: 12.204916, 05:40:50\n",
      "Step: 589, MSE: 0.060232, PSNR: 12.201746, 05:40:50\n",
      "Step: 590, MSE: 0.060245, PSNR: 12.200796, 05:40:50\n",
      "Step: 591, MSE: 0.060349, PSNR: 12.193284, 05:40:50\n",
      "Step: 592, MSE: 0.060036, PSNR: 12.215852, 05:40:51\n",
      "Step: 593, MSE: 0.060241, PSNR: 12.201070, 05:40:51\n",
      "Step: 594, MSE: 0.060258, PSNR: 12.199818, 05:40:51\n",
      "Step: 595, MSE: 0.060420, PSNR: 12.188177, 05:40:51\n",
      "Step: 596, MSE: 0.060212, PSNR: 12.203148, 05:40:51\n",
      "Step: 597, MSE: 0.060333, PSNR: 12.194444, 05:40:51\n",
      "Step: 598, MSE: 0.060338, PSNR: 12.194080, 05:40:52\n",
      "Step: 599, MSE: 0.060228, PSNR: 12.201992, 05:40:52\n",
      "Step: 600, MSE: 0.060207, PSNR: 12.203503, 05:40:52\n",
      "Step: 601, MSE: 0.060148, PSNR: 12.207757, 05:40:52\n",
      "Step: 602, MSE: 0.060171, PSNR: 12.206108, 05:40:52\n",
      "Step: 603, MSE: 0.060257, PSNR: 12.199902, 05:40:52\n",
      "Step: 604, MSE: 0.060184, PSNR: 12.205208, 05:40:52\n",
      "Step: 605, MSE: 0.060334, PSNR: 12.194394, 05:40:53\n",
      "Step: 606, MSE: 0.060204, PSNR: 12.203764, 05:40:53\n",
      "Step: 607, MSE: 0.060217, PSNR: 12.202840, 05:40:53\n",
      "Step: 608, MSE: 0.060227, PSNR: 12.202106, 05:40:53\n",
      "Step: 609, MSE: 0.060332, PSNR: 12.194528, 05:40:53\n",
      "Step: 610, MSE: 0.060331, PSNR: 12.194588, 05:40:53\n",
      "Step: 611, MSE: 0.060243, PSNR: 12.200924, 05:40:53\n",
      "Step: 612, MSE: 0.060312, PSNR: 12.195986, 05:40:54\n",
      "Step: 613, MSE: 0.060241, PSNR: 12.201075, 05:40:54\n",
      "Step: 614, MSE: 0.060256, PSNR: 12.199991, 05:40:54\n",
      "Step: 615, MSE: 0.060373, PSNR: 12.191538, 05:40:54\n",
      "Step: 616, MSE: 0.060163, PSNR: 12.206717, 05:40:54\n",
      "Step: 617, MSE: 0.060281, PSNR: 12.198221, 05:40:54\n",
      "Step: 618, MSE: 0.060296, PSNR: 12.197143, 05:40:55\n",
      "Step: 619, MSE: 0.060138, PSNR: 12.208513, 05:40:55\n",
      "Step: 620, MSE: 0.060033, PSNR: 12.216101, 05:40:55\n",
      "Step: 621, MSE: 0.060331, PSNR: 12.194606, 05:40:55\n",
      "Step: 622, MSE: 0.060231, PSNR: 12.201799, 05:40:55\n",
      "Step: 623, MSE: 0.060484, PSNR: 12.183582, 05:40:55\n",
      "Step: 624, MSE: 0.060188, PSNR: 12.204875, 05:40:55\n",
      "Step: 625, MSE: 0.060259, PSNR: 12.199802, 05:40:56\n",
      "Step: 626, MSE: 0.060303, PSNR: 12.196592, 05:40:56\n",
      "Step: 627, MSE: 0.060461, PSNR: 12.185222, 05:40:56\n",
      "Step: 628, MSE: 0.060086, PSNR: 12.212272, 05:40:56\n",
      "Step: 629, MSE: 0.060349, PSNR: 12.193295, 05:40:56\n",
      "Step: 630, MSE: 0.060293, PSNR: 12.197305, 05:40:56\n",
      "Step: 631, MSE: 0.060369, PSNR: 12.191891, 05:40:57\n",
      "Step: 632, MSE: 0.060347, PSNR: 12.193459, 05:40:57\n",
      "Step: 633, MSE: 0.060389, PSNR: 12.190441, 05:40:57\n",
      "Step: 634, MSE: 0.060109, PSNR: 12.210617, 05:40:57\n",
      "Step: 635, MSE: 0.060067, PSNR: 12.213649, 05:40:57\n",
      "Step: 636, MSE: 0.060293, PSNR: 12.197367, 05:40:57\n",
      "Step: 637, MSE: 0.060085, PSNR: 12.212338, 05:40:57\n",
      "Step: 638, MSE: 0.060094, PSNR: 12.211719, 05:40:58\n",
      "Step: 639, MSE: 0.060184, PSNR: 12.205180, 05:40:58\n",
      "Step: 640, MSE: 0.060162, PSNR: 12.206773, 05:40:58\n",
      "Step: 641, MSE: 0.060476, PSNR: 12.184202, 05:40:58\n",
      "Step: 642, MSE: 0.060312, PSNR: 12.195945, 05:40:58\n",
      "Step: 643, MSE: 0.060177, PSNR: 12.205710, 05:40:58\n",
      "Step: 644, MSE: 0.060264, PSNR: 12.199439, 05:40:58\n",
      "Step: 645, MSE: 0.060356, PSNR: 12.192772, 05:40:59\n",
      "Step: 646, MSE: 0.060172, PSNR: 12.206048, 05:40:59\n",
      "Step: 647, MSE: 0.060174, PSNR: 12.205881, 05:40:59\n",
      "Step: 648, MSE: 0.060275, PSNR: 12.198663, 05:40:59\n",
      "Step: 649, MSE: 0.060195, PSNR: 12.204416, 05:40:59\n",
      "Step: 650, MSE: 0.060278, PSNR: 12.198428, 05:40:59\n",
      "Step: 651, MSE: 0.060366, PSNR: 12.192079, 05:40:59\n",
      "Step: 652, MSE: 0.060209, PSNR: 12.203394, 05:41:00\n",
      "Step: 653, MSE: 0.060347, PSNR: 12.193431, 05:41:00\n",
      "Step: 654, MSE: 0.060125, PSNR: 12.209480, 05:41:00\n",
      "Step: 655, MSE: 0.060302, PSNR: 12.196682, 05:41:00\n",
      "Step: 656, MSE: 0.060318, PSNR: 12.195557, 05:41:00\n",
      "Step: 657, MSE: 0.060289, PSNR: 12.197627, 05:41:00\n",
      "Step: 658, MSE: 0.060078, PSNR: 12.212837, 05:41:00\n",
      "Step: 659, MSE: 0.060232, PSNR: 12.201693, 05:41:01\n",
      "Step: 660, MSE: 0.060144, PSNR: 12.208080, 05:41:01\n",
      "Step: 661, MSE: 0.060140, PSNR: 12.208371, 05:41:01\n",
      "Step: 662, MSE: 0.060410, PSNR: 12.188920, 05:41:01\n",
      "Step: 663, MSE: 0.060086, PSNR: 12.212296, 05:41:01\n",
      "Step: 664, MSE: 0.060342, PSNR: 12.193798, 05:41:01\n",
      "Step: 665, MSE: 0.060268, PSNR: 12.199123, 05:41:01\n",
      "Step: 666, MSE: 0.060238, PSNR: 12.201317, 05:41:02\n",
      "Step: 667, MSE: 0.060179, PSNR: 12.205561, 05:41:02\n",
      "Step: 668, MSE: 0.060246, PSNR: 12.200751, 05:41:02\n",
      "Step: 669, MSE: 0.060335, PSNR: 12.194279, 05:41:02\n",
      "Step: 670, MSE: 0.060176, PSNR: 12.205780, 05:41:02\n",
      "Step: 671, MSE: 0.060474, PSNR: 12.184339, 05:41:03\n",
      "Step: 672, MSE: 0.060247, PSNR: 12.200681, 05:41:03\n",
      "Step: 673, MSE: 0.060320, PSNR: 12.195421, 05:41:03\n",
      "Step: 674, MSE: 0.060267, PSNR: 12.199169, 05:41:03\n",
      "Step: 675, MSE: 0.060279, PSNR: 12.198364, 05:41:03\n",
      "Step: 676, MSE: 0.060309, PSNR: 12.196159, 05:41:03\n",
      "Step: 677, MSE: 0.060214, PSNR: 12.203060, 05:41:03\n",
      "Step: 678, MSE: 0.060237, PSNR: 12.201357, 05:41:03\n",
      "Step: 679, MSE: 0.060387, PSNR: 12.190537, 05:41:04\n",
      "Step: 680, MSE: 0.060374, PSNR: 12.191535, 05:41:04\n",
      "Step: 681, MSE: 0.060182, PSNR: 12.205342, 05:41:04\n",
      "Step: 682, MSE: 0.060163, PSNR: 12.206712, 05:41:04\n",
      "Step: 683, MSE: 0.060229, PSNR: 12.201959, 05:41:04\n",
      "Step: 684, MSE: 0.060389, PSNR: 12.190419, 05:41:04\n",
      "Step: 685, MSE: 0.060188, PSNR: 12.204935, 05:41:05\n",
      "Step: 686, MSE: 0.060216, PSNR: 12.202847, 05:41:05\n",
      "Step: 687, MSE: 0.060332, PSNR: 12.194530, 05:41:05\n",
      "Step: 688, MSE: 0.060128, PSNR: 12.209267, 05:41:05\n",
      "Step: 689, MSE: 0.060390, PSNR: 12.190382, 05:41:05\n",
      "Step: 690, MSE: 0.060356, PSNR: 12.192763, 05:41:05\n",
      "Step: 691, MSE: 0.060402, PSNR: 12.189505, 05:41:06\n",
      "Step: 692, MSE: 0.060212, PSNR: 12.203182, 05:41:06\n",
      "Step: 693, MSE: 0.060265, PSNR: 12.199353, 05:41:06\n",
      "Step: 694, MSE: 0.060353, PSNR: 12.192985, 05:41:06\n",
      "Step: 695, MSE: 0.060229, PSNR: 12.201911, 05:41:06\n",
      "Step: 696, MSE: 0.060380, PSNR: 12.191077, 05:41:06\n",
      "Step: 697, MSE: 0.060245, PSNR: 12.200765, 05:41:07\n",
      "Step: 698, MSE: 0.060154, PSNR: 12.207341, 05:41:07\n",
      "Step: 699, MSE: 0.060198, PSNR: 12.204209, 05:41:07\n",
      "Step: 700, MSE: 0.060313, PSNR: 12.195913, 05:41:07\n",
      "Step: 701, MSE: 0.060328, PSNR: 12.194787, 05:41:07\n",
      "Step: 702, MSE: 0.060416, PSNR: 12.188457, 05:41:07\n",
      "Step: 703, MSE: 0.060258, PSNR: 12.199848, 05:41:07\n",
      "Step: 704, MSE: 0.060389, PSNR: 12.190398, 05:41:07\n",
      "Step: 705, MSE: 0.060177, PSNR: 12.205669, 05:41:08\n",
      "Step: 706, MSE: 0.060241, PSNR: 12.201059, 05:41:08\n",
      "Step: 707, MSE: 0.060334, PSNR: 12.194409, 05:41:08\n",
      "Step: 708, MSE: 0.060278, PSNR: 12.198412, 05:41:08\n",
      "Step: 709, MSE: 0.060244, PSNR: 12.200878, 05:41:08\n",
      "Step: 710, MSE: 0.060207, PSNR: 12.203535, 05:41:08\n",
      "Step: 711, MSE: 0.060322, PSNR: 12.195248, 05:41:09\n",
      "Step: 712, MSE: 0.060283, PSNR: 12.198030, 05:41:09\n",
      "Step: 713, MSE: 0.060273, PSNR: 12.198772, 05:41:09\n",
      "Step: 714, MSE: 0.060426, PSNR: 12.187757, 05:41:09\n",
      "Step: 715, MSE: 0.060341, PSNR: 12.193890, 05:41:09\n",
      "Step: 716, MSE: 0.060207, PSNR: 12.203521, 05:41:09\n",
      "Step: 717, MSE: 0.060164, PSNR: 12.206610, 05:41:09\n",
      "Step: 718, MSE: 0.060268, PSNR: 12.199108, 05:41:10\n",
      "Step: 719, MSE: 0.060172, PSNR: 12.206083, 05:41:10\n",
      "Step: 720, MSE: 0.060235, PSNR: 12.201496, 05:41:10\n",
      "Step: 721, MSE: 0.060279, PSNR: 12.198371, 05:41:10\n",
      "Step: 722, MSE: 0.060298, PSNR: 12.196942, 05:41:10\n",
      "Step: 723, MSE: 0.060214, PSNR: 12.203039, 05:41:10\n",
      "Step: 724, MSE: 0.060233, PSNR: 12.201635, 05:41:10\n",
      "Step: 725, MSE: 0.060170, PSNR: 12.206226, 05:41:11\n",
      "Step: 726, MSE: 0.060289, PSNR: 12.197643, 05:41:11\n",
      "Step: 727, MSE: 0.060088, PSNR: 12.212132, 05:41:11\n",
      "Step: 728, MSE: 0.060277, PSNR: 12.198485, 05:41:11\n",
      "Step: 729, MSE: 0.060392, PSNR: 12.190241, 05:41:11\n",
      "Step: 730, MSE: 0.060139, PSNR: 12.208427, 05:41:11\n",
      "Step: 731, MSE: 0.060246, PSNR: 12.200716, 05:41:12\n",
      "Step: 732, MSE: 0.060168, PSNR: 12.206359, 05:41:12\n",
      "Step: 733, MSE: 0.060219, PSNR: 12.202674, 05:41:12\n",
      "Step: 734, MSE: 0.060252, PSNR: 12.200319, 05:41:12\n",
      "Step: 735, MSE: 0.060325, PSNR: 12.195016, 05:41:12\n",
      "Step: 736, MSE: 0.060184, PSNR: 12.205179, 05:41:12\n",
      "Step: 737, MSE: 0.060156, PSNR: 12.207226, 05:41:13\n",
      "Step: 738, MSE: 0.060236, PSNR: 12.201453, 05:41:13\n",
      "Step: 739, MSE: 0.060389, PSNR: 12.190414, 05:41:13\n",
      "Step: 740, MSE: 0.060391, PSNR: 12.190289, 05:41:13\n",
      "Step: 741, MSE: 0.060427, PSNR: 12.187712, 05:41:13\n",
      "Step: 742, MSE: 0.060419, PSNR: 12.188293, 05:41:13\n",
      "Step: 743, MSE: 0.060250, PSNR: 12.200453, 05:41:14\n",
      "Step: 744, MSE: 0.060242, PSNR: 12.201004, 05:41:14\n",
      "Step: 745, MSE: 0.060369, PSNR: 12.191886, 05:41:14\n",
      "Step: 746, MSE: 0.060172, PSNR: 12.206029, 05:41:14\n",
      "Step: 747, MSE: 0.060333, PSNR: 12.194473, 05:41:14\n",
      "Step: 748, MSE: 0.060355, PSNR: 12.192884, 05:41:14\n",
      "Step: 749, MSE: 0.060335, PSNR: 12.194280, 05:41:15\n",
      "Step: 750, MSE: 0.060405, PSNR: 12.189237, 05:41:15\n",
      "Step: 751, MSE: 0.060252, PSNR: 12.200303, 05:41:15\n",
      "Step: 752, MSE: 0.060286, PSNR: 12.197843, 05:41:15\n",
      "Step: 753, MSE: 0.060386, PSNR: 12.190615, 05:41:15\n",
      "Step: 754, MSE: 0.060269, PSNR: 12.199041, 05:41:15\n",
      "Step: 755, MSE: 0.060249, PSNR: 12.200531, 05:41:16\n",
      "Step: 756, MSE: 0.060233, PSNR: 12.201643, 05:41:16\n",
      "Step: 757, MSE: 0.060133, PSNR: 12.208868, 05:41:16\n",
      "Step: 758, MSE: 0.060331, PSNR: 12.194584, 05:41:16\n",
      "Step: 759, MSE: 0.060356, PSNR: 12.192780, 05:41:16\n",
      "Step: 760, MSE: 0.060271, PSNR: 12.198896, 05:41:16\n",
      "Step: 761, MSE: 0.060260, PSNR: 12.199684, 05:41:16\n",
      "Step: 762, MSE: 0.060193, PSNR: 12.204529, 05:41:17\n",
      "Step: 763, MSE: 0.060359, PSNR: 12.192611, 05:41:17\n",
      "Step: 764, MSE: 0.060444, PSNR: 12.186497, 05:41:17\n",
      "Step: 765, MSE: 0.060157, PSNR: 12.207127, 05:41:17\n",
      "Step: 766, MSE: 0.060336, PSNR: 12.194243, 05:41:17\n",
      "Step: 767, MSE: 0.060320, PSNR: 12.195360, 05:41:17\n",
      "Step: 768, MSE: 0.060229, PSNR: 12.201979, 05:41:18\n",
      "Step: 769, MSE: 0.060211, PSNR: 12.203212, 05:41:18\n",
      "Step: 770, MSE: 0.060123, PSNR: 12.209559, 05:41:18\n",
      "Step: 771, MSE: 0.060498, PSNR: 12.182623, 05:41:18\n",
      "Step: 772, MSE: 0.060134, PSNR: 12.208775, 05:41:18\n",
      "Step: 773, MSE: 0.060306, PSNR: 12.196403, 05:41:18\n",
      "Step: 774, MSE: 0.060371, PSNR: 12.191715, 05:41:18\n",
      "Step: 775, MSE: 0.060390, PSNR: 12.190381, 05:41:19\n",
      "Step: 776, MSE: 0.060174, PSNR: 12.205882, 05:41:19\n",
      "Step: 777, MSE: 0.060338, PSNR: 12.194123, 05:41:19\n",
      "Step: 778, MSE: 0.060127, PSNR: 12.209340, 05:41:19\n",
      "Step: 779, MSE: 0.060237, PSNR: 12.201353, 05:41:19\n",
      "Step: 780, MSE: 0.060166, PSNR: 12.206521, 05:41:19\n",
      "Step: 781, MSE: 0.060439, PSNR: 12.186857, 05:41:20\n",
      "Step: 782, MSE: 0.060272, PSNR: 12.198831, 05:41:20\n",
      "Step: 783, MSE: 0.060335, PSNR: 12.194291, 05:41:20\n",
      "Step: 784, MSE: 0.060219, PSNR: 12.202688, 05:41:20\n",
      "Step: 785, MSE: 0.060432, PSNR: 12.187342, 05:41:20\n",
      "Step: 786, MSE: 0.060410, PSNR: 12.188882, 05:41:20\n",
      "Step: 787, MSE: 0.060234, PSNR: 12.201580, 05:41:21\n",
      "Step: 788, MSE: 0.060507, PSNR: 12.181963, 05:41:21\n",
      "Step: 789, MSE: 0.060119, PSNR: 12.209882, 05:41:21\n",
      "Step: 790, MSE: 0.060315, PSNR: 12.195721, 05:41:21\n",
      "Step: 791, MSE: 0.060300, PSNR: 12.196851, 05:41:21\n",
      "Step: 792, MSE: 0.060145, PSNR: 12.208021, 05:41:21\n",
      "Step: 793, MSE: 0.060274, PSNR: 12.198732, 05:41:21\n",
      "Step: 794, MSE: 0.060345, PSNR: 12.193575, 05:41:21\n",
      "Step: 795, MSE: 0.060384, PSNR: 12.190817, 05:41:22\n",
      "Step: 796, MSE: 0.060236, PSNR: 12.201415, 05:41:22\n",
      "Step: 797, MSE: 0.060413, PSNR: 12.188723, 05:41:22\n",
      "Step: 798, MSE: 0.060341, PSNR: 12.193908, 05:41:22\n",
      "Step: 799, MSE: 0.060226, PSNR: 12.202152, 05:41:22\n",
      "Step: 800, MSE: 0.060418, PSNR: 12.188325, 05:41:22\n",
      "Step: 801, MSE: 0.060149, PSNR: 12.207693, 05:41:22\n",
      "Step: 802, MSE: 0.060192, PSNR: 12.204597, 05:41:23\n",
      "Step: 803, MSE: 0.060093, PSNR: 12.211761, 05:41:23\n",
      "Step: 804, MSE: 0.060182, PSNR: 12.205318, 05:41:23\n",
      "Step: 805, MSE: 0.060281, PSNR: 12.198231, 05:41:23\n",
      "Step: 806, MSE: 0.060219, PSNR: 12.202673, 05:41:23\n",
      "Step: 807, MSE: 0.060358, PSNR: 12.192645, 05:41:23\n",
      "Step: 808, MSE: 0.060381, PSNR: 12.190971, 05:41:24\n",
      "Step: 809, MSE: 0.060277, PSNR: 12.198460, 05:41:24\n",
      "Step: 810, MSE: 0.060346, PSNR: 12.193521, 05:41:24\n",
      "Step: 811, MSE: 0.060245, PSNR: 12.200775, 05:41:24\n",
      "Step: 812, MSE: 0.060287, PSNR: 12.197735, 05:41:24\n",
      "Step: 813, MSE: 0.060238, PSNR: 12.201271, 05:41:24\n",
      "Step: 814, MSE: 0.060419, PSNR: 12.188287, 05:41:24\n",
      "Step: 815, MSE: 0.060357, PSNR: 12.192750, 05:41:25\n",
      "Step: 816, MSE: 0.060200, PSNR: 12.204014, 05:41:25\n",
      "Step: 817, MSE: 0.060305, PSNR: 12.196472, 05:41:25\n",
      "Step: 818, MSE: 0.060340, PSNR: 12.193914, 05:41:25\n",
      "Step: 819, MSE: 0.060185, PSNR: 12.205153, 05:41:25\n",
      "Step: 820, MSE: 0.060335, PSNR: 12.194271, 05:41:25\n",
      "Step: 821, MSE: 0.060389, PSNR: 12.190441, 05:41:26\n",
      "Step: 822, MSE: 0.060320, PSNR: 12.195376, 05:41:26\n",
      "Step: 823, MSE: 0.060294, PSNR: 12.197279, 05:41:26\n",
      "Step: 824, MSE: 0.060214, PSNR: 12.202993, 05:41:26\n",
      "Step: 825, MSE: 0.060182, PSNR: 12.205359, 05:41:26\n",
      "Step: 826, MSE: 0.060266, PSNR: 12.199244, 05:41:26\n",
      "Step: 827, MSE: 0.060374, PSNR: 12.191486, 05:41:26\n",
      "Step: 828, MSE: 0.060211, PSNR: 12.203230, 05:41:27\n",
      "Step: 829, MSE: 0.060136, PSNR: 12.208657, 05:41:27\n",
      "Step: 830, MSE: 0.060130, PSNR: 12.209076, 05:41:27\n",
      "Step: 831, MSE: 0.060231, PSNR: 12.201781, 05:41:27\n",
      "Step: 832, MSE: 0.060294, PSNR: 12.197238, 05:41:27\n",
      "Step: 833, MSE: 0.060166, PSNR: 12.206454, 05:41:27\n",
      "Step: 834, MSE: 0.060377, PSNR: 12.191256, 05:41:27\n",
      "Step: 835, MSE: 0.060226, PSNR: 12.202147, 05:41:28\n",
      "Step: 836, MSE: 0.060148, PSNR: 12.207777, 05:41:28\n",
      "Step: 837, MSE: 0.060162, PSNR: 12.206764, 05:41:28\n",
      "Step: 838, MSE: 0.060198, PSNR: 12.204185, 05:41:28\n",
      "Step: 839, MSE: 0.060259, PSNR: 12.199766, 05:41:28\n",
      "Step: 840, MSE: 0.060289, PSNR: 12.197595, 05:41:28\n",
      "Step: 841, MSE: 0.060178, PSNR: 12.205611, 05:41:29\n",
      "Step: 842, MSE: 0.060257, PSNR: 12.199953, 05:41:29\n",
      "Step: 843, MSE: 0.060155, PSNR: 12.207307, 05:41:29\n",
      "Step: 844, MSE: 0.060050, PSNR: 12.214888, 05:41:29\n",
      "Step: 845, MSE: 0.060137, PSNR: 12.208574, 05:41:29\n",
      "Step: 846, MSE: 0.060296, PSNR: 12.197083, 05:41:29\n",
      "Step: 847, MSE: 0.060102, PSNR: 12.211117, 05:41:30\n",
      "Step: 848, MSE: 0.060507, PSNR: 12.181913, 05:41:30\n",
      "Step: 849, MSE: 0.060129, PSNR: 12.209188, 05:41:30\n",
      "Step: 850, MSE: 0.060260, PSNR: 12.199682, 05:41:30\n",
      "Step: 851, MSE: 0.060319, PSNR: 12.195472, 05:41:30\n",
      "Step: 852, MSE: 0.060450, PSNR: 12.186033, 05:41:30\n",
      "Step: 853, MSE: 0.060202, PSNR: 12.203908, 05:41:30\n",
      "Step: 854, MSE: 0.060363, PSNR: 12.192325, 05:41:31\n",
      "Step: 855, MSE: 0.060327, PSNR: 12.194919, 05:41:31\n",
      "Step: 856, MSE: 0.060263, PSNR: 12.199474, 05:41:31\n",
      "Step: 857, MSE: 0.060314, PSNR: 12.195854, 05:41:31\n",
      "Step: 858, MSE: 0.060325, PSNR: 12.195001, 05:41:31\n",
      "Step: 859, MSE: 0.060348, PSNR: 12.193376, 05:41:31\n",
      "Step: 860, MSE: 0.060112, PSNR: 12.210406, 05:41:32\n",
      "Step: 861, MSE: 0.060233, PSNR: 12.201632, 05:41:32\n",
      "Step: 862, MSE: 0.060083, PSNR: 12.212477, 05:41:32\n",
      "Step: 863, MSE: 0.060178, PSNR: 12.205597, 05:41:32\n",
      "Step: 864, MSE: 0.060180, PSNR: 12.205444, 05:41:32\n",
      "Step: 865, MSE: 0.060371, PSNR: 12.191688, 05:41:32\n",
      "Step: 866, MSE: 0.060282, PSNR: 12.198115, 05:41:32\n",
      "Step: 867, MSE: 0.060266, PSNR: 12.199302, 05:41:33\n",
      "Step: 868, MSE: 0.060326, PSNR: 12.194969, 05:41:33\n",
      "Step: 869, MSE: 0.060001, PSNR: 12.218407, 05:41:33\n",
      "Step: 870, MSE: 0.060328, PSNR: 12.194816, 05:41:33\n",
      "Step: 871, MSE: 0.060206, PSNR: 12.203578, 05:41:33\n",
      "Step: 872, MSE: 0.060313, PSNR: 12.195860, 05:41:33\n",
      "Step: 873, MSE: 0.060266, PSNR: 12.199306, 05:41:33\n",
      "Step: 874, MSE: 0.060227, PSNR: 12.202058, 05:41:34\n",
      "Step: 875, MSE: 0.060244, PSNR: 12.200891, 05:41:34\n",
      "Step: 876, MSE: 0.060394, PSNR: 12.190033, 05:41:34\n",
      "Step: 877, MSE: 0.060270, PSNR: 12.198996, 05:41:34\n",
      "Step: 878, MSE: 0.060049, PSNR: 12.214928, 05:41:34\n",
      "Step: 879, MSE: 0.060323, PSNR: 12.195188, 05:41:34\n",
      "Step: 880, MSE: 0.060374, PSNR: 12.191486, 05:41:34\n",
      "Step: 881, MSE: 0.060254, PSNR: 12.200166, 05:41:35\n",
      "Step: 882, MSE: 0.060200, PSNR: 12.204029, 05:41:35\n",
      "Step: 883, MSE: 0.060124, PSNR: 12.209543, 05:41:35\n",
      "Step: 884, MSE: 0.060120, PSNR: 12.209793, 05:41:35\n",
      "Step: 885, MSE: 0.060251, PSNR: 12.200355, 05:41:35\n",
      "Step: 886, MSE: 0.060177, PSNR: 12.205701, 05:41:35\n",
      "Step: 887, MSE: 0.060176, PSNR: 12.205763, 05:41:35\n",
      "Step: 888, MSE: 0.060260, PSNR: 12.199693, 05:41:36\n",
      "Step: 889, MSE: 0.060300, PSNR: 12.196804, 05:41:36\n",
      "Step: 890, MSE: 0.060300, PSNR: 12.196855, 05:41:36\n",
      "Step: 891, MSE: 0.060432, PSNR: 12.187332, 05:41:36\n",
      "Step: 892, MSE: 0.060360, PSNR: 12.192515, 05:41:36\n",
      "Step: 893, MSE: 0.060246, PSNR: 12.200739, 05:41:36\n",
      "Step: 894, MSE: 0.059998, PSNR: 12.218614, 05:41:37\n",
      "Step: 895, MSE: 0.060044, PSNR: 12.215301, 05:41:37\n",
      "Step: 896, MSE: 0.060243, PSNR: 12.200909, 05:41:37\n",
      "Step: 897, MSE: 0.060106, PSNR: 12.210809, 05:41:37\n",
      "Step: 898, MSE: 0.060266, PSNR: 12.199276, 05:41:37\n",
      "Step: 899, MSE: 0.060381, PSNR: 12.191032, 05:41:37\n",
      "Step: 900, MSE: 0.060200, PSNR: 12.204000, 05:41:37\n",
      "Step: 901, MSE: 0.060372, PSNR: 12.191648, 05:41:38\n",
      "Step: 902, MSE: 0.060297, PSNR: 12.197045, 05:41:38\n",
      "Step: 903, MSE: 0.060087, PSNR: 12.212192, 05:41:38\n",
      "Step: 904, MSE: 0.060164, PSNR: 12.206629, 05:41:38\n",
      "Step: 905, MSE: 0.060315, PSNR: 12.195720, 05:41:38\n",
      "Step: 906, MSE: 0.060349, PSNR: 12.193299, 05:41:38\n",
      "Step: 907, MSE: 0.060285, PSNR: 12.197881, 05:41:39\n",
      "Step: 908, MSE: 0.060203, PSNR: 12.203788, 05:41:39\n",
      "Step: 909, MSE: 0.060296, PSNR: 12.197112, 05:41:39\n",
      "Step: 910, MSE: 0.060285, PSNR: 12.197904, 05:41:39\n",
      "Step: 911, MSE: 0.060308, PSNR: 12.196245, 05:41:39\n",
      "Step: 912, MSE: 0.060214, PSNR: 12.202991, 05:41:39\n",
      "Step: 913, MSE: 0.060158, PSNR: 12.207051, 05:41:40\n",
      "Step: 914, MSE: 0.060349, PSNR: 12.193290, 05:41:40\n",
      "Step: 915, MSE: 0.060326, PSNR: 12.194968, 05:41:40\n",
      "Step: 916, MSE: 0.060176, PSNR: 12.205738, 05:41:40\n",
      "Step: 917, MSE: 0.060458, PSNR: 12.185473, 05:41:40\n",
      "Step: 918, MSE: 0.060246, PSNR: 12.200717, 05:41:40\n",
      "Step: 919, MSE: 0.060220, PSNR: 12.202573, 05:41:40\n",
      "Step: 920, MSE: 0.060136, PSNR: 12.208632, 05:41:41\n",
      "Step: 921, MSE: 0.060191, PSNR: 12.204666, 05:41:41\n",
      "Step: 922, MSE: 0.060335, PSNR: 12.194339, 05:41:41\n",
      "Step: 923, MSE: 0.060350, PSNR: 12.193247, 05:41:41\n",
      "Step: 924, MSE: 0.060319, PSNR: 12.195438, 05:41:41\n",
      "Step: 925, MSE: 0.060260, PSNR: 12.199737, 05:41:42\n",
      "Step: 926, MSE: 0.060110, PSNR: 12.210531, 05:41:42\n",
      "Step: 927, MSE: 0.060482, PSNR: 12.183714, 05:41:42\n",
      "Step: 928, MSE: 0.060200, PSNR: 12.204028, 05:41:42\n",
      "Step: 929, MSE: 0.060326, PSNR: 12.194968, 05:41:42\n",
      "Step: 930, MSE: 0.060105, PSNR: 12.210917, 05:41:42\n",
      "Step: 931, MSE: 0.060374, PSNR: 12.191500, 05:41:43\n",
      "Step: 932, MSE: 0.060125, PSNR: 12.209419, 05:41:43\n",
      "Step: 933, MSE: 0.060301, PSNR: 12.196732, 05:41:43\n",
      "Step: 934, MSE: 0.060294, PSNR: 12.197285, 05:41:43\n",
      "Step: 935, MSE: 0.060311, PSNR: 12.196055, 05:41:43\n",
      "Step: 936, MSE: 0.060240, PSNR: 12.201167, 05:41:43\n",
      "Step: 937, MSE: 0.060190, PSNR: 12.204756, 05:41:44\n",
      "Step: 938, MSE: 0.060282, PSNR: 12.198106, 05:41:44\n",
      "Step: 939, MSE: 0.060289, PSNR: 12.197599, 05:41:44\n",
      "Step: 940, MSE: 0.060284, PSNR: 12.197952, 05:41:44\n",
      "Step: 941, MSE: 0.060250, PSNR: 12.200412, 05:41:44\n",
      "Step: 942, MSE: 0.060447, PSNR: 12.186285, 05:41:44\n",
      "Step: 943, MSE: 0.060353, PSNR: 12.193033, 05:41:44\n",
      "Step: 944, MSE: 0.060340, PSNR: 12.193933, 05:41:45\n",
      "Step: 945, MSE: 0.060379, PSNR: 12.191154, 05:41:45\n",
      "Step: 946, MSE: 0.060328, PSNR: 12.194815, 05:41:45\n",
      "Step: 947, MSE: 0.060218, PSNR: 12.202755, 05:41:45\n",
      "Step: 948, MSE: 0.060187, PSNR: 12.205001, 05:41:45\n",
      "Step: 949, MSE: 0.060247, PSNR: 12.200679, 05:41:46\n",
      "Step: 950, MSE: 0.060331, PSNR: 12.194574, 05:41:46\n",
      "Step: 951, MSE: 0.060303, PSNR: 12.196592, 05:41:46\n",
      "Step: 952, MSE: 0.060192, PSNR: 12.204645, 05:41:46\n",
      "Step: 953, MSE: 0.060125, PSNR: 12.209477, 05:41:46\n",
      "Step: 954, MSE: 0.060314, PSNR: 12.195850, 05:41:46\n",
      "Step: 955, MSE: 0.060270, PSNR: 12.198957, 05:41:46\n",
      "Step: 956, MSE: 0.060366, PSNR: 12.192102, 05:41:47\n",
      "Step: 957, MSE: 0.060416, PSNR: 12.188483, 05:41:47\n",
      "Step: 958, MSE: 0.060366, PSNR: 12.192050, 05:41:47\n",
      "Step: 959, MSE: 0.060244, PSNR: 12.200886, 05:41:47\n",
      "Step: 960, MSE: 0.060372, PSNR: 12.191650, 05:41:47\n",
      "Step: 961, MSE: 0.060313, PSNR: 12.195859, 05:41:47\n",
      "Step: 962, MSE: 0.060212, PSNR: 12.203189, 05:41:48\n",
      "Step: 963, MSE: 0.060324, PSNR: 12.195112, 05:41:48\n",
      "Step: 964, MSE: 0.060187, PSNR: 12.204947, 05:41:48\n",
      "Step: 965, MSE: 0.060265, PSNR: 12.199363, 05:41:48\n",
      "Step: 966, MSE: 0.060197, PSNR: 12.204283, 05:41:48\n",
      "Step: 967, MSE: 0.060138, PSNR: 12.208529, 05:41:48\n",
      "Step: 968, MSE: 0.060052, PSNR: 12.214708, 05:41:48\n",
      "Step: 969, MSE: 0.060139, PSNR: 12.208426, 05:41:49\n",
      "Step: 970, MSE: 0.060456, PSNR: 12.185593, 05:41:49\n",
      "Step: 971, MSE: 0.060190, PSNR: 12.204727, 05:41:49\n",
      "Step: 972, MSE: 0.060172, PSNR: 12.206049, 05:41:49\n",
      "Step: 973, MSE: 0.060338, PSNR: 12.194110, 05:41:49\n",
      "Step: 974, MSE: 0.060246, PSNR: 12.200708, 05:41:49\n",
      "Step: 975, MSE: 0.060144, PSNR: 12.208043, 05:41:49\n",
      "Step: 976, MSE: 0.060126, PSNR: 12.209379, 05:41:50\n",
      "Step: 977, MSE: 0.060324, PSNR: 12.195130, 05:41:50\n",
      "Step: 978, MSE: 0.060187, PSNR: 12.204988, 05:41:50\n",
      "Step: 979, MSE: 0.060232, PSNR: 12.201761, 05:41:50\n",
      "Step: 980, MSE: 0.060253, PSNR: 12.200209, 05:41:50\n",
      "Step: 981, MSE: 0.060178, PSNR: 12.205645, 05:41:50\n",
      "Step: 982, MSE: 0.060502, PSNR: 12.182321, 05:41:51\n",
      "Step: 983, MSE: 0.060282, PSNR: 12.198133, 05:41:51\n",
      "Step: 984, MSE: 0.060174, PSNR: 12.205877, 05:41:51\n",
      "Step: 985, MSE: 0.060222, PSNR: 12.202482, 05:41:51\n",
      "Step: 986, MSE: 0.060323, PSNR: 12.195144, 05:41:51\n",
      "Step: 987, MSE: 0.060294, PSNR: 12.197276, 05:41:51\n",
      "Step: 988, MSE: 0.060306, PSNR: 12.196415, 05:41:51\n",
      "Step: 989, MSE: 0.060335, PSNR: 12.194310, 05:41:52\n",
      "Step: 990, MSE: 0.060381, PSNR: 12.191029, 05:41:52\n",
      "Step: 991, MSE: 0.060157, PSNR: 12.207104, 05:41:52\n",
      "Step: 992, MSE: 0.060492, PSNR: 12.183013, 05:41:52\n",
      "Step: 993, MSE: 0.060148, PSNR: 12.207762, 05:41:52\n",
      "Step: 994, MSE: 0.060190, PSNR: 12.204758, 05:41:52\n",
      "Step: 995, MSE: 0.060298, PSNR: 12.196983, 05:41:52\n",
      "Step: 996, MSE: 0.060196, PSNR: 12.204306, 05:41:53\n",
      "Step: 997, MSE: 0.060200, PSNR: 12.204070, 05:41:53\n",
      "Step: 998, MSE: 0.060115, PSNR: 12.210157, 05:41:53\n",
      "Step: 999, MSE: 0.060264, PSNR: 12.199391, 05:41:53\n",
      "Step: 1000, MSE: 0.060258, PSNR: 12.199830, 05:41:53\n",
      "Step: 1001, MSE: 0.060207, PSNR: 12.203549, 05:41:53\n",
      "Step: 1002, MSE: 0.060331, PSNR: 12.194630, 05:41:53\n",
      "Step: 1003, MSE: 0.060199, PSNR: 12.204099, 05:41:53\n",
      "Step: 1004, MSE: 0.060100, PSNR: 12.211232, 05:41:54\n",
      "Step: 1005, MSE: 0.060277, PSNR: 12.198490, 05:41:54\n",
      "Step: 1006, MSE: 0.060229, PSNR: 12.201942, 05:41:54\n",
      "Step: 1007, MSE: 0.060227, PSNR: 12.202096, 05:41:54\n",
      "Step: 1008, MSE: 0.060306, PSNR: 12.196388, 05:41:54\n",
      "Step: 1009, MSE: 0.060212, PSNR: 12.203197, 05:41:54\n",
      "Step: 1010, MSE: 0.060192, PSNR: 12.204618, 05:41:55\n",
      "Step: 1011, MSE: 0.060246, PSNR: 12.200751, 05:41:55\n",
      "Step: 1012, MSE: 0.060271, PSNR: 12.198915, 05:41:55\n",
      "Step: 1013, MSE: 0.060176, PSNR: 12.205744, 05:41:55\n",
      "Step: 1014, MSE: 0.060224, PSNR: 12.202326, 05:41:55\n",
      "Step: 1015, MSE: 0.060326, PSNR: 12.194987, 05:41:56\n",
      "Step: 1016, MSE: 0.060213, PSNR: 12.203079, 05:41:56\n",
      "Step: 1017, MSE: 0.060505, PSNR: 12.182095, 05:41:56\n",
      "Step: 1018, MSE: 0.060063, PSNR: 12.213954, 05:41:56\n",
      "Step: 1019, MSE: 0.060380, PSNR: 12.191095, 05:41:56\n",
      "Step: 1020, MSE: 0.060300, PSNR: 12.196860, 05:41:56\n",
      "Step: 1021, MSE: 0.060222, PSNR: 12.202481, 05:41:56\n",
      "Step: 1022, MSE: 0.060064, PSNR: 12.213852, 05:41:57\n",
      "Step: 1023, MSE: 0.060353, PSNR: 12.193045, 05:41:57\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 6    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 155  |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.61 GiB is free. Process 1242132 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 217.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 461\u001b[0m\n\u001b[1;32m    447\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m RecurrentPPO(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpLstmPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# LSTM 정책 사용\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs\n\u001b[1;32m    457\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m    464\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:450\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[1;32m    443\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:414\u001b[0m, in \u001b[0;36mRecurrentPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 414\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    416\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.61 GiB is free. Process 1242132 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.66 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 217.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 512, 512).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(512)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((512, 512))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 512 or target.shape[-2] < 512:\n",
    "            target = torchvision.transforms.Resize(512)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "#BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=100000, T_PSNR=30, T_steps=1000):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 512, 512)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 512, 512), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.MultiBinary(1 * 8 * 512 * 512)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        # 학습 상태\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            self.target_image = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 첫 스텝에서 초기 상태와 동일한 행동 적용\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1  # 스텝 증가\n",
    "            # reset과 동일한 로직을 호출해 초기 상태 생성\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 행동(action)을 이진 상태(state)와 동일한 형식으로 변환\n",
    "        action = np.reshape(action, (1, 8, 512, 512)).astype(np.int8)\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, action).astype(np.int8)\n",
    "\n",
    "        # 이진화된 새로운 상태를 torch 텐서로 변환\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "        reward = -mse\n",
    "\n",
    "        # 출력 추가\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        # 상태 업데이트\n",
    "        self.state = new_state\n",
    "        self.observation = self.state  # 관찰값은 항상 상태와 동일\n",
    "\n",
    "        # 종료 조건\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 행동 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        관찰값이 0~0.2인 경우 -> 행동 0으로 고정.\n",
    "        관찰값이 0.8~1인 경우 -> 행동 1로 고정.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "        return mask\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=100000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=1000\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "#ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# 정책 네트워크 설정\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])],  # 네트워크 구조\n",
    "    lstm_hidden_size=256,                          # LSTM hidden state 크기\n",
    "    shared_lstm=False                              # 정책 및 가치 네트워크에서 별도 LSTM 사용\n",
    ")\n",
    "\n",
    "# RecurrentPPO 모델 생성\n",
    "ppo_model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",  # LSTM 정책 사용\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"./ppo_with_mask/\",\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# 학습\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
