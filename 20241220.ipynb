{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024, 1024])\n",
      "Using cuda device\n",
      "Initial MSE: 0.002263, Initial PSNR: 26.452951\n",
      "Logging to ./ppo_with_mask/PPO_37\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002263, Initial PSNR: 26.452951\n",
      "Step: 1, MSE: 0.062118, PSNR: 12.067814\n",
      "Step: 2, MSE: 0.062117, PSNR: 12.067862\n",
      "Step: 3, MSE: 0.062252, PSNR: 12.058498\n",
      "Step: 4, MSE: 0.062137, PSNR: 12.066463\n",
      "Step: 5, MSE: 0.062030, PSNR: 12.074009\n",
      "Step: 6, MSE: 0.062054, PSNR: 12.072287\n",
      "Step: 7, MSE: 0.062117, PSNR: 12.067915\n",
      "Step: 8, MSE: 0.062134, PSNR: 12.066677\n",
      "Step: 9, MSE: 0.062183, PSNR: 12.063289\n",
      "Step: 10, MSE: 0.062128, PSNR: 12.067102\n",
      "Step: 11, MSE: 0.062057, PSNR: 12.072113\n",
      "Step: 12, MSE: 0.062162, PSNR: 12.064744\n",
      "Step: 13, MSE: 0.062106, PSNR: 12.068654\n",
      "Step: 14, MSE: 0.062106, PSNR: 12.068634\n",
      "Step: 15, MSE: 0.062048, PSNR: 12.072734\n",
      "Step: 16, MSE: 0.062232, PSNR: 12.059877\n",
      "Step: 17, MSE: 0.062147, PSNR: 12.065817\n",
      "Step: 18, MSE: 0.062211, PSNR: 12.061359\n",
      "Step: 19, MSE: 0.062139, PSNR: 12.066350\n",
      "Step: 20, MSE: 0.062078, PSNR: 12.070600\n",
      "Step: 21, MSE: 0.062082, PSNR: 12.070374\n",
      "Step: 22, MSE: 0.062107, PSNR: 12.068604\n",
      "Step: 23, MSE: 0.062072, PSNR: 12.071039\n",
      "Step: 24, MSE: 0.062059, PSNR: 12.071964\n",
      "Step: 25, MSE: 0.062115, PSNR: 12.068048\n",
      "Step: 26, MSE: 0.062221, PSNR: 12.060641\n",
      "Step: 27, MSE: 0.062193, PSNR: 12.062568\n",
      "Step: 28, MSE: 0.062213, PSNR: 12.061200\n",
      "Step: 29, MSE: 0.062217, PSNR: 12.060932\n",
      "Step: 30, MSE: 0.062107, PSNR: 12.068615\n",
      "Step: 31, MSE: 0.062042, PSNR: 12.073114\n",
      "Step: 32, MSE: 0.062068, PSNR: 12.071290\n",
      "Step: 33, MSE: 0.062180, PSNR: 12.063495\n",
      "Step: 34, MSE: 0.062046, PSNR: 12.072839\n",
      "Step: 35, MSE: 0.062167, PSNR: 12.064428\n",
      "Step: 36, MSE: 0.062079, PSNR: 12.070576\n",
      "Step: 37, MSE: 0.062128, PSNR: 12.067127\n",
      "Step: 38, MSE: 0.062159, PSNR: 12.064974\n",
      "Step: 39, MSE: 0.062092, PSNR: 12.069622\n",
      "Step: 40, MSE: 0.062077, PSNR: 12.070713\n",
      "Step: 41, MSE: 0.062204, PSNR: 12.061845\n",
      "Step: 42, MSE: 0.061969, PSNR: 12.078252\n",
      "Step: 43, MSE: 0.062150, PSNR: 12.065578\n",
      "Step: 44, MSE: 0.062118, PSNR: 12.067835\n",
      "Step: 45, MSE: 0.062006, PSNR: 12.075647\n",
      "Step: 46, MSE: 0.062060, PSNR: 12.071853\n",
      "Step: 47, MSE: 0.062052, PSNR: 12.072456\n",
      "Step: 48, MSE: 0.062050, PSNR: 12.072593\n",
      "Step: 49, MSE: 0.062091, PSNR: 12.069703\n",
      "Step: 50, MSE: 0.062075, PSNR: 12.070805\n",
      "Step: 51, MSE: 0.062070, PSNR: 12.071151\n",
      "Step: 52, MSE: 0.062108, PSNR: 12.068520\n",
      "Step: 53, MSE: 0.062185, PSNR: 12.063178\n",
      "Step: 54, MSE: 0.062130, PSNR: 12.066988\n",
      "Step: 55, MSE: 0.062145, PSNR: 12.065905\n",
      "Step: 56, MSE: 0.062141, PSNR: 12.066244\n",
      "Step: 57, MSE: 0.062233, PSNR: 12.059761\n",
      "Step: 58, MSE: 0.062110, PSNR: 12.068377\n",
      "Step: 59, MSE: 0.062142, PSNR: 12.066173\n",
      "Step: 60, MSE: 0.062102, PSNR: 12.068969\n",
      "Step: 61, MSE: 0.062092, PSNR: 12.069632\n",
      "Step: 62, MSE: 0.062010, PSNR: 12.075396\n",
      "Step: 63, MSE: 0.062121, PSNR: 12.067637\n",
      "Step: 64, MSE: 0.062063, PSNR: 12.071704\n",
      "Step: 65, MSE: 0.062124, PSNR: 12.067403\n",
      "Step: 66, MSE: 0.062020, PSNR: 12.074652\n",
      "Step: 67, MSE: 0.062121, PSNR: 12.067618\n",
      "Step: 68, MSE: 0.062144, PSNR: 12.065987\n",
      "Step: 69, MSE: 0.062111, PSNR: 12.068346\n",
      "Step: 70, MSE: 0.062189, PSNR: 12.062862\n",
      "Step: 71, MSE: 0.062043, PSNR: 12.073099\n",
      "Step: 72, MSE: 0.062127, PSNR: 12.067213\n",
      "Step: 73, MSE: 0.062097, PSNR: 12.069305\n",
      "Step: 74, MSE: 0.062098, PSNR: 12.069201\n",
      "Step: 75, MSE: 0.062130, PSNR: 12.066961\n",
      "Step: 76, MSE: 0.062082, PSNR: 12.070368\n",
      "Step: 77, MSE: 0.062071, PSNR: 12.071106\n",
      "Step: 78, MSE: 0.062097, PSNR: 12.069271\n",
      "Step: 79, MSE: 0.061938, PSNR: 12.080414\n",
      "Step: 80, MSE: 0.062122, PSNR: 12.067518\n",
      "Step: 81, MSE: 0.062104, PSNR: 12.068824\n",
      "Step: 82, MSE: 0.062105, PSNR: 12.068735\n",
      "Step: 83, MSE: 0.062213, PSNR: 12.061169\n",
      "Step: 84, MSE: 0.062000, PSNR: 12.076107\n",
      "Step: 85, MSE: 0.062171, PSNR: 12.064140\n",
      "Step: 86, MSE: 0.062102, PSNR: 12.068939\n",
      "Step: 87, MSE: 0.062198, PSNR: 12.062259\n",
      "Step: 88, MSE: 0.062221, PSNR: 12.060628\n",
      "Step: 89, MSE: 0.062031, PSNR: 12.073921\n",
      "Step: 90, MSE: 0.062105, PSNR: 12.068769\n",
      "Step: 91, MSE: 0.062117, PSNR: 12.067875\n",
      "Step: 92, MSE: 0.062083, PSNR: 12.070286\n",
      "Step: 93, MSE: 0.062107, PSNR: 12.068591\n",
      "Step: 94, MSE: 0.062153, PSNR: 12.065412\n",
      "Step: 95, MSE: 0.062062, PSNR: 12.071735\n",
      "Step: 96, MSE: 0.062105, PSNR: 12.068739\n",
      "Step: 97, MSE: 0.062152, PSNR: 12.065434\n",
      "Step: 98, MSE: 0.062130, PSNR: 12.066971\n",
      "Step: 99, MSE: 0.062118, PSNR: 12.067858\n",
      "Step: 100, MSE: 0.062090, PSNR: 12.069780\n",
      "Step: 101, MSE: 0.062097, PSNR: 12.069296\n",
      "Step: 102, MSE: 0.062102, PSNR: 12.068960\n",
      "Step: 103, MSE: 0.062047, PSNR: 12.072768\n",
      "Step: 104, MSE: 0.062136, PSNR: 12.066551\n",
      "Step: 105, MSE: 0.062047, PSNR: 12.072760\n",
      "Step: 106, MSE: 0.062234, PSNR: 12.059712\n",
      "Step: 107, MSE: 0.062143, PSNR: 12.066045\n",
      "Step: 108, MSE: 0.062120, PSNR: 12.067669\n",
      "Step: 109, MSE: 0.062151, PSNR: 12.065502\n",
      "Step: 110, MSE: 0.062162, PSNR: 12.064751\n",
      "Step: 111, MSE: 0.062096, PSNR: 12.069373\n",
      "Step: 112, MSE: 0.062170, PSNR: 12.064192\n",
      "Step: 113, MSE: 0.062043, PSNR: 12.073084\n",
      "Step: 114, MSE: 0.062084, PSNR: 12.070215\n",
      "Step: 115, MSE: 0.062036, PSNR: 12.073542\n",
      "Step: 116, MSE: 0.062187, PSNR: 12.063003\n",
      "Step: 117, MSE: 0.062017, PSNR: 12.074879\n",
      "Step: 118, MSE: 0.062121, PSNR: 12.067636\n",
      "Step: 119, MSE: 0.062235, PSNR: 12.059678\n",
      "Step: 120, MSE: 0.062084, PSNR: 12.070230\n",
      "Step: 121, MSE: 0.062137, PSNR: 12.066479\n",
      "Step: 122, MSE: 0.062173, PSNR: 12.064013\n",
      "Step: 123, MSE: 0.062072, PSNR: 12.071051\n",
      "Step: 124, MSE: 0.062102, PSNR: 12.068916\n",
      "Step: 125, MSE: 0.062167, PSNR: 12.064400\n",
      "Step: 126, MSE: 0.062101, PSNR: 12.069001\n",
      "Step: 127, MSE: 0.062074, PSNR: 12.070894\n",
      "Step: 128, MSE: 0.062206, PSNR: 12.061699\n",
      "Step: 129, MSE: 0.062160, PSNR: 12.064919\n",
      "Step: 130, MSE: 0.062129, PSNR: 12.067052\n",
      "Step: 131, MSE: 0.062052, PSNR: 12.072431\n",
      "Step: 132, MSE: 0.062151, PSNR: 12.065485\n",
      "Step: 133, MSE: 0.062137, PSNR: 12.066521\n",
      "Step: 134, MSE: 0.062153, PSNR: 12.065382\n",
      "Step: 135, MSE: 0.062104, PSNR: 12.068810\n",
      "Step: 136, MSE: 0.062108, PSNR: 12.068554\n",
      "Step: 137, MSE: 0.061991, PSNR: 12.076726\n",
      "Step: 138, MSE: 0.062109, PSNR: 12.068472\n",
      "Step: 139, MSE: 0.062059, PSNR: 12.071964\n",
      "Step: 140, MSE: 0.062091, PSNR: 12.069715\n",
      "Step: 141, MSE: 0.062071, PSNR: 12.071095\n",
      "Step: 142, MSE: 0.062041, PSNR: 12.073226\n",
      "Step: 143, MSE: 0.062123, PSNR: 12.067488\n",
      "Step: 144, MSE: 0.062155, PSNR: 12.065266\n",
      "Step: 145, MSE: 0.062222, PSNR: 12.060594\n",
      "Step: 146, MSE: 0.062067, PSNR: 12.071398\n",
      "Step: 147, MSE: 0.062110, PSNR: 12.068385\n",
      "Step: 148, MSE: 0.062010, PSNR: 12.075373\n",
      "Step: 149, MSE: 0.062116, PSNR: 12.067956\n",
      "Step: 150, MSE: 0.062058, PSNR: 12.071989\n",
      "Step: 151, MSE: 0.062138, PSNR: 12.066401\n",
      "Step: 152, MSE: 0.062206, PSNR: 12.061670\n",
      "Step: 153, MSE: 0.062057, PSNR: 12.072121\n",
      "Step: 154, MSE: 0.062016, PSNR: 12.074958\n",
      "Step: 155, MSE: 0.062071, PSNR: 12.071104\n",
      "Step: 156, MSE: 0.062152, PSNR: 12.065478\n",
      "Step: 157, MSE: 0.062161, PSNR: 12.064821\n",
      "Step: 158, MSE: 0.062097, PSNR: 12.069274\n",
      "Step: 159, MSE: 0.062119, PSNR: 12.067780\n",
      "Step: 160, MSE: 0.062086, PSNR: 12.070038\n",
      "Step: 161, MSE: 0.062079, PSNR: 12.070572\n",
      "Step: 162, MSE: 0.062121, PSNR: 12.067650\n",
      "Step: 163, MSE: 0.062147, PSNR: 12.065811\n",
      "Step: 164, MSE: 0.062102, PSNR: 12.068922\n",
      "Step: 165, MSE: 0.062206, PSNR: 12.061657\n",
      "Step: 166, MSE: 0.062180, PSNR: 12.063461\n",
      "Step: 167, MSE: 0.062113, PSNR: 12.068207\n",
      "Step: 168, MSE: 0.062183, PSNR: 12.063312\n",
      "Step: 169, MSE: 0.062028, PSNR: 12.074089\n",
      "Step: 170, MSE: 0.062177, PSNR: 12.063731\n",
      "Step: 171, MSE: 0.062202, PSNR: 12.061969\n",
      "Step: 172, MSE: 0.062185, PSNR: 12.063117\n",
      "Step: 173, MSE: 0.062027, PSNR: 12.074170\n",
      "Step: 174, MSE: 0.062185, PSNR: 12.063147\n",
      "Step: 175, MSE: 0.062111, PSNR: 12.068327\n",
      "Step: 176, MSE: 0.062155, PSNR: 12.065273\n",
      "Step: 177, MSE: 0.062159, PSNR: 12.064993\n",
      "Step: 178, MSE: 0.062088, PSNR: 12.069907\n",
      "Step: 179, MSE: 0.062107, PSNR: 12.068629\n",
      "Step: 180, MSE: 0.061952, PSNR: 12.079464\n",
      "Step: 181, MSE: 0.062026, PSNR: 12.074278\n",
      "Step: 182, MSE: 0.062222, PSNR: 12.060530\n",
      "Step: 183, MSE: 0.062113, PSNR: 12.068161\n",
      "Step: 184, MSE: 0.062159, PSNR: 12.064935\n",
      "Step: 185, MSE: 0.062161, PSNR: 12.064790\n",
      "Step: 186, MSE: 0.062094, PSNR: 12.069484\n",
      "Step: 187, MSE: 0.062177, PSNR: 12.063667\n",
      "Step: 188, MSE: 0.062215, PSNR: 12.061070\n",
      "Step: 189, MSE: 0.062127, PSNR: 12.067217\n",
      "Step: 190, MSE: 0.062149, PSNR: 12.065647\n",
      "Step: 191, MSE: 0.062132, PSNR: 12.066850\n",
      "Step: 192, MSE: 0.062175, PSNR: 12.063808\n",
      "Step: 193, MSE: 0.062157, PSNR: 12.065120\n",
      "Step: 194, MSE: 0.062142, PSNR: 12.066137\n",
      "Step: 195, MSE: 0.062152, PSNR: 12.065424\n",
      "Step: 196, MSE: 0.062160, PSNR: 12.064917\n",
      "Step: 197, MSE: 0.062118, PSNR: 12.067822\n",
      "Step: 198, MSE: 0.062046, PSNR: 12.072829\n",
      "Step: 199, MSE: 0.062024, PSNR: 12.074406\n",
      "Step: 200, MSE: 0.062101, PSNR: 12.069016\n",
      "Step: 201, MSE: 0.062177, PSNR: 12.063670\n",
      "Step: 202, MSE: 0.062196, PSNR: 12.062344\n",
      "Step: 203, MSE: 0.062240, PSNR: 12.059301\n",
      "Step: 204, MSE: 0.062024, PSNR: 12.074389\n",
      "Step: 205, MSE: 0.062144, PSNR: 12.065994\n",
      "Step: 206, MSE: 0.062030, PSNR: 12.073993\n",
      "Step: 207, MSE: 0.062049, PSNR: 12.072643\n",
      "Step: 208, MSE: 0.062071, PSNR: 12.071132\n",
      "Step: 209, MSE: 0.061968, PSNR: 12.078304\n",
      "Step: 210, MSE: 0.062021, PSNR: 12.074584\n",
      "Step: 211, MSE: 0.062152, PSNR: 12.065482\n",
      "Step: 212, MSE: 0.062142, PSNR: 12.066145\n",
      "Step: 213, MSE: 0.062190, PSNR: 12.062782\n",
      "Step: 214, MSE: 0.062188, PSNR: 12.062910\n",
      "Step: 215, MSE: 0.062137, PSNR: 12.066519\n",
      "Step: 216, MSE: 0.062103, PSNR: 12.068848\n",
      "Step: 217, MSE: 0.062097, PSNR: 12.069284\n",
      "Step: 218, MSE: 0.062172, PSNR: 12.064058\n",
      "Step: 219, MSE: 0.062059, PSNR: 12.071939\n",
      "Step: 220, MSE: 0.062077, PSNR: 12.070714\n",
      "Step: 221, MSE: 0.062172, PSNR: 12.064066\n",
      "Step: 222, MSE: 0.062051, PSNR: 12.072546\n",
      "Step: 223, MSE: 0.062041, PSNR: 12.073179\n",
      "Step: 224, MSE: 0.062069, PSNR: 12.071257\n",
      "Step: 225, MSE: 0.062116, PSNR: 12.067970\n",
      "Step: 226, MSE: 0.061919, PSNR: 12.081733\n",
      "Step: 227, MSE: 0.062023, PSNR: 12.074463\n",
      "Step: 228, MSE: 0.062125, PSNR: 12.067362\n",
      "Step: 229, MSE: 0.062062, PSNR: 12.071748\n",
      "Step: 230, MSE: 0.062112, PSNR: 12.068253\n",
      "Step: 231, MSE: 0.062088, PSNR: 12.069937\n",
      "Step: 232, MSE: 0.062114, PSNR: 12.068132\n",
      "Step: 233, MSE: 0.062192, PSNR: 12.062650\n",
      "Step: 234, MSE: 0.062014, PSNR: 12.075118\n",
      "Step: 235, MSE: 0.062038, PSNR: 12.073442\n",
      "Step: 236, MSE: 0.062051, PSNR: 12.072539\n",
      "Step: 237, MSE: 0.062109, PSNR: 12.068442\n",
      "Step: 238, MSE: 0.062114, PSNR: 12.068121\n",
      "Step: 239, MSE: 0.062142, PSNR: 12.066174\n",
      "Step: 240, MSE: 0.062054, PSNR: 12.072286\n",
      "Step: 241, MSE: 0.062203, PSNR: 12.061871\n",
      "Step: 242, MSE: 0.062117, PSNR: 12.067910\n",
      "Step: 243, MSE: 0.062095, PSNR: 12.069434\n",
      "Step: 244, MSE: 0.062148, PSNR: 12.065695\n",
      "Step: 245, MSE: 0.062184, PSNR: 12.063180\n",
      "Step: 246, MSE: 0.062239, PSNR: 12.059340\n",
      "Step: 247, MSE: 0.062120, PSNR: 12.067713\n",
      "Step: 248, MSE: 0.062039, PSNR: 12.073327\n",
      "Step: 249, MSE: 0.062152, PSNR: 12.065429\n",
      "Step: 250, MSE: 0.061975, PSNR: 12.077841\n",
      "Step: 251, MSE: 0.062122, PSNR: 12.067533\n",
      "Step: 252, MSE: 0.062181, PSNR: 12.063452\n",
      "Step: 253, MSE: 0.062071, PSNR: 12.071079\n",
      "Step: 254, MSE: 0.062138, PSNR: 12.066442\n",
      "Step: 255, MSE: 0.062123, PSNR: 12.067484\n",
      "Step: 256, MSE: 0.062083, PSNR: 12.070276\n",
      "Step: 257, MSE: 0.062164, PSNR: 12.064609\n",
      "Step: 258, MSE: 0.062153, PSNR: 12.065382\n",
      "Step: 259, MSE: 0.062096, PSNR: 12.069372\n",
      "Step: 260, MSE: 0.062143, PSNR: 12.066074\n",
      "Step: 261, MSE: 0.062052, PSNR: 12.072419\n",
      "Step: 262, MSE: 0.062156, PSNR: 12.065205\n",
      "Step: 263, MSE: 0.062258, PSNR: 12.058072\n",
      "Step: 264, MSE: 0.062038, PSNR: 12.073393\n",
      "Step: 265, MSE: 0.062181, PSNR: 12.063393\n",
      "Step: 266, MSE: 0.062206, PSNR: 12.061677\n",
      "Step: 267, MSE: 0.062178, PSNR: 12.063608\n",
      "Step: 268, MSE: 0.062008, PSNR: 12.075535\n",
      "Step: 269, MSE: 0.062069, PSNR: 12.071248\n",
      "Step: 270, MSE: 0.062145, PSNR: 12.065968\n",
      "Step: 271, MSE: 0.062010, PSNR: 12.075387\n",
      "Step: 272, MSE: 0.062050, PSNR: 12.072607\n",
      "Step: 273, MSE: 0.062127, PSNR: 12.067224\n",
      "Step: 274, MSE: 0.062105, PSNR: 12.068767\n",
      "Step: 275, MSE: 0.061979, PSNR: 12.077585\n",
      "Step: 276, MSE: 0.062123, PSNR: 12.067472\n",
      "Step: 277, MSE: 0.062126, PSNR: 12.067241\n",
      "Step: 278, MSE: 0.062033, PSNR: 12.073799\n",
      "Step: 279, MSE: 0.062127, PSNR: 12.067165\n",
      "Step: 280, MSE: 0.062223, PSNR: 12.060472\n",
      "Step: 281, MSE: 0.062037, PSNR: 12.073512\n",
      "Step: 282, MSE: 0.062161, PSNR: 12.064800\n",
      "Step: 283, MSE: 0.062090, PSNR: 12.069792\n",
      "Step: 284, MSE: 0.062097, PSNR: 12.069321\n",
      "Step: 285, MSE: 0.062079, PSNR: 12.070543\n",
      "Step: 286, MSE: 0.062083, PSNR: 12.070303\n",
      "Step: 287, MSE: 0.062245, PSNR: 12.058978\n",
      "Step: 288, MSE: 0.061953, PSNR: 12.079399\n",
      "Step: 289, MSE: 0.062026, PSNR: 12.074241\n",
      "Step: 290, MSE: 0.062039, PSNR: 12.073346\n",
      "Step: 291, MSE: 0.062200, PSNR: 12.062130\n",
      "Step: 292, MSE: 0.062114, PSNR: 12.068081\n",
      "Step: 293, MSE: 0.062133, PSNR: 12.066794\n",
      "Step: 294, MSE: 0.062061, PSNR: 12.071835\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 1024, 1024).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(1024)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((1024, 1024))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 1024 or target.shape[-2] < 1024:\n",
    "            target = torchvision.transforms.Resize(1024)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "#BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=100000, T_PSNR=30, T_steps=1000):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 1024, 1024)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.MultiBinary(1 * 8 * 1024 * 1024)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        # 학습 상태\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            self.target_image = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image = next(self.data_iter)\n",
    "    \n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}\")\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}\")\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 첫 스텝에서 초기 상태와 동일한 행동 적용\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1  # 스텝 증가\n",
    "            # reset과 동일한 로직을 호출해 초기 상태 생성\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        # 행동(action)을 이진 상태(state)와 동일한 형식으로 변환\n",
    "        action = np.reshape(action, (1, 8, 1024, 1024)).astype(np.int8)\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, action).astype(np.float32)\n",
    "\n",
    "        # 이진화된 새로운 상태를 torch 텐서로 변환\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "        reward = -mse\n",
    "\n",
    "        # 출력 추가\n",
    "        print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}\")\n",
    "\n",
    "        # 상태 업데이트\n",
    "        self.state = new_state\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "\n",
    "        # 종료 조건\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 행동 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        관찰값이 0~0.2인 경우 -> 행동 0으로 고정.\n",
    "        관찰값이 0.8~1인 경우 -> 행동 1로 고정.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "        return mask\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=100000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=1000\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
