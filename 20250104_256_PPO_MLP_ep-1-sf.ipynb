{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edb12d4-2114-40ca-bb64-b1ffebe207af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 메시지는 콘솔과 파일에 동시에 기록됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 256, 256])\n",
      "Warning: PPO model not found at ./ppo_MlpPolicy_models/0_ppo_MlpPolicy_latest.zip. Starting training from scratch.\n",
      "Starting training from scratch.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 1\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:35:13\u001b[0m\n",
      "Logging to ./ppo_MlpPolicy/PPO_41\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.003557, PSNR Change: -0.000982, PSNR Diff: -0.000982 (New Max), Reward: -0.79, 22:35:13 Pre-flip Model Output=0.106766, New State Value=0, Flip Count=0, Flip Pixcel=5 * 35 * 59\n",
      "Step: 100, PSNR Before: 25.008846, PSNR After: 25.008764, PSNR Change: -0.000082, PSNR Diff: 0.004225 (New Max), Reward: -0.07, 22:35:15 Pre-flip Model Output=0.056323, New State Value=0, Flip Count=36, Flip Pixcel=3 * 52 * 59\n",
      "Step: 200, PSNR Before: 25.011770, PSNR After: 25.011703, PSNR Change: -0.000067, PSNR Diff: 0.007164 (New Max), Reward: -0.05, 22:35:16 Pre-flip Model Output=0.217836, New State Value=0, Flip Count=59, Flip Pixcel=2 * 126 * 232\n",
      "Step: 300, PSNR Before: 25.015854, PSNR After: 25.015568, PSNR Change: -0.000286, PSNR Diff: 0.011028 (New Max), Reward: -0.23, 22:35:17 Pre-flip Model Output=0.569946, New State Value=1, Flip Count=88, Flip Pixcel=3 * 223 * 35\n",
      "Step: 400, PSNR Before: 25.021017, PSNR After: 25.021029, PSNR Change: 0.000011, PSNR Diff: 0.016489 (New Max), Reward: 0.01, 22:35:18 Pre-flip Model Output=0.786580, New State Value=0, Flip Count=127, Flip Pixcel=7 * 78 * 63\n",
      "Step: 500, PSNR Before: 25.025352, PSNR After: 25.025303, PSNR Change: -0.000050, PSNR Diff: 0.020763 (New Max), Reward: -0.04, 22:35:20 Pre-flip Model Output=0.193924, New State Value=0, Flip Count=154, Flip Pixcel=2 * 152 * 169\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 71  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 7   |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "Step: 600, PSNR Before: 25.029083, PSNR After: 25.029165, PSNR Change: 0.000082, PSNR Diff: 0.024626 (New Max), Reward: 0.07, 22:35:33 Pre-flip Model Output=0.020198, New State Value=1, Flip Count=178, Flip Pixcel=6 * 156 * 126\n",
      "Step: 700, PSNR Before: 25.032028, PSNR After: 25.032051, PSNR Change: 0.000023, PSNR Diff: 0.027512 (New Max), Reward: 0.02, 22:35:35 Pre-flip Model Output=0.198027, New State Value=1, Flip Count=208, Flip Pixcel=4 * 231 * 30\n",
      "Step: 800, PSNR Before: 25.036800, PSNR After: 25.036428, PSNR Change: -0.000372, PSNR Diff: 0.031889 (New Max), Reward: -0.30, 22:35:36 Pre-flip Model Output=0.346659, New State Value=0, Flip Count=238, Flip Pixcel=5 * 219 * 157\n",
      "Step: 900, PSNR Before: 25.039440, PSNR After: 25.039215, PSNR Change: -0.000225, PSNR Diff: 0.034676 (New Max), Reward: -0.18, 22:35:37 Pre-flip Model Output=0.023584, New State Value=0, Flip Count=265, Flip Pixcel=3 * 212 * 11\n",
      "Step: 1000, PSNR Before: 25.044649, PSNR After: 25.043924, PSNR Change: -0.000725, PSNR Diff: 0.039385 (New Max), Reward: -0.58, 22:35:38 Pre-flip Model Output=0.300343, New State Value=0, Flip Count=298, Flip Pixcel=5 * 207 * 31\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 1024         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022989279 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -13.2        |\n",
      "|    explained_variance   | 0.00063      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    value_loss           | 7.57         |\n",
      "------------------------------------------\n",
      "Step: 1100, PSNR Before: 25.050394, PSNR After: 25.049994, PSNR Change: -0.000401, PSNR Diff: 0.045454 (New Max), Reward: -0.32, 22:35:52 Pre-flip Model Output=0.235686, New State Value=0, Flip Count=335, Flip Pixcel=5 * 64 * 148\n",
      "Step: 1200, PSNR Before: 25.055031, PSNR After: 25.054895, PSNR Change: -0.000135, PSNR Diff: 0.050356 (New Max), Reward: -0.11, 22:35:53 Pre-flip Model Output=0.343131, New State Value=0, Flip Count=365, Flip Pixcel=4 * 114 * 231\n",
      "Step: 1300, PSNR Before: 25.059233, PSNR After: 25.059631, PSNR Change: 0.000399, PSNR Diff: 0.055092 (New Max), Reward: 0.32, 22:35:55 Pre-flip Model Output=0.975298, New State Value=0, Flip Count=397, Flip Pixcel=4 * 81 * 196\n",
      "Step: 1400, PSNR Before: 25.064129, PSNR After: 25.063875, PSNR Change: -0.000254, PSNR Diff: 0.059336 (New Max), Reward: -0.20, 22:35:56 Pre-flip Model Output=0.787802, New State Value=1, Flip Count=435, Flip Pixcel=6 * 139 * 125\n",
      "Step: 1500, PSNR Before: 25.068939, PSNR After: 25.068741, PSNR Change: -0.000198, PSNR Diff: 0.064201 (New Max), Reward: -0.16, 22:35:57 Pre-flip Model Output=0.864531, New State Value=1, Flip Count=467, Flip Pixcel=0 * 38 * 248\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008649718 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.139      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.487       |\n",
      "-----------------------------------------\n",
      "Step: 1600, PSNR Before: 25.074020, PSNR After: 25.073822, PSNR Change: -0.000198, PSNR Diff: 0.069283 (New Max), Reward: -0.16, 22:36:11 Pre-flip Model Output=0.427607, New State Value=0, Flip Count=502, Flip Pixcel=7 * 175 * 222\n",
      "Step: 1700, PSNR Before: 25.077908, PSNR After: 25.077847, PSNR Change: -0.000061, PSNR Diff: 0.073307 (New Max), Reward: -0.05, 22:36:12 Pre-flip Model Output=0.483403, New State Value=0, Flip Count=533, Flip Pixcel=0 * 201 * 90\n",
      "Step: 1800, PSNR Before: 25.082005, PSNR After: 25.081688, PSNR Change: -0.000317, PSNR Diff: 0.077148 (New Max), Reward: -0.25, 22:36:13 Pre-flip Model Output=0.199851, New State Value=0, Flip Count=562, Flip Pixcel=7 * 93 * 194\n",
      "Step: 1900, PSNR Before: 25.086382, PSNR After: 25.087021, PSNR Change: 0.000639, PSNR Diff: 0.082481 (New Max), Reward: 0.51, 22:36:15 Pre-flip Model Output=0.566189, New State Value=0, Flip Count=593, Flip Pixcel=7 * 205 * 26\n",
      "Step: 2000, PSNR Before: 25.091785, PSNR After: 25.091551, PSNR Change: -0.000235, PSNR Diff: 0.087011 (New Max), Reward: -0.19, 22:36:16 Pre-flip Model Output=0.842790, New State Value=1, Flip Count=622, Flip Pixcel=7 * 215 * 83\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018959202 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.192      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 0.289       |\n",
      "-----------------------------------------\n",
      "Step: 2100, PSNR Before: 25.094109, PSNR After: 25.093945, PSNR Change: -0.000164, PSNR Diff: 0.089405 (New Max), Reward: -0.13, 22:36:29 Pre-flip Model Output=0.118160, New State Value=0, Flip Count=648, Flip Pixcel=1 * 228 * 197\n",
      "Step: 2200, PSNR Before: 25.098488, PSNR After: 25.098122, PSNR Change: -0.000366, PSNR Diff: 0.093582 (New Max), Reward: -0.29, 22:36:31 Pre-flip Model Output=0.983259, New State Value=1, Flip Count=679, Flip Pixcel=1 * 18 * 24\n",
      "Step: 2300, PSNR Before: 25.101393, PSNR After: 25.101309, PSNR Change: -0.000084, PSNR Diff: 0.096769 (New Max), Reward: -0.07, 22:36:32 Pre-flip Model Output=0.444349, New State Value=0, Flip Count=710, Flip Pixcel=6 * 16 * 131\n",
      "\u001b[94mStep: 2381, PSNR Before: 25.104473, PSNR After: 25.104717, PSNR Change: 0.000244, PSNR Diff: 0.100178 (New Max), Reward: 0.20, 22:36:33 Pre-flip Model Output=0.351310, New State Value=1, Flip Count=733, Flip Pixcel=4 * 156 * 142\u001b[0m\n",
      "\u001b[94mStep: 2383, PSNR Before: 25.104717, PSNR After: 25.104900, PSNR Change: 0.000183, PSNR Diff: 0.100361 (New Max), Reward: 0.15, 22:36:33 Pre-flip Model Output=0.506617, New State Value=0, Flip Count=734, Flip Pixcel=2 * 61 * 181\u001b[0m\n",
      "\u001b[94mStep: 2385, PSNR Before: 25.104900, PSNR After: 25.104906, PSNR Change: 0.000006, PSNR Diff: 0.100367 (New Max), Reward: 0.00, 22:36:33 Pre-flip Model Output=0.260054, New State Value=1, Flip Count=735, Flip Pixcel=6 * 60 * 104\u001b[0m\n",
      "\u001b[94mStep: 2388, PSNR Before: 25.104906, PSNR After: 25.105162, PSNR Change: 0.000256, PSNR Diff: 0.100622 (New Max), Reward: 0.20, 22:36:33 Pre-flip Model Output=0.386976, New State Value=1, Flip Count=736, Flip Pixcel=5 * 131 * 45\u001b[0m\n",
      "\u001b[94mStep: 2393, PSNR Before: 25.105162, PSNR After: 25.105322, PSNR Change: 0.000160, PSNR Diff: 0.100782 (New Max), Reward: 0.13, 22:36:33 Pre-flip Model Output=0.425655, New State Value=1, Flip Count=737, Flip Pixcel=6 * 18 * 163\u001b[0m\n",
      "\u001b[94mStep: 2396, PSNR Before: 25.105322, PSNR After: 25.105560, PSNR Change: 0.000238, PSNR Diff: 0.101021 (New Max), Reward: 0.19, 22:36:33 Pre-flip Model Output=0.538283, New State Value=0, Flip Count=738, Flip Pixcel=7 * 217 * 148\u001b[0m\n",
      "Step: 2400, PSNR Before: 25.105560, PSNR After: 25.105553, PSNR Change: -0.000008, PSNR Diff: 0.101013 (New Max), Reward: -0.01, 22:36:33 Pre-flip Model Output=0.809753, New State Value=1, Flip Count=738, Flip Pixcel=2 * 61 * 118\n",
      "\u001b[94mStep: 2405, PSNR Before: 25.105560, PSNR After: 25.105600, PSNR Change: 0.000040, PSNR Diff: 0.101061 (New Max), Reward: 0.03, 22:36:33 Pre-flip Model Output=0.543092, New State Value=0, Flip Count=739, Flip Pixcel=0 * 216 * 197\u001b[0m\n",
      "\u001b[94mStep: 2409, PSNR Before: 25.105600, PSNR After: 25.105608, PSNR Change: 0.000008, PSNR Diff: 0.101068 (New Max), Reward: 0.01, 22:36:33 Pre-flip Model Output=0.200881, New State Value=1, Flip Count=740, Flip Pixcel=1 * 86 * 4\u001b[0m\n",
      "\u001b[94mStep: 2410, PSNR Before: 25.105608, PSNR After: 25.105671, PSNR Change: 0.000063, PSNR Diff: 0.101131 (New Max), Reward: 0.05, 22:36:33 Pre-flip Model Output=0.279788, New State Value=1, Flip Count=741, Flip Pixcel=5 * 118 * 199\u001b[0m\n",
      "\u001b[94mStep: 2417, PSNR Before: 25.105671, PSNR After: 25.105709, PSNR Change: 0.000038, PSNR Diff: 0.101170 (New Max), Reward: 0.03, 22:36:33 Pre-flip Model Output=0.003814, New State Value=1, Flip Count=742, Flip Pixcel=6 * 29 * 80\u001b[0m\n",
      "\u001b[94mStep: 2425, PSNR Before: 25.105709, PSNR After: 25.105774, PSNR Change: 0.000065, PSNR Diff: 0.101234 (New Max), Reward: 0.05, 22:36:34 Pre-flip Model Output=0.143982, New State Value=1, Flip Count=743, Flip Pixcel=6 * 68 * 131\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 2\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:36:34\u001b[0m\n",
      "\u001b[41mEpisode 1: Total Reward: -350.30\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004486, PSNR Change: -0.000053, PSNR Diff: -0.000053 (New Max), Reward: -0.04, 22:36:34 Pre-flip Model Output=0.404209, New State Value=0, Flip Count=0, Flip Pixcel=1 * 129 * 90\n",
      "Step: 100, PSNR Before: 25.009846, PSNR After: 25.009838, PSNR Change: -0.000008, PSNR Diff: 0.005299 (New Max), Reward: -0.01, 22:36:35 Pre-flip Model Output=0.006557, New State Value=0, Flip Count=35, Flip Pixcel=6 * 109 * 98\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.43e+03   |\n",
      "|    ep_rew_mean          | -350       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 30         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02416785 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.199     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0837    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "Step: 200, PSNR Before: 25.013725, PSNR After: 25.013393, PSNR Change: -0.000332, PSNR Diff: 0.008854 (New Max), Reward: -0.27, 22:36:49 Pre-flip Model Output=0.919453, New State Value=1, Flip Count=66, Flip Pixcel=2 * 46 * 253\n",
      "Step: 300, PSNR Before: 25.020969, PSNR After: 25.020864, PSNR Change: -0.000105, PSNR Diff: 0.016325 (New Max), Reward: -0.08, 22:36:50 Pre-flip Model Output=0.507472, New State Value=1, Flip Count=108, Flip Pixcel=1 * 209 * 113\n",
      "Step: 400, PSNR Before: 25.025162, PSNR After: 25.025206, PSNR Change: 0.000044, PSNR Diff: 0.020666 (New Max), Reward: 0.04, 22:36:51 Pre-flip Model Output=0.061130, New State Value=1, Flip Count=139, Flip Pixcel=5 * 205 * 219\n",
      "Step: 500, PSNR Before: 25.031841, PSNR After: 25.031384, PSNR Change: -0.000458, PSNR Diff: 0.026844 (New Max), Reward: -0.37, 22:36:52 Pre-flip Model Output=0.047608, New State Value=0, Flip Count=180, Flip Pixcel=5 * 105 * 51\n",
      "Step: 600, PSNR Before: 25.035110, PSNR After: 25.035379, PSNR Change: 0.000269, PSNR Diff: 0.030840 (New Max), Reward: 0.22, 22:36:53 Pre-flip Model Output=0.413205, New State Value=1, Flip Count=211, Flip Pixcel=6 * 125 * 6\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.43e+03    |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011719669 |\n",
      "|    clip_fraction        | 0.00957     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 43.2        |\n",
      "-----------------------------------------\n",
      "Step: 700, PSNR Before: 25.041683, PSNR After: 25.041006, PSNR Change: -0.000677, PSNR Diff: 0.036467 (New Max), Reward: -0.54, 22:37:06 Pre-flip Model Output=0.843271, New State Value=1, Flip Count=246, Flip Pixcel=0 * 121 * 216\n",
      "Step: 800, PSNR Before: 25.044884, PSNR After: 25.044662, PSNR Change: -0.000221, PSNR Diff: 0.040123 (New Max), Reward: -0.18, 22:37:07 Pre-flip Model Output=0.603470, New State Value=1, Flip Count=269, Flip Pixcel=0 * 146 * 58\n",
      "Step: 900, PSNR Before: 25.050301, PSNR After: 25.050297, PSNR Change: -0.000004, PSNR Diff: 0.045757 (New Max), Reward: -0.00, 22:37:08 Pre-flip Model Output=0.428825, New State Value=0, Flip Count=307, Flip Pixcel=7 * 186 * 28\n",
      "Step: 1000, PSNR Before: 25.053120, PSNR After: 25.053347, PSNR Change: 0.000227, PSNR Diff: 0.048807 (New Max), Reward: 0.18, 22:37:09 Pre-flip Model Output=0.707708, New State Value=0, Flip Count=328, Flip Pixcel=2 * 145 * 87\n",
      "Step: 1100, PSNR Before: 25.057943, PSNR After: 25.057491, PSNR Change: -0.000452, PSNR Diff: 0.052952 (New Max), Reward: -0.36, 22:37:10 Pre-flip Model Output=0.745035, New State Value=1, Flip Count=364, Flip Pixcel=0 * 134 * 144\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.43e+03    |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 3584        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032225337 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.187      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.103      |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Step: 1200, PSNR Before: 25.063389, PSNR After: 25.063372, PSNR Change: -0.000017, PSNR Diff: 0.058832 (New Max), Reward: -0.01, 22:37:23 Pre-flip Model Output=0.201311, New State Value=0, Flip Count=396, Flip Pixcel=0 * 230 * 167\n",
      "Step: 1300, PSNR Before: 25.069818, PSNR After: 25.069866, PSNR Change: 0.000048, PSNR Diff: 0.065327 (New Max), Reward: 0.04, 22:37:25 Pre-flip Model Output=0.946898, New State Value=0, Flip Count=434, Flip Pixcel=1 * 102 * 114\n",
      "Step: 1400, PSNR Before: 25.074619, PSNR After: 25.074453, PSNR Change: -0.000166, PSNR Diff: 0.069914 (New Max), Reward: -0.13, 22:37:26 Pre-flip Model Output=0.638483, New State Value=1, Flip Count=465, Flip Pixcel=0 * 170 * 252\n",
      "Step: 1500, PSNR Before: 25.077461, PSNR After: 25.077202, PSNR Change: -0.000259, PSNR Diff: 0.072662 (New Max), Reward: -0.21, 22:37:27 Pre-flip Model Output=0.364135, New State Value=0, Flip Count=491, Flip Pixcel=1 * 153 * 199\n",
      "Step: 1600, PSNR Before: 25.081997, PSNR After: 25.081873, PSNR Change: -0.000124, PSNR Diff: 0.077333 (New Max), Reward: -0.10, 22:37:28 Pre-flip Model Output=0.002057, New State Value=0, Flip Count=524, Flip Pixcel=6 * 61 * 60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.43e+03    |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033241287 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.169      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0983     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "Step: 1700, PSNR Before: 25.086126, PSNR After: 25.086250, PSNR Change: 0.000124, PSNR Diff: 0.081711 (New Max), Reward: 0.10, 22:37:41 Pre-flip Model Output=0.002144, New State Value=1, Flip Count=551, Flip Pixcel=5 * 250 * 33\n",
      "Step: 1800, PSNR Before: 25.090614, PSNR After: 25.090694, PSNR Change: 0.000080, PSNR Diff: 0.086155 (New Max), Reward: 0.06, 22:37:42 Pre-flip Model Output=0.002244, New State Value=1, Flip Count=584, Flip Pixcel=6 * 61 * 112\n",
      "Step: 1900, PSNR Before: 25.096474, PSNR After: 25.096153, PSNR Change: -0.000320, PSNR Diff: 0.091614 (New Max), Reward: -0.26, 22:37:43 Pre-flip Model Output=0.579881, New State Value=1, Flip Count=624, Flip Pixcel=7 * 152 * 173\n",
      "Step: 2000, PSNR Before: 25.100677, PSNR After: 25.100882, PSNR Change: 0.000204, PSNR Diff: 0.096342 (New Max), Reward: 0.16, 22:37:44 Pre-flip Model Output=0.049325, New State Value=1, Flip Count=653, Flip Pixcel=1 * 232 * 97\n",
      "\u001b[94mStep: 2089, PSNR Before: 25.104462, PSNR After: 25.104603, PSNR Change: 0.000141, PSNR Diff: 0.100063 (New Max), Reward: 0.11, 22:37:45 Pre-flip Model Output=0.369763, New State Value=1, Flip Count=676, Flip Pixcel=4 * 141 * 109\u001b[0m\n",
      "\u001b[94mStep: 2092, PSNR Before: 25.104603, PSNR After: 25.104671, PSNR Change: 0.000069, PSNR Diff: 0.100132 (New Max), Reward: 0.05, 22:37:45 Pre-flip Model Output=0.179286, New State Value=1, Flip Count=677, Flip Pixcel=6 * 51 * 114\u001b[0m\n",
      "\u001b[94mStep: 2093, PSNR Before: 25.104671, PSNR After: 25.104685, PSNR Change: 0.000013, PSNR Diff: 0.100145 (New Max), Reward: 0.01, 22:37:45 Pre-flip Model Output=0.402087, New State Value=1, Flip Count=678, Flip Pixcel=6 * 192 * 32\u001b[0m\n",
      "\u001b[94mStep: 2094, PSNR Before: 25.104685, PSNR After: 25.104704, PSNR Change: 0.000019, PSNR Diff: 0.100164 (New Max), Reward: 0.02, 22:37:45 Pre-flip Model Output=0.754586, New State Value=0, Flip Count=679, Flip Pixcel=0 * 246 * 35\u001b[0m\n",
      "\u001b[94mStep: 2095, PSNR Before: 25.104704, PSNR After: 25.104977, PSNR Change: 0.000273, PSNR Diff: 0.100437 (New Max), Reward: 0.22, 22:37:45 Pre-flip Model Output=0.045237, New State Value=1, Flip Count=680, Flip Pixcel=2 * 143 * 72\u001b[0m\n",
      "Step: 2100, PSNR Before: 25.104977, PSNR After: 25.104519, PSNR Change: -0.000458, PSNR Diff: 0.099979 (New Max), Reward: -0.37, 22:37:45 Pre-flip Model Output=0.445986, New State Value=0, Flip Count=680, Flip Pixcel=1 * 153 * 29\n",
      "\u001b[94mStep: 2101, PSNR Before: 25.104977, PSNR After: 25.105124, PSNR Change: 0.000147, PSNR Diff: 0.100584 (New Max), Reward: 0.12, 22:37:45 Pre-flip Model Output=0.820198, New State Value=0, Flip Count=681, Flip Pixcel=2 * 109 * 219\u001b[0m\n",
      "\u001b[94mStep: 2104, PSNR Before: 25.105124, PSNR After: 25.105343, PSNR Change: 0.000219, PSNR Diff: 0.100803 (New Max), Reward: 0.18, 22:37:45 Pre-flip Model Output=0.067399, New State Value=1, Flip Count=682, Flip Pixcel=5 * 21 * 171\u001b[0m\n",
      "\u001b[94mStep: 2108, PSNR Before: 25.105343, PSNR After: 25.105352, PSNR Change: 0.000010, PSNR Diff: 0.100813 (New Max), Reward: 0.01, 22:37:45 Pre-flip Model Output=0.079390, New State Value=1, Flip Count=683, Flip Pixcel=6 * 165 * 14\u001b[0m\n",
      "\u001b[94mStep: 2110, PSNR Before: 25.105352, PSNR After: 25.105619, PSNR Change: 0.000267, PSNR Diff: 0.101080 (New Max), Reward: 0.21, 22:37:45 Pre-flip Model Output=0.601279, New State Value=0, Flip Count=684, Flip Pixcel=3 * 53 * 0\u001b[0m\n",
      "\u001b[94mStep: 2114, PSNR Before: 25.105619, PSNR After: 25.105639, PSNR Change: 0.000019, PSNR Diff: 0.101099 (New Max), Reward: 0.02, 22:37:46 Pre-flip Model Output=0.989779, New State Value=0, Flip Count=685, Flip Pixcel=5 * 253 * 255\u001b[0m\n",
      "\u001b[94mStep: 2116, PSNR Before: 25.105639, PSNR After: 25.105724, PSNR Change: 0.000086, PSNR Diff: 0.101185 (New Max), Reward: 0.07, 22:37:46 Pre-flip Model Output=0.248558, New State Value=1, Flip Count=686, Flip Pixcel=6 * 55 * 210\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 3\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:37:46\u001b[0m\n",
      "\u001b[41mEpisode 2: Total Reward: -226.40\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004160, PSNR Change: -0.000380, PSNR Diff: -0.000380 (New Max), Reward: -0.30, 22:37:46 Pre-flip Model Output=0.002756, New State Value=0, Flip Count=0, Flip Pixcel=7 * 157 * 3\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -288        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 4608        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036977567 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.182      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.105      |\n",
      "|    value_loss           | 0.265       |\n",
      "-----------------------------------------\n",
      "Step: 100, PSNR Before: 25.008507, PSNR After: 25.008320, PSNR Change: -0.000187, PSNR Diff: 0.003780 (New Max), Reward: -0.15, 22:37:59 Pre-flip Model Output=0.017321, New State Value=0, Flip Count=24, Flip Pixcel=6 * 229 * 250\n",
      "Step: 200, PSNR Before: 25.012083, PSNR After: 25.012142, PSNR Change: 0.000059, PSNR Diff: 0.007603 (New Max), Reward: 0.05, 22:38:00 Pre-flip Model Output=0.193102, New State Value=1, Flip Count=58, Flip Pixcel=2 * 224 * 157\n",
      "Step: 300, PSNR Before: 25.016909, PSNR After: 25.016611, PSNR Change: -0.000298, PSNR Diff: 0.012072 (New Max), Reward: -0.24, 22:38:02 Pre-flip Model Output=0.695096, New State Value=1, Flip Count=89, Flip Pixcel=0 * 71 * 114\n",
      "Step: 400, PSNR Before: 25.022268, PSNR After: 25.021849, PSNR Change: -0.000420, PSNR Diff: 0.017309 (New Max), Reward: -0.34, 22:38:03 Pre-flip Model Output=0.003867, New State Value=0, Flip Count=128, Flip Pixcel=2 * 67 * 5\n",
      "Step: 500, PSNR Before: 25.027800, PSNR After: 25.027866, PSNR Change: 0.000067, PSNR Diff: 0.023327 (New Max), Reward: 0.05, 22:38:04 Pre-flip Model Output=0.365001, New State Value=1, Flip Count=168, Flip Pixcel=3 * 33 * 86\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -288        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028470134 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.553       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "Step: 600, PSNR Before: 25.032124, PSNR After: 25.032137, PSNR Change: 0.000013, PSNR Diff: 0.027597 (New Max), Reward: 0.01, 22:38:17 Pre-flip Model Output=0.001538, New State Value=1, Flip Count=204, Flip Pixcel=6 * 145 * 214\n",
      "Step: 700, PSNR Before: 25.035904, PSNR After: 25.035618, PSNR Change: -0.000286, PSNR Diff: 0.031078 (New Max), Reward: -0.23, 22:38:18 Pre-flip Model Output=0.481528, New State Value=0, Flip Count=230, Flip Pixcel=1 * 49 * 72\n",
      "Step: 800, PSNR Before: 25.039719, PSNR After: 25.039600, PSNR Change: -0.000118, PSNR Diff: 0.035061 (New Max), Reward: -0.09, 22:38:19 Pre-flip Model Output=0.055054, New State Value=0, Flip Count=258, Flip Pixcel=1 * 88 * 237\n",
      "Step: 900, PSNR Before: 25.044971, PSNR After: 25.044750, PSNR Change: -0.000221, PSNR Diff: 0.040211 (New Max), Reward: -0.18, 22:38:20 Pre-flip Model Output=0.067651, New State Value=0, Flip Count=291, Flip Pixcel=7 * 25 * 9\n",
      "Step: 1000, PSNR Before: 25.048763, PSNR After: 25.047901, PSNR Change: -0.000862, PSNR Diff: 0.043362 (New Max), Reward: -0.69, 22:38:21 Pre-flip Model Output=0.670394, New State Value=1, Flip Count=316, Flip Pixcel=7 * 192 * 29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -288        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 5632        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042843673 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.19       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.111      |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Step: 1100, PSNR Before: 25.053728, PSNR After: 25.053339, PSNR Change: -0.000389, PSNR Diff: 0.048800 (New Max), Reward: -0.31, 22:38:34 Pre-flip Model Output=0.079670, New State Value=0, Flip Count=343, Flip Pixcel=3 * 242 * 152\n",
      "Step: 1200, PSNR Before: 25.058050, PSNR After: 25.057890, PSNR Change: -0.000160, PSNR Diff: 0.053350 (New Max), Reward: -0.13, 22:38:35 Pre-flip Model Output=0.428523, New State Value=0, Flip Count=372, Flip Pixcel=5 * 67 * 118\n",
      "Step: 1300, PSNR Before: 25.063610, PSNR After: 25.063908, PSNR Change: 0.000298, PSNR Diff: 0.059368 (New Max), Reward: 0.24, 22:38:37 Pre-flip Model Output=0.000002, New State Value=1, Flip Count=403, Flip Pixcel=5 * 92 * 5\n",
      "Step: 1400, PSNR Before: 25.069328, PSNR After: 25.068756, PSNR Change: -0.000572, PSNR Diff: 0.064217 (New Max), Reward: -0.46, 22:38:38 Pre-flip Model Output=0.301722, New State Value=0, Flip Count=433, Flip Pixcel=7 * 146 * 104\n",
      "Step: 1500, PSNR Before: 25.072983, PSNR After: 25.072672, PSNR Change: -0.000311, PSNR Diff: 0.068132 (New Max), Reward: -0.25, 22:38:39 Pre-flip Model Output=0.878465, New State Value=1, Flip Count=455, Flip Pixcel=5 * 100 * 169\n",
      "Step: 1600, PSNR Before: 25.076887, PSNR After: 25.077059, PSNR Change: 0.000172, PSNR Diff: 0.072519 (New Max), Reward: 0.14, 22:38:40 Pre-flip Model Output=0.256518, New State Value=1, Flip Count=485, Flip Pixcel=3 * 216 * 130\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.27e+03   |\n",
      "|    ep_rew_mean          | -288       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 207        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03865902 |\n",
      "|    clip_fraction        | 0.392      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.189     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.108     |\n",
      "|    value_loss           | 0.297      |\n",
      "----------------------------------------\n",
      "Step: 1700, PSNR Before: 25.083427, PSNR After: 25.083160, PSNR Change: -0.000267, PSNR Diff: 0.078621 (New Max), Reward: -0.21, 22:38:53 Pre-flip Model Output=0.233993, New State Value=0, Flip Count=516, Flip Pixcel=7 * 37 * 233\n",
      "Step: 1800, PSNR Before: 25.087379, PSNR After: 25.087452, PSNR Change: 0.000072, PSNR Diff: 0.082912 (New Max), Reward: 0.06, 22:38:54 Pre-flip Model Output=0.700246, New State Value=0, Flip Count=544, Flip Pixcel=6 * 27 * 197\n",
      "Step: 1900, PSNR Before: 25.092560, PSNR After: 25.092394, PSNR Change: -0.000166, PSNR Diff: 0.087854 (New Max), Reward: -0.13, 22:38:55 Pre-flip Model Output=0.524051, New State Value=1, Flip Count=578, Flip Pixcel=0 * 87 * 102\n",
      "Step: 2000, PSNR Before: 25.096600, PSNR After: 25.096319, PSNR Change: -0.000280, PSNR Diff: 0.091780 (New Max), Reward: -0.22, 22:38:56 Pre-flip Model Output=0.803185, New State Value=1, Flip Count=614, Flip Pixcel=7 * 247 * 99\n",
      "Step: 2100, PSNR Before: 25.100037, PSNR After: 25.099503, PSNR Change: -0.000534, PSNR Diff: 0.094963 (New Max), Reward: -0.43, 22:38:57 Pre-flip Model Output=0.000973, New State Value=0, Flip Count=641, Flip Pixcel=5 * 38 * 18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -288        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 6656        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042972535 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.204      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.112      |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Step: 2200, PSNR Before: 25.103130, PSNR After: 25.102638, PSNR Change: -0.000492, PSNR Diff: 0.098099 (New Max), Reward: -0.39, 22:39:10 Pre-flip Model Output=0.812323, New State Value=1, Flip Count=668, Flip Pixcel=0 * 35 * 41\n",
      "\u001b[94mStep: 2243, PSNR Before: 25.104538, PSNR After: 25.104683, PSNR Change: 0.000145, PSNR Diff: 0.100143 (New Max), Reward: 0.12, 22:39:11 Pre-flip Model Output=0.000090, New State Value=1, Flip Count=685, Flip Pixcel=3 * 190 * 147\u001b[0m\n",
      "\u001b[94mStep: 2246, PSNR Before: 25.104683, PSNR After: 25.104731, PSNR Change: 0.000048, PSNR Diff: 0.100191 (New Max), Reward: 0.04, 22:39:11 Pre-flip Model Output=0.280981, New State Value=1, Flip Count=686, Flip Pixcel=4 * 122 * 19\u001b[0m\n",
      "\u001b[94mStep: 2247, PSNR Before: 25.104731, PSNR After: 25.104801, PSNR Change: 0.000071, PSNR Diff: 0.100262 (New Max), Reward: 0.06, 22:39:11 Pre-flip Model Output=0.350464, New State Value=1, Flip Count=687, Flip Pixcel=0 * 95 * 158\u001b[0m\n",
      "\u001b[94mStep: 2248, PSNR Before: 25.104801, PSNR After: 25.104818, PSNR Change: 0.000017, PSNR Diff: 0.100279 (New Max), Reward: 0.01, 22:39:11 Pre-flip Model Output=0.151254, New State Value=1, Flip Count=688, Flip Pixcel=6 * 45 * 61\u001b[0m\n",
      "\u001b[94mStep: 2254, PSNR Before: 25.104818, PSNR After: 25.104906, PSNR Change: 0.000088, PSNR Diff: 0.100367 (New Max), Reward: 0.07, 22:39:11 Pre-flip Model Output=0.000003, New State Value=1, Flip Count=689, Flip Pixcel=1 * 172 * 160\u001b[0m\n",
      "\u001b[94mStep: 2255, PSNR Before: 25.104906, PSNR After: 25.105150, PSNR Change: 0.000244, PSNR Diff: 0.100611 (New Max), Reward: 0.20, 22:39:11 Pre-flip Model Output=0.376772, New State Value=1, Flip Count=690, Flip Pixcel=6 * 231 * 115\u001b[0m\n",
      "\u001b[94mStep: 2258, PSNR Before: 25.105150, PSNR After: 25.105152, PSNR Change: 0.000002, PSNR Diff: 0.100613 (New Max), Reward: 0.00, 22:39:11 Pre-flip Model Output=0.132457, New State Value=1, Flip Count=691, Flip Pixcel=4 * 91 * 180\u001b[0m\n",
      "\u001b[94mStep: 2259, PSNR Before: 25.105152, PSNR After: 25.105263, PSNR Change: 0.000111, PSNR Diff: 0.100723 (New Max), Reward: 0.09, 22:39:11 Pre-flip Model Output=0.885123, New State Value=0, Flip Count=692, Flip Pixcel=4 * 209 * 207\u001b[0m\n",
      "\u001b[94mStep: 2265, PSNR Before: 25.105263, PSNR After: 25.105343, PSNR Change: 0.000080, PSNR Diff: 0.100803 (New Max), Reward: 0.06, 22:39:11 Pre-flip Model Output=0.480637, New State Value=1, Flip Count=693, Flip Pixcel=0 * 225 * 162\u001b[0m\n",
      "\u001b[94mStep: 2275, PSNR Before: 25.105343, PSNR After: 25.105562, PSNR Change: 0.000219, PSNR Diff: 0.101023 (New Max), Reward: 0.18, 22:39:11 Pre-flip Model Output=0.003627, New State Value=1, Flip Count=694, Flip Pixcel=6 * 53 * 36\u001b[0m\n",
      "\u001b[94mStep: 2276, PSNR Before: 25.105562, PSNR After: 25.105570, PSNR Change: 0.000008, PSNR Diff: 0.101030 (New Max), Reward: 0.01, 22:39:11 Pre-flip Model Output=0.268376, New State Value=1, Flip Count=695, Flip Pixcel=7 * 0 * 87\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 4\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:39:11\u001b[0m\n",
      "\u001b[41mEpisode 3: Total Reward: -304.91\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004555, PSNR Change: 0.000015, PSNR Diff: 0.000015 (New Max), Reward: 0.01, 22:39:11 Pre-flip Model Output=0.920413, New State Value=0, Flip Count=1, Flip Pixcel=7 * 196 * 36\n",
      "Step: 100, PSNR Before: 25.007572, PSNR After: 25.007780, PSNR Change: 0.000208, PSNR Diff: 0.003241 (New Max), Reward: 0.17, 22:39:13 Pre-flip Model Output=0.228850, New State Value=1, Flip Count=24, Flip Pixcel=6 * 235 * 134\n",
      "Step: 200, PSNR Before: 25.012600, PSNR After: 25.012352, PSNR Change: -0.000248, PSNR Diff: 0.007812 (New Max), Reward: -0.20, 22:39:14 Pre-flip Model Output=0.729680, New State Value=1, Flip Count=52, Flip Pixcel=5 * 60 * 179\n",
      "Step: 300, PSNR Before: 25.016491, PSNR After: 25.016195, PSNR Change: -0.000296, PSNR Diff: 0.011656 (New Max), Reward: -0.24, 22:39:15 Pre-flip Model Output=0.005818, New State Value=0, Flip Count=80, Flip Pixcel=3 * 92 * 27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -294        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044850133 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.198      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.113      |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Step: 400, PSNR Before: 25.019693, PSNR After: 25.019726, PSNR Change: 0.000032, PSNR Diff: 0.015186 (New Max), Reward: 0.03, 22:39:28 Pre-flip Model Output=0.207007, New State Value=1, Flip Count=110, Flip Pixcel=4 * 180 * 90\n",
      "Step: 500, PSNR Before: 25.023241, PSNR After: 25.022961, PSNR Change: -0.000280, PSNR Diff: 0.018421 (New Max), Reward: -0.22, 22:39:29 Pre-flip Model Output=0.219622, New State Value=0, Flip Count=137, Flip Pixcel=5 * 223 * 250\n",
      "Step: 600, PSNR Before: 25.027830, PSNR After: 25.027845, PSNR Change: 0.000015, PSNR Diff: 0.023306 (New Max), Reward: 0.01, 22:39:30 Pre-flip Model Output=0.220742, New State Value=1, Flip Count=172, Flip Pixcel=6 * 63 * 248\n",
      "Step: 700, PSNR Before: 25.034058, PSNR After: 25.034044, PSNR Change: -0.000013, PSNR Diff: 0.029505 (New Max), Reward: -0.01, 22:39:31 Pre-flip Model Output=0.094820, New State Value=0, Flip Count=203, Flip Pixcel=2 * 31 * 108\n",
      "Step: 800, PSNR Before: 25.037079, PSNR After: 25.036983, PSNR Change: -0.000095, PSNR Diff: 0.032444 (New Max), Reward: -0.08, 22:39:32 Pre-flip Model Output=0.194042, New State Value=0, Flip Count=225, Flip Pixcel=6 * 217 * 45\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -294        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 7680        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021594265 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.71        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "Step: 900, PSNR Before: 25.040884, PSNR After: 25.040693, PSNR Change: -0.000191, PSNR Diff: 0.036154 (New Max), Reward: -0.15, 22:39:45 Pre-flip Model Output=0.002624, New State Value=0, Flip Count=255, Flip Pixcel=6 * 44 * 202\n",
      "Step: 1000, PSNR Before: 25.046921, PSNR After: 25.046490, PSNR Change: -0.000431, PSNR Diff: 0.041950 (New Max), Reward: -0.34, 22:39:46 Pre-flip Model Output=0.022043, New State Value=0, Flip Count=288, Flip Pixcel=5 * 6 * 148\n",
      "Step: 1100, PSNR Before: 25.051735, PSNR After: 25.051941, PSNR Change: 0.000206, PSNR Diff: 0.047401 (New Max), Reward: 0.16, 22:39:47 Pre-flip Model Output=0.109690, New State Value=1, Flip Count=322, Flip Pixcel=4 * 211 * 108\n",
      "Step: 1200, PSNR Before: 25.054855, PSNR After: 25.054876, PSNR Change: 0.000021, PSNR Diff: 0.050337 (New Max), Reward: 0.02, 22:39:49 Pre-flip Model Output=0.500550, New State Value=0, Flip Count=349, Flip Pixcel=7 * 16 * 197\n",
      "Step: 1300, PSNR Before: 25.058891, PSNR After: 25.059212, PSNR Change: 0.000320, PSNR Diff: 0.054672 (New Max), Reward: 0.26, 22:39:50 Pre-flip Model Output=0.506910, New State Value=0, Flip Count=384, Flip Pixcel=7 * 95 * 16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -294        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046438843 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.14       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.111      |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "Step: 1400, PSNR Before: 25.064394, PSNR After: 25.064678, PSNR Change: 0.000284, PSNR Diff: 0.060139 (New Max), Reward: 0.23, 22:40:03 Pre-flip Model Output=0.492283, New State Value=1, Flip Count=412, Flip Pixcel=1 * 181 * 108\n",
      "Step: 1500, PSNR Before: 25.068134, PSNR After: 25.068031, PSNR Change: -0.000103, PSNR Diff: 0.063492 (New Max), Reward: -0.08, 22:40:04 Pre-flip Model Output=0.011378, New State Value=0, Flip Count=442, Flip Pixcel=2 * 68 * 212\n",
      "Step: 1600, PSNR Before: 25.073181, PSNR After: 25.073118, PSNR Change: -0.000063, PSNR Diff: 0.068579 (New Max), Reward: -0.05, 22:40:05 Pre-flip Model Output=0.284977, New State Value=0, Flip Count=475, Flip Pixcel=7 * 140 * 202\n",
      "Step: 1700, PSNR Before: 25.079367, PSNR After: 25.079060, PSNR Change: -0.000307, PSNR Diff: 0.074520 (New Max), Reward: -0.25, 22:40:06 Pre-flip Model Output=0.002530, New State Value=0, Flip Count=511, Flip Pixcel=3 * 133 * 44\n",
      "Step: 1800, PSNR Before: 25.083895, PSNR After: 25.083817, PSNR Change: -0.000078, PSNR Diff: 0.079277 (New Max), Reward: -0.06, 22:40:07 Pre-flip Model Output=0.113184, New State Value=0, Flip Count=543, Flip Pixcel=5 * 5 * 139\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.27e+03    |\n",
      "|    ep_rew_mean          | -294        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 8704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047847595 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.166      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.114      |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Step: 1900, PSNR Before: 25.090103, PSNR After: 25.090464, PSNR Change: 0.000360, PSNR Diff: 0.085924 (New Max), Reward: 0.29, 22:40:20 Pre-flip Model Output=0.124693, New State Value=1, Flip Count=586, Flip Pixcel=4 * 171 * 176\n",
      "Step: 2000, PSNR Before: 25.095123, PSNR After: 25.095312, PSNR Change: 0.000189, PSNR Diff: 0.090773 (New Max), Reward: 0.15, 22:40:21 Pre-flip Model Output=0.444426, New State Value=1, Flip Count=624, Flip Pixcel=6 * 79 * 7\n",
      "Step: 2100, PSNR Before: 25.100615, PSNR After: 25.100674, PSNR Change: 0.000059, PSNR Diff: 0.096134 (New Max), Reward: 0.05, 22:40:22 Pre-flip Model Output=0.001274, New State Value=1, Flip Count=655, Flip Pixcel=3 * 221 * 136\n",
      "\u001b[94mStep: 2193, PSNR Before: 25.104443, PSNR After: 25.104551, PSNR Change: 0.000109, PSNR Diff: 0.100012 (New Max), Reward: 0.09, 22:40:23 Pre-flip Model Output=0.297578, New State Value=1, Flip Count=681, Flip Pixcel=6 * 139 * 102\u001b[0m\n",
      "\u001b[94mStep: 2197, PSNR Before: 25.104551, PSNR After: 25.104851, PSNR Change: 0.000299, PSNR Diff: 0.100311 (New Max), Reward: 0.24, 22:40:23 Pre-flip Model Output=0.486814, New State Value=1, Flip Count=682, Flip Pixcel=7 * 216 * 181\u001b[0m\n",
      "\u001b[94mStep: 2198, PSNR Before: 25.104851, PSNR After: 25.104877, PSNR Change: 0.000027, PSNR Diff: 0.100338 (New Max), Reward: 0.02, 22:40:23 Pre-flip Model Output=0.554188, New State Value=0, Flip Count=683, Flip Pixcel=2 * 147 * 209\u001b[0m\n",
      "Step: 2200, PSNR Before: 25.104877, PSNR After: 25.104418, PSNR Change: -0.000460, PSNR Diff: 0.099878 (New Max), Reward: -0.37, 22:40:23 Pre-flip Model Output=0.403724, New State Value=0, Flip Count=683, Flip Pixcel=3 * 41 * 18\n",
      "\u001b[94mStep: 2207, PSNR Before: 25.104877, PSNR After: 25.105015, PSNR Change: 0.000137, PSNR Diff: 0.100475 (New Max), Reward: 0.11, 22:40:23 Pre-flip Model Output=0.638337, New State Value=0, Flip Count=684, Flip Pixcel=2 * 25 * 167\u001b[0m\n",
      "\u001b[94mStep: 2209, PSNR Before: 25.105015, PSNR After: 25.105167, PSNR Change: 0.000153, PSNR Diff: 0.100628 (New Max), Reward: 0.12, 22:40:23 Pre-flip Model Output=0.004727, New State Value=1, Flip Count=685, Flip Pixcel=6 * 45 * 152\u001b[0m\n",
      "\u001b[94mStep: 2210, PSNR Before: 25.105167, PSNR After: 25.105328, PSNR Change: 0.000160, PSNR Diff: 0.100788 (New Max), Reward: 0.13, 22:40:23 Pre-flip Model Output=0.890025, New State Value=0, Flip Count=686, Flip Pixcel=5 * 204 * 249\u001b[0m\n",
      "\u001b[94mStep: 2212, PSNR Before: 25.105328, PSNR After: 25.105555, PSNR Change: 0.000227, PSNR Diff: 0.101015 (New Max), Reward: 0.18, 22:40:23 Pre-flip Model Output=0.439996, New State Value=1, Flip Count=687, Flip Pixcel=0 * 86 * 119\u001b[0m\n",
      "\u001b[94mStep: 2218, PSNR Before: 25.105555, PSNR After: 25.105639, PSNR Change: 0.000084, PSNR Diff: 0.101099 (New Max), Reward: 0.07, 22:40:23 Pre-flip Model Output=0.641538, New State Value=0, Flip Count=688, Flip Pixcel=7 * 172 * 77\u001b[0m\n",
      "\u001b[94mStep: 2219, PSNR Before: 25.105639, PSNR After: 25.105747, PSNR Change: 0.000109, PSNR Diff: 0.101208 (New Max), Reward: 0.09, 22:40:23 Pre-flip Model Output=0.483368, New State Value=1, Flip Count=689, Flip Pixcel=1 * 147 * 167\u001b[0m\n",
      "\u001b[94mStep: 2222, PSNR Before: 25.105747, PSNR After: 25.105780, PSNR Change: 0.000032, PSNR Diff: 0.101240 (New Max), Reward: 0.03, 22:40:23 Pre-flip Model Output=0.087432, New State Value=1, Flip Count=690, Flip Pixcel=5 * 122 * 192\u001b[0m\n",
      "\u001b[94mStep: 2225, PSNR Before: 25.105780, PSNR After: 25.105793, PSNR Change: 0.000013, PSNR Diff: 0.101254 (New Max), Reward: 0.01, 22:40:24 Pre-flip Model Output=0.281768, New State Value=1, Flip Count=691, Flip Pixcel=2 * 42 * 98\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 5\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:40:24\u001b[0m\n",
      "\u001b[41mEpisode 4: Total Reward: -280.90\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004322, PSNR Change: -0.000217, PSNR Diff: -0.000217 (New Max), Reward: -0.17, 22:40:24 Pre-flip Model Output=0.328792, New State Value=0, Flip Count=0, Flip Pixcel=6 * 116 * 68\n",
      "Step: 100, PSNR Before: 25.011223, PSNR After: 25.011059, PSNR Change: -0.000164, PSNR Diff: 0.006519 (New Max), Reward: -0.13, 22:40:25 Pre-flip Model Output=0.195601, New State Value=0, Flip Count=37, Flip Pixcel=2 * 116 * 190\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -291        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048817117 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.112      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.115      |\n",
      "|    value_loss           | 0.331       |\n",
      "-----------------------------------------\n",
      "Step: 200, PSNR Before: 25.015045, PSNR After: 25.015182, PSNR Change: 0.000137, PSNR Diff: 0.010643 (New Max), Reward: 0.11, 22:40:38 Pre-flip Model Output=0.418814, New State Value=1, Flip Count=68, Flip Pixcel=4 * 68 * 122\n",
      "Step: 300, PSNR Before: 25.019751, PSNR After: 25.019644, PSNR Change: -0.000107, PSNR Diff: 0.015104 (New Max), Reward: -0.09, 22:40:39 Pre-flip Model Output=0.724712, New State Value=1, Flip Count=99, Flip Pixcel=5 * 8 * 52\n",
      "Step: 400, PSNR Before: 25.023306, PSNR After: 25.023346, PSNR Change: 0.000040, PSNR Diff: 0.018806 (New Max), Reward: 0.03, 22:40:40 Pre-flip Model Output=0.322557, New State Value=1, Flip Count=128, Flip Pixcel=6 * 202 * 39\n",
      "Step: 500, PSNR Before: 25.026871, PSNR After: 25.026840, PSNR Change: -0.000031, PSNR Diff: 0.022301 (New Max), Reward: -0.02, 22:40:41 Pre-flip Model Output=0.049670, New State Value=0, Flip Count=152, Flip Pixcel=4 * 45 * 30\n",
      "Step: 600, PSNR Before: 25.031271, PSNR After: 25.031334, PSNR Change: 0.000063, PSNR Diff: 0.026794 (New Max), Reward: 0.05, 22:40:42 Pre-flip Model Output=0.281873, New State Value=1, Flip Count=188, Flip Pixcel=2 * 66 * 56\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -291        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 9728        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029798359 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.61        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 10.5        |\n",
      "-----------------------------------------\n",
      "Step: 700, PSNR Before: 25.033520, PSNR After: 25.033476, PSNR Change: -0.000044, PSNR Diff: 0.028936 (New Max), Reward: -0.04, 22:40:55 Pre-flip Model Output=0.003447, New State Value=0, Flip Count=213, Flip Pixcel=4 * 50 * 37\n",
      "Step: 800, PSNR Before: 25.040380, PSNR After: 25.040474, PSNR Change: 0.000093, PSNR Diff: 0.035934 (New Max), Reward: 0.07, 22:40:56 Pre-flip Model Output=0.000164, New State Value=1, Flip Count=254, Flip Pixcel=3 * 58 * 215\n",
      "Step: 900, PSNR Before: 25.045647, PSNR After: 25.045004, PSNR Change: -0.000643, PSNR Diff: 0.040464 (New Max), Reward: -0.51, 22:40:57 Pre-flip Model Output=0.195622, New State Value=0, Flip Count=286, Flip Pixcel=5 * 40 * 120\n",
      "Step: 1000, PSNR Before: 25.050503, PSNR After: 25.050255, PSNR Change: -0.000248, PSNR Diff: 0.045715 (New Max), Reward: -0.20, 22:40:58 Pre-flip Model Output=0.456176, New State Value=0, Flip Count=320, Flip Pixcel=4 * 101 * 25\n",
      "Step: 1100, PSNR Before: 25.055256, PSNR After: 25.054951, PSNR Change: -0.000305, PSNR Diff: 0.050411 (New Max), Reward: -0.24, 22:40:59 Pre-flip Model Output=0.000060, New State Value=0, Flip Count=353, Flip Pixcel=5 * 10 * 228\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -291        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051856153 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.185      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Step: 1200, PSNR Before: 25.059900, PSNR After: 25.059826, PSNR Change: -0.000074, PSNR Diff: 0.055286 (New Max), Reward: -0.06, 22:41:12 Pre-flip Model Output=0.948188, New State Value=1, Flip Count=376, Flip Pixcel=3 * 167 * 234\n",
      "Step: 1300, PSNR Before: 25.063404, PSNR After: 25.063639, PSNR Change: 0.000235, PSNR Diff: 0.059099 (New Max), Reward: 0.19, 22:41:13 Pre-flip Model Output=0.652979, New State Value=0, Flip Count=398, Flip Pixcel=4 * 44 * 87\n",
      "Step: 1400, PSNR Before: 25.069366, PSNR After: 25.069496, PSNR Change: 0.000130, PSNR Diff: 0.064957 (New Max), Reward: 0.10, 22:41:14 Pre-flip Model Output=0.492446, New State Value=1, Flip Count=431, Flip Pixcel=0 * 124 * 145\n",
      "Step: 1500, PSNR Before: 25.075775, PSNR After: 25.075539, PSNR Change: -0.000237, PSNR Diff: 0.070999 (New Max), Reward: -0.19, 22:41:16 Pre-flip Model Output=0.019560, New State Value=0, Flip Count=468, Flip Pixcel=6 * 225 * 68\n",
      "Step: 1600, PSNR Before: 25.080225, PSNR After: 25.079845, PSNR Change: -0.000380, PSNR Diff: 0.075306 (New Max), Reward: -0.30, 22:41:17 Pre-flip Model Output=0.632076, New State Value=1, Flip Count=498, Flip Pixcel=7 * 235 * 224\n",
      "Step: 1700, PSNR Before: 25.085541, PSNR After: 25.085127, PSNR Change: -0.000414, PSNR Diff: 0.080587 (New Max), Reward: -0.33, 22:41:18 Pre-flip Model Output=0.000007, New State Value=0, Flip Count=534, Flip Pixcel=0 * 142 * 3\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -291        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 10752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051789023 |\n",
      "|    clip_fraction        | 0.529       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.206      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.12       |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Step: 1800, PSNR Before: 25.088989, PSNR After: 25.088688, PSNR Change: -0.000301, PSNR Diff: 0.084148 (New Max), Reward: -0.24, 22:41:31 Pre-flip Model Output=0.090371, New State Value=0, Flip Count=560, Flip Pixcel=6 * 233 * 147\n",
      "Step: 1900, PSNR Before: 25.091913, PSNR After: 25.091341, PSNR Change: -0.000572, PSNR Diff: 0.086802 (New Max), Reward: -0.46, 22:41:32 Pre-flip Model Output=0.660932, New State Value=1, Flip Count=581, Flip Pixcel=7 * 214 * 231\n",
      "Step: 2000, PSNR Before: 25.095306, PSNR After: 25.094780, PSNR Change: -0.000526, PSNR Diff: 0.090240 (New Max), Reward: -0.42, 22:41:33 Pre-flip Model Output=0.645611, New State Value=1, Flip Count=611, Flip Pixcel=5 * 232 * 227\n",
      "Step: 2100, PSNR Before: 25.099039, PSNR After: 25.099434, PSNR Change: 0.000395, PSNR Diff: 0.094894 (New Max), Reward: 0.32, 22:41:34 Pre-flip Model Output=0.503537, New State Value=0, Flip Count=642, Flip Pixcel=5 * 184 * 240\n",
      "Step: 2200, PSNR Before: 25.103809, PSNR After: 25.103617, PSNR Change: -0.000193, PSNR Diff: 0.099077 (New Max), Reward: -0.15, 22:41:35 Pre-flip Model Output=0.459058, New State Value=0, Flip Count=673, Flip Pixcel=7 * 25 * 40\n",
      "\u001b[94mStep: 2213, PSNR Before: 25.104494, PSNR After: 25.104540, PSNR Change: 0.000046, PSNR Diff: 0.100000 (New Max), Reward: 0.04, 22:41:35 Pre-flip Model Output=0.390757, New State Value=1, Flip Count=680, Flip Pixcel=7 * 218 * 64\u001b[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -291        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052657165 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.208      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.117      |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "\u001b[94mStep: 2219, PSNR Before: 25.104540, PSNR After: 25.104570, PSNR Change: 0.000031, PSNR Diff: 0.100031 (New Max), Reward: 0.02, 22:41:47 Pre-flip Model Output=0.352085, New State Value=1, Flip Count=681, Flip Pixcel=4 * 114 * 228\u001b[0m\n",
      "\u001b[94mStep: 2222, PSNR Before: 25.104570, PSNR After: 25.104582, PSNR Change: 0.000011, PSNR Diff: 0.100042 (New Max), Reward: 0.01, 22:41:47 Pre-flip Model Output=0.675604, New State Value=0, Flip Count=682, Flip Pixcel=1 * 102 * 87\u001b[0m\n",
      "\u001b[94mStep: 2223, PSNR Before: 25.104582, PSNR After: 25.104666, PSNR Change: 0.000084, PSNR Diff: 0.100126 (New Max), Reward: 0.07, 22:41:47 Pre-flip Model Output=0.480102, New State Value=1, Flip Count=683, Flip Pixcel=2 * 65 * 111\u001b[0m\n",
      "\u001b[94mStep: 2224, PSNR Before: 25.104666, PSNR After: 25.104753, PSNR Change: 0.000088, PSNR Diff: 0.100214 (New Max), Reward: 0.07, 22:41:47 Pre-flip Model Output=0.382655, New State Value=1, Flip Count=684, Flip Pixcel=7 * 254 * 35\u001b[0m\n",
      "\u001b[94mStep: 2225, PSNR Before: 25.104753, PSNR After: 25.104916, PSNR Change: 0.000162, PSNR Diff: 0.100376 (New Max), Reward: 0.13, 22:41:47 Pre-flip Model Output=0.000180, New State Value=1, Flip Count=685, Flip Pixcel=3 * 98 * 211\u001b[0m\n",
      "\u001b[94mStep: 2226, PSNR Before: 25.104916, PSNR After: 25.105206, PSNR Change: 0.000290, PSNR Diff: 0.100666 (New Max), Reward: 0.23, 22:41:47 Pre-flip Model Output=0.427886, New State Value=1, Flip Count=686, Flip Pixcel=4 * 136 * 168\u001b[0m\n",
      "\u001b[94mStep: 2227, PSNR Before: 25.105206, PSNR After: 25.105228, PSNR Change: 0.000023, PSNR Diff: 0.100689 (New Max), Reward: 0.02, 22:41:47 Pre-flip Model Output=0.566277, New State Value=0, Flip Count=687, Flip Pixcel=0 * 203 * 156\u001b[0m\n",
      "\u001b[94mStep: 2228, PSNR Before: 25.105228, PSNR After: 25.105320, PSNR Change: 0.000092, PSNR Diff: 0.100780 (New Max), Reward: 0.07, 22:41:47 Pre-flip Model Output=0.404929, New State Value=1, Flip Count=688, Flip Pixcel=1 * 48 * 1\u001b[0m\n",
      "\u001b[94mStep: 2229, PSNR Before: 25.105320, PSNR After: 25.105333, PSNR Change: 0.000013, PSNR Diff: 0.100794 (New Max), Reward: 0.01, 22:41:47 Pre-flip Model Output=0.329228, New State Value=1, Flip Count=689, Flip Pixcel=1 * 89 * 114\u001b[0m\n",
      "\u001b[94mStep: 2234, PSNR Before: 25.105333, PSNR After: 25.105461, PSNR Change: 0.000128, PSNR Diff: 0.100922 (New Max), Reward: 0.10, 22:41:47 Pre-flip Model Output=0.055556, New State Value=1, Flip Count=690, Flip Pixcel=4 * 199 * 124\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 6\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:41:47\u001b[0m\n",
      "\u001b[41mEpisode 5: Total Reward: -284.05\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004017, PSNR Change: -0.000523, PSNR Diff: -0.000523 (New Max), Reward: -0.42, 22:41:47 Pre-flip Model Output=0.661960, New State Value=1, Flip Count=0, Flip Pixcel=0 * 83 * 163\n",
      "Step: 100, PSNR Before: 25.009451, PSNR After: 25.009235, PSNR Change: -0.000216, PSNR Diff: 0.004696 (New Max), Reward: -0.17, 22:41:49 Pre-flip Model Output=0.002422, New State Value=0, Flip Count=37, Flip Pixcel=1 * 1 * 40\n",
      "Step: 200, PSNR Before: 25.013798, PSNR After: 25.013445, PSNR Change: -0.000353, PSNR Diff: 0.008905 (New Max), Reward: -0.28, 22:41:50 Pre-flip Model Output=0.473718, New State Value=0, Flip Count=67, Flip Pixcel=0 * 14 * 126\n",
      "Step: 300, PSNR Before: 25.018429, PSNR After: 25.018509, PSNR Change: 0.000080, PSNR Diff: 0.013969 (New Max), Reward: 0.06, 22:41:51 Pre-flip Model Output=0.453826, New State Value=1, Flip Count=103, Flip Pixcel=4 * 8 * 240\n",
      "Step: 400, PSNR Before: 25.023254, PSNR After: 25.022915, PSNR Change: -0.000340, PSNR Diff: 0.018375 (New Max), Reward: -0.27, 22:41:52 Pre-flip Model Output=0.210868, New State Value=0, Flip Count=137, Flip Pixcel=7 * 25 * 109\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.26e+03   |\n",
      "|    ep_rew_mean          | -289       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 400        |\n",
      "|    total_timesteps      | 11776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05148129 |\n",
      "|    clip_fraction        | 0.505      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.2      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.176     |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.119     |\n",
      "|    value_loss           | 0.236      |\n",
      "----------------------------------------\n",
      "Step: 500, PSNR Before: 25.026367, PSNR After: 25.026402, PSNR Change: 0.000034, PSNR Diff: 0.021862 (New Max), Reward: 0.03, 22:42:05 Pre-flip Model Output=0.562690, New State Value=0, Flip Count=166, Flip Pixcel=5 * 138 * 229\n",
      "Step: 600, PSNR Before: 25.029890, PSNR After: 25.029009, PSNR Change: -0.000881, PSNR Diff: 0.024469 (New Max), Reward: -0.70, 22:42:06 Pre-flip Model Output=0.854570, New State Value=1, Flip Count=194, Flip Pixcel=0 * 205 * 70\n",
      "Step: 700, PSNR Before: 25.033609, PSNR After: 25.033527, PSNR Change: -0.000082, PSNR Diff: 0.028988 (New Max), Reward: -0.07, 22:42:07 Pre-flip Model Output=0.000089, New State Value=0, Flip Count=223, Flip Pixcel=4 * 71 * 217\n",
      "Step: 800, PSNR Before: 25.037342, PSNR After: 25.037260, PSNR Change: -0.000082, PSNR Diff: 0.032721 (New Max), Reward: -0.07, 22:42:08 Pre-flip Model Output=0.605367, New State Value=1, Flip Count=252, Flip Pixcel=0 * 70 * 160\n",
      "Step: 900, PSNR Before: 25.041843, PSNR After: 25.041626, PSNR Change: -0.000217, PSNR Diff: 0.037086 (New Max), Reward: -0.17, 22:42:09 Pre-flip Model Output=0.291452, New State Value=0, Flip Count=285, Flip Pixcel=6 * 188 * 180\n",
      "Step: 1000, PSNR Before: 25.047165, PSNR After: 25.046627, PSNR Change: -0.000538, PSNR Diff: 0.042088 (New Max), Reward: -0.43, 22:42:11 Pre-flip Model Output=0.762860, New State Value=1, Flip Count=323, Flip Pixcel=1 * 86 * 151\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -289        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031568978 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 9.3         |\n",
      "-----------------------------------------\n",
      "Step: 1100, PSNR Before: 25.051615, PSNR After: 25.051374, PSNR Change: -0.000240, PSNR Diff: 0.046835 (New Max), Reward: -0.19, 22:42:23 Pre-flip Model Output=0.150148, New State Value=0, Flip Count=357, Flip Pixcel=6 * 169 * 21\n",
      "Step: 1200, PSNR Before: 25.056551, PSNR After: 25.056725, PSNR Change: 0.000174, PSNR Diff: 0.052185 (New Max), Reward: 0.14, 22:42:25 Pre-flip Model Output=0.275261, New State Value=1, Flip Count=389, Flip Pixcel=6 * 83 * 55\n",
      "Step: 1300, PSNR Before: 25.061787, PSNR After: 25.061882, PSNR Change: 0.000095, PSNR Diff: 0.057343 (New Max), Reward: 0.08, 22:42:26 Pre-flip Model Output=0.687494, New State Value=0, Flip Count=420, Flip Pixcel=0 * 118 * 139\n",
      "Step: 1400, PSNR Before: 25.068905, PSNR After: 25.068865, PSNR Change: -0.000040, PSNR Diff: 0.064325 (New Max), Reward: -0.03, 22:42:27 Pre-flip Model Output=0.892368, New State Value=1, Flip Count=466, Flip Pixcel=5 * 207 * 2\n",
      "Step: 1500, PSNR Before: 25.073393, PSNR After: 25.073750, PSNR Change: 0.000357, PSNR Diff: 0.069210 (New Max), Reward: 0.29, 22:42:28 Pre-flip Model Output=0.035848, New State Value=1, Flip Count=502, Flip Pixcel=4 * 188 * 73\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.26e+03    |\n",
      "|    ep_rew_mean          | -289        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 12800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053547762 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.206      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.118      |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "Step: 1600, PSNR Before: 25.079216, PSNR After: 25.079235, PSNR Change: 0.000019, PSNR Diff: 0.074696 (New Max), Reward: 0.02, 22:42:41 Pre-flip Model Output=0.139755, New State Value=1, Flip Count=533, Flip Pixcel=2 * 105 * 164\n",
      "Step: 1700, PSNR Before: 25.084503, PSNR After: 25.084436, PSNR Change: -0.000067, PSNR Diff: 0.079897 (New Max), Reward: -0.05, 22:42:42 Pre-flip Model Output=0.000125, New State Value=0, Flip Count=562, Flip Pixcel=5 * 5 * 154\n",
      "Step: 1800, PSNR Before: 25.089579, PSNR After: 25.089224, PSNR Change: -0.000355, PSNR Diff: 0.084684 (New Max), Reward: -0.28, 22:42:43 Pre-flip Model Output=0.647420, New State Value=1, Flip Count=600, Flip Pixcel=1 * 202 * 237\n",
      "Step: 1900, PSNR Before: 25.094746, PSNR After: 25.094406, PSNR Change: -0.000340, PSNR Diff: 0.089867 (New Max), Reward: -0.27, 22:42:44 Pre-flip Model Output=0.395730, New State Value=0, Flip Count=631, Flip Pixcel=6 * 107 * 23\n",
      "Step: 2000, PSNR Before: 25.101793, PSNR After: 25.101254, PSNR Change: -0.000540, PSNR Diff: 0.096714 (New Max), Reward: -0.43, 22:42:46 Pre-flip Model Output=0.723149, New State Value=1, Flip Count=672, Flip Pixcel=7 * 159 * 60\n",
      "\u001b[94mStep: 2027, PSNR Before: 25.104015, PSNR After: 25.104740, PSNR Change: 0.000725, PSNR Diff: 0.100201 (New Max), Reward: 0.58, 22:42:46 Pre-flip Model Output=0.473234, New State Value=1, Flip Count=685, Flip Pixcel=0 * 204 * 109\u001b[0m\n",
      "\u001b[94mStep: 2029, PSNR Before: 25.104740, PSNR After: 25.105316, PSNR Change: 0.000576, PSNR Diff: 0.100777 (New Max), Reward: 0.46, 22:42:46 Pre-flip Model Output=0.501690, New State Value=0, Flip Count=686, Flip Pixcel=5 * 121 * 204\u001b[0m\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.26e+03  |\n",
      "|    ep_rew_mean          | -289      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 29        |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 453       |\n",
      "|    total_timesteps      | 13312     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0557042 |\n",
      "|    clip_fraction        | 0.556     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.2     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.197    |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.121    |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "\u001b[94mStep: 2033, PSNR Before: 25.105316, PSNR After: 25.105389, PSNR Change: 0.000072, PSNR Diff: 0.100849 (New Max), Reward: 0.06, 22:42:57 Pre-flip Model Output=0.627906, New State Value=0, Flip Count=687, Flip Pixcel=6 * 234 * 253\u001b[0m\n",
      "\u001b[94mStep: 2035, PSNR Before: 25.105389, PSNR After: 25.105886, PSNR Change: 0.000498, PSNR Diff: 0.101347 (New Max), Reward: 0.40, 22:42:57 Pre-flip Model Output=0.316685, New State Value=1, Flip Count=688, Flip Pixcel=7 * 22 * 194\u001b[0m\n",
      "\u001b[94mStep: 2041, PSNR Before: 25.105886, PSNR After: 25.106060, PSNR Change: 0.000174, PSNR Diff: 0.101521 (New Max), Reward: 0.14, 22:42:58 Pre-flip Model Output=0.102080, New State Value=1, Flip Count=689, Flip Pixcel=7 * 113 * 225\u001b[0m\n",
      "\u001b[94mStep: 2044, PSNR Before: 25.106060, PSNR After: 25.106148, PSNR Change: 0.000088, PSNR Diff: 0.101608 (New Max), Reward: 0.07, 22:42:58 Pre-flip Model Output=0.366813, New State Value=1, Flip Count=690, Flip Pixcel=2 * 23 * 145\u001b[0m\n",
      "\u001b[94mStep: 2047, PSNR Before: 25.106148, PSNR After: 25.106285, PSNR Change: 0.000137, PSNR Diff: 0.101746 (New Max), Reward: 0.11, 22:42:58 Pre-flip Model Output=0.005840, New State Value=1, Flip Count=691, Flip Pixcel=5 * 234 * 0\u001b[0m\n",
      "\u001b[94mStep: 2048, PSNR Before: 25.106285, PSNR After: 25.106560, PSNR Change: 0.000275, PSNR Diff: 0.102020 (New Max), Reward: 0.22, 22:42:58 Pre-flip Model Output=0.349569, New State Value=1, Flip Count=692, Flip Pixcel=6 * 16 * 157\u001b[0m\n",
      "\u001b[94mStep: 2049, PSNR Before: 25.106560, PSNR After: 25.106623, PSNR Change: 0.000063, PSNR Diff: 0.102083 (New Max), Reward: 0.05, 22:42:58 Pre-flip Model Output=0.377825, New State Value=1, Flip Count=693, Flip Pixcel=7 * 173 * 152\u001b[0m\n",
      "\u001b[94mStep: 2050, PSNR Before: 25.106623, PSNR After: 25.106865, PSNR Change: 0.000242, PSNR Diff: 0.102325 (New Max), Reward: 0.19, 22:42:58 Pre-flip Model Output=0.210244, New State Value=1, Flip Count=694, Flip Pixcel=3 * 81 * 221\u001b[0m\n",
      "\u001b[94mStep: 2053, PSNR Before: 25.106865, PSNR After: 25.106911, PSNR Change: 0.000046, PSNR Diff: 0.102371 (New Max), Reward: 0.04, 22:42:58 Pre-flip Model Output=0.000046, New State Value=1, Flip Count=695, Flip Pixcel=4 * 227 * 89\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 7\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:42:58\u001b[0m\n",
      "\u001b[41mEpisode 6: Total Reward: -197.21\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004473, PSNR Change: -0.000067, PSNR Diff: -0.000067 (New Max), Reward: -0.05, 22:42:58 Pre-flip Model Output=0.472070, New State Value=0, Flip Count=0, Flip Pixcel=4 * 165 * 209\n",
      "Step: 100, PSNR Before: 25.009604, PSNR After: 25.009438, PSNR Change: -0.000166, PSNR Diff: 0.004898 (New Max), Reward: -0.13, 22:42:59 Pre-flip Model Output=0.045793, New State Value=0, Flip Count=34, Flip Pixcel=2 * 92 * 148\n",
      "Step: 200, PSNR Before: 25.015375, PSNR After: 25.015354, PSNR Change: -0.000021, PSNR Diff: 0.010815 (New Max), Reward: -0.02, 22:43:00 Pre-flip Model Output=0.301510, New State Value=0, Flip Count=74, Flip Pixcel=2 * 99 * 235\n",
      "Step: 300, PSNR Before: 25.019705, PSNR After: 25.019871, PSNR Change: 0.000166, PSNR Diff: 0.015331 (New Max), Reward: 0.13, 22:43:01 Pre-flip Model Output=0.766145, New State Value=0, Flip Count=99, Flip Pixcel=3 * 83 * 6\n",
      "Step: 400, PSNR Before: 25.022106, PSNR After: 25.022259, PSNR Change: 0.000153, PSNR Diff: 0.017719 (New Max), Reward: 0.12, 22:43:03 Pre-flip Model Output=0.366896, New State Value=1, Flip Count=118, Flip Pixcel=3 * 45 * 82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.22e+03    |\n",
      "|    ep_rew_mean          | -274        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 13824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053887248 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.16       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.12       |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n",
      "Step: 500, PSNR Before: 25.026567, PSNR After: 25.026529, PSNR Change: -0.000038, PSNR Diff: 0.021990 (New Max), Reward: -0.03, 22:43:15 Pre-flip Model Output=0.829968, New State Value=1, Flip Count=145, Flip Pixcel=5 * 106 * 85\n",
      "Step: 600, PSNR Before: 25.030859, PSNR After: 25.030878, PSNR Change: 0.000019, PSNR Diff: 0.026339 (New Max), Reward: 0.02, 22:43:17 Pre-flip Model Output=0.055725, New State Value=1, Flip Count=177, Flip Pixcel=2 * 54 * 87\n",
      "Step: 700, PSNR Before: 25.038391, PSNR After: 25.038033, PSNR Change: -0.000359, PSNR Diff: 0.033493 (New Max), Reward: -0.29, 22:43:18 Pre-flip Model Output=0.367077, New State Value=0, Flip Count=213, Flip Pixcel=1 * 111 * 50\n",
      "Step: 800, PSNR Before: 25.043800, PSNR After: 25.043354, PSNR Change: -0.000446, PSNR Diff: 0.038815 (New Max), Reward: -0.36, 22:43:19 Pre-flip Model Output=0.902982, New State Value=1, Flip Count=246, Flip Pixcel=7 * 172 * 224\n",
      "Step: 900, PSNR Before: 25.050392, PSNR After: 25.050274, PSNR Change: -0.000118, PSNR Diff: 0.045734 (New Max), Reward: -0.09, 22:43:20 Pre-flip Model Output=0.401141, New State Value=0, Flip Count=280, Flip Pixcel=7 * 222 * 74\n",
      "Step: 1000, PSNR Before: 25.054018, PSNR After: 25.053663, PSNR Change: -0.000355, PSNR Diff: 0.049124 (New Max), Reward: -0.28, 22:43:21 Pre-flip Model Output=0.777053, New State Value=1, Flip Count=302, Flip Pixcel=7 * 3 * 181\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.22e+03    |\n",
      "|    ep_rew_mean          | -274        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054092832 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.11       |\n",
      "|    value_loss           | 0.41        |\n",
      "-----------------------------------------\n",
      "Step: 1100, PSNR Before: 25.058506, PSNR After: 25.058382, PSNR Change: -0.000124, PSNR Diff: 0.053843 (New Max), Reward: -0.10, 22:43:34 Pre-flip Model Output=0.687596, New State Value=1, Flip Count=334, Flip Pixcel=1 * 234 * 23\n",
      "Step: 1200, PSNR Before: 25.063093, PSNR After: 25.062489, PSNR Change: -0.000605, PSNR Diff: 0.057949 (New Max), Reward: -0.48, 22:43:35 Pre-flip Model Output=0.364982, New State Value=0, Flip Count=367, Flip Pixcel=0 * 84 * 103\n",
      "Step: 1300, PSNR Before: 25.071142, PSNR After: 25.071325, PSNR Change: 0.000183, PSNR Diff: 0.066786 (New Max), Reward: 0.15, 22:43:36 Pre-flip Model Output=0.562331, New State Value=0, Flip Count=410, Flip Pixcel=5 * 81 * 76\n",
      "Step: 1400, PSNR Before: 25.075321, PSNR After: 25.075310, PSNR Change: -0.000011, PSNR Diff: 0.070770 (New Max), Reward: -0.01, 22:43:37 Pre-flip Model Output=0.000117, New State Value=0, Flip Count=437, Flip Pixcel=4 * 151 * 153\n",
      "Step: 1500, PSNR Before: 25.079573, PSNR After: 25.079401, PSNR Change: -0.000172, PSNR Diff: 0.074862 (New Max), Reward: -0.14, 22:43:39 Pre-flip Model Output=0.971763, New State Value=1, Flip Count=467, Flip Pixcel=4 * 113 * 72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.22e+03    |\n",
      "|    ep_rew_mean          | -274        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 506         |\n",
      "|    total_timesteps      | 14848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054532275 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.18       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.121      |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "Step: 1600, PSNR Before: 25.082134, PSNR After: 25.082140, PSNR Change: 0.000006, PSNR Diff: 0.077600 (New Max), Reward: 0.00, 22:43:51 Pre-flip Model Output=0.006021, New State Value=1, Flip Count=490, Flip Pixcel=4 * 206 * 213\n",
      "Step: 1700, PSNR Before: 25.086113, PSNR After: 25.086250, PSNR Change: 0.000137, PSNR Diff: 0.081711 (New Max), Reward: 0.11, 22:43:52 Pre-flip Model Output=0.286380, New State Value=1, Flip Count=527, Flip Pixcel=6 * 126 * 179\n",
      "Step: 1800, PSNR Before: 25.090494, PSNR After: 25.090485, PSNR Change: -0.000010, PSNR Diff: 0.085945 (New Max), Reward: -0.01, 22:43:54 Pre-flip Model Output=0.000744, New State Value=0, Flip Count=553, Flip Pixcel=2 * 39 * 243\n",
      "Step: 1900, PSNR Before: 25.096420, PSNR After: 25.096306, PSNR Change: -0.000114, PSNR Diff: 0.091766 (New Max), Reward: -0.09, 22:43:55 Pre-flip Model Output=0.208109, New State Value=0, Flip Count=591, Flip Pixcel=5 * 75 * 103\n",
      "Step: 2000, PSNR Before: 25.099066, PSNR After: 25.098715, PSNR Change: -0.000351, PSNR Diff: 0.094175 (New Max), Reward: -0.28, 22:43:56 Pre-flip Model Output=0.893809, New State Value=1, Flip Count=617, Flip Pixcel=7 * 211 * 75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.22e+03    |\n",
      "|    ep_rew_mean          | -274        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 523         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057746932 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.137      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.124      |\n",
      "|    value_loss           | 0.33        |\n",
      "-----------------------------------------\n",
      "\u001b[94mStep: 2078, PSNR Before: 25.104481, PSNR After: 25.104673, PSNR Change: 0.000193, PSNR Diff: 0.100134 (New Max), Reward: 0.15, 22:44:08 Pre-flip Model Output=0.387907, New State Value=1, Flip Count=646, Flip Pixcel=1 * 83 * 139\u001b[0m\n",
      "\u001b[94mStep: 2079, PSNR Before: 25.104673, PSNR After: 25.104824, PSNR Change: 0.000151, PSNR Diff: 0.100285 (New Max), Reward: 0.12, 22:44:08 Pre-flip Model Output=0.447937, New State Value=1, Flip Count=647, Flip Pixcel=7 * 127 * 148\u001b[0m\n",
      "\u001b[94mStep: 2080, PSNR Before: 25.104824, PSNR After: 25.104933, PSNR Change: 0.000109, PSNR Diff: 0.100393 (New Max), Reward: 0.09, 22:44:08 Pre-flip Model Output=0.401080, New State Value=1, Flip Count=648, Flip Pixcel=0 * 111 * 172\u001b[0m\n",
      "\u001b[94mStep: 2081, PSNR Before: 25.104933, PSNR After: 25.104940, PSNR Change: 0.000008, PSNR Diff: 0.100401 (New Max), Reward: 0.01, 22:44:08 Pre-flip Model Output=0.000934, New State Value=1, Flip Count=649, Flip Pixcel=4 * 162 * 189\u001b[0m\n",
      "\u001b[94mStep: 2083, PSNR Before: 25.104940, PSNR After: 25.104961, PSNR Change: 0.000021, PSNR Diff: 0.100422 (New Max), Reward: 0.02, 22:44:08 Pre-flip Model Output=0.088208, New State Value=1, Flip Count=650, Flip Pixcel=5 * 225 * 83\u001b[0m\n",
      "\u001b[94mStep: 2087, PSNR Before: 25.104961, PSNR After: 25.105211, PSNR Change: 0.000250, PSNR Diff: 0.100672 (New Max), Reward: 0.20, 22:44:09 Pre-flip Model Output=0.325215, New State Value=1, Flip Count=651, Flip Pixcel=5 * 151 * 119\u001b[0m\n",
      "\u001b[94mStep: 2092, PSNR Before: 25.105211, PSNR After: 25.105246, PSNR Change: 0.000034, PSNR Diff: 0.100706 (New Max), Reward: 0.03, 22:44:09 Pre-flip Model Output=0.000107, New State Value=1, Flip Count=652, Flip Pixcel=1 * 84 * 82\u001b[0m\n",
      "\u001b[94mStep: 2094, PSNR Before: 25.105246, PSNR After: 25.105352, PSNR Change: 0.000107, PSNR Diff: 0.100813 (New Max), Reward: 0.09, 22:44:09 Pre-flip Model Output=0.153818, New State Value=1, Flip Count=653, Flip Pixcel=6 * 129 * 219\u001b[0m\n",
      "Step: 2100, PSNR Before: 25.105352, PSNR After: 25.105303, PSNR Change: -0.000050, PSNR Diff: 0.100763 (New Max), Reward: -0.04, 22:44:09 Pre-flip Model Output=0.294843, New State Value=0, Flip Count=653, Flip Pixcel=2 * 177 * 243\n",
      "\u001b[94mStep: 2102, PSNR Before: 25.105352, PSNR After: 25.105522, PSNR Change: 0.000170, PSNR Diff: 0.100983 (New Max), Reward: 0.14, 22:44:09 Pre-flip Model Output=0.394284, New State Value=1, Flip Count=654, Flip Pixcel=1 * 91 * 121\u001b[0m\n",
      "\u001b[94mStep: 2104, PSNR Before: 25.105522, PSNR After: 25.105669, PSNR Change: 0.000147, PSNR Diff: 0.101130 (New Max), Reward: 0.12, 22:44:09 Pre-flip Model Output=0.141165, New State Value=1, Flip Count=655, Flip Pixcel=6 * 84 * 177\u001b[0m\n",
      "\u001b[94mStep: 2105, PSNR Before: 25.105669, PSNR After: 25.105755, PSNR Change: 0.000086, PSNR Diff: 0.101215 (New Max), Reward: 0.07, 22:44:09 Pre-flip Model Output=0.427153, New State Value=1, Flip Count=656, Flip Pixcel=2 * 116 * 242\u001b[0m\n",
      "\u001b[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\u001b[0m\n",
      "\u001b[40;93m[Episode Start] Currently using dataset file: ('dataset/0001.png',), Episode count: 8\u001b[0m\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.002122, Initial PSNR: 25.004539, 22:44:09\u001b[0m\n",
      "\u001b[41mEpisode 7: Total Reward: -239.18\u001b[0m\n",
      "Step: 0, PSNR Before: 25.004539, PSNR After: 25.004581, PSNR Change: 0.000042, PSNR Diff: 0.000042 (New Max), Reward: 0.03, 22:44:09 Pre-flip Model Output=0.538436, New State Value=0, Flip Count=1, Flip Pixcel=3 * 115 * 251\n",
      "Step: 100, PSNR Before: 25.006958, PSNR After: 25.006508, PSNR Change: -0.000450, PSNR Diff: 0.001968 (New Max), Reward: -0.36, 22:44:10 Pre-flip Model Output=0.002146, New State Value=0, Flip Count=24, Flip Pixcel=3 * 189 * 124\n",
      "Step: 200, PSNR Before: 25.011618, PSNR After: 25.011654, PSNR Change: 0.000036, PSNR Diff: 0.007114 (New Max), Reward: 0.03, 22:44:11 Pre-flip Model Output=0.000009, New State Value=1, Flip Count=61, Flip Pixcel=1 * 124 * 214\n",
      "Step: 300, PSNR Before: 25.016687, PSNR After: 25.016527, PSNR Change: -0.000160, PSNR Diff: 0.011988 (New Max), Reward: -0.13, 22:44:12 Pre-flip Model Output=0.289631, New State Value=0, Flip Count=94, Flip Pixcel=6 * 163 * 83\n",
      "Step: 400, PSNR Before: 25.019590, PSNR After: 25.019348, PSNR Change: -0.000242, PSNR Diff: 0.014809 (New Max), Reward: -0.19, 22:44:13 Pre-flip Model Output=0.000006, New State Value=0, Flip Count=112, Flip Pixcel=1 * 208 * 216\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.21e+03    |\n",
      "|    ep_rew_mean          | -269        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 15872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057168454 |\n",
      "|    clip_fraction        | 0.571       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.204      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.12       |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 718\u001b[0m\n\u001b[1;32m    715\u001b[0m callback \u001b[38;5;241m=\u001b[39m CallbackList([reward_logging_callback, stop_callback])\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart model saving at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[1], line 424\u001b[0m, in \u001b[0;36mBinaryHologramEnv.step\u001b[0;34m(self, action, lr, z)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflip_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 플립 증가\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# 현재 상태로 새로운 시뮬레이션 수행\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m binary_after \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    425\u001b[0m binary_after \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39mTensor(binary_after, meta\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m7.56e-6\u001b[39m, \u001b[38;5;241m7.56e-6\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwl\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m515e-9\u001b[39m})\n\u001b[1;32m    426\u001b[0m sim_after \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39msimulate(binary_after, z)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 로그를 저장할 디렉토리 설정\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# 현재 파일 이름과 실행 시간 가져오기\n",
    "if '__file__' in globals():\n",
    "    current_file = os.path.splitext(os.path.basename(__file__))[0]  # 현재 파일 이름(확장자 제거)\n",
    "else:\n",
    "    current_file = \"interactive\"  # 인터프리터나 노트북 환경에서 기본 파일 이름 사용\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # 현재 시간\n",
    "log_filename = os.path.join(log_dir, f\"{current_file}_{current_datetime}.log\")  # log 폴더에 파일 저장\n",
    "\n",
    "# 로그 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),  # 동적으로 생성된 파일 이름 사용\n",
    "        logging.StreamHandler()  # 콘솔 출력\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, data):\n",
    "        for file in self.files:\n",
    "            file.write(data)\n",
    "            file.flush()  # 실시간 저장\n",
    "\n",
    "    def flush(self):\n",
    "        for file in self.files:\n",
    "            file.flush()\n",
    "\n",
    "\n",
    "# stdout을 파일과 콘솔로 동시에 출력\n",
    "log_file = open(log_filename, \"a\")\n",
    "sys.stdout = Tee(sys.stdout, log_file)\n",
    "\n",
    "# 테스트 출력\n",
    "print(\"이 메시지는 콘솔과 파일에 동시에 기록됩니다.\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import torch\n",
    "import shutil  # 파일 복사를 위한 모듈\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "\n",
    "IPS = 256\n",
    "CH = 8\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, IPS, IPS).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(IPS)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((IPS, IPS))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=self.meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < IPS or target.shape[-2] < IPS:\n",
    "            target = torchvision.transforms.Resize(IPS)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        # 데이터와 파일 경로를 함께 반환\n",
    "        return target, self.target_list[idx]\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "        # 관찰 공간: (4, 채널, 픽셀, 픽셀)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4, CH, IPS, IPS), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: 픽셀 하나를 선택하는 인덱스 (채널 * 픽셀 *픽셀)\n",
    "        self.num_pixels = CH * IPS * IPS\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        # 타겟 함수와 데이터 로더 설정\n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # 환경 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  # 추가된 속성\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 데이터 로더에서 첫 배치 설정\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "        # 연속 실패 관련 변수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "        self.flip_count = 0\n",
    "\n",
    "        # PSNR 저장 변수\n",
    "        self.previous_psnr = None\n",
    "\n",
    "        self.result_before = None\n",
    "        self.target_image_np = None\n",
    "\n",
    "        self.episode_num_count = 0\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.episode_num_count += 1  # Increment episode count at the start of each reset\n",
    "\n",
    "        # 이터레이터에서 다음 데이터를 가져옴\n",
    "        try:\n",
    "            self.target_image, self.current_file = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            # 데이터셋 끝에 도달하면 이터레이터를 다시 생성하고 처음부터 다시 시작\n",
    "            print(\"\\033[40;93m[INFO] Reached the end of dataset. Restarting from the beginning.\\033[0m\")\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image, self.current_file = next(self.data_iter)\n",
    "\n",
    "        print(f\"\\033[40;93m[Episode Start] Currently using dataset file: {self.current_file}, Episode count: {self.episode_num_count}\\033[0m\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "\n",
    "        # 연속 실패 처리\n",
    "        #if self.retry_current_target:  # 이전 에피소드에서 실패한 경우\n",
    "        #    self.consecutive_fail_count += 1\n",
    "        #else:\n",
    "        #    self.consecutive_fail_count = 0  # 성공적인 에피소드로 연속 실패 초기화\n",
    "\n",
    "        # 최대 연속 실패 기록 갱신\n",
    "        #self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        # 실패 플래그에 따라 데이터 유지 또는 새 데이터 로드\n",
    "        #if not self.retry_current_target:\n",
    "        #    try:\n",
    "        #        self.target_image = next(self.data_iter)\n",
    "        #    except StopIteration:\n",
    "        #        print(\"\\033[93m[INFO] Reached the end of dataset. Restarting from the beginning.\\033[0m\")\n",
    "        #        self.data_iter = iter(self.trainloader)\n",
    "        #        self.target_image = next(self.data_iter)\n",
    "\n",
    "        # 타겟 이미지 형식 출력\n",
    "        #print(f\"[DEBUG]Target image shape: {self.target_image.shape}, dtype: {self.target_image.dtype}\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        # 매 에피소드마다 초기화\n",
    "        self.max_psnr_diff = float('-inf')\n",
    "        self.steps = 0\n",
    "        self.flip_count = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # Ensure observation shape is (채널, 픽셀, 픽셀)\n",
    "        self.observation = model_output.squeeze(0).cpu().numpy()  # (채널, 픽셀, 픽셀)\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # Binary state\n",
    "\n",
    "        # 시뮬레이션 전 binary 형상을 (1, 채널, 픽셀, 픽셀)로 복원\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()  # (1, 채널, 픽셀, 픽셀)\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "        sim_image = result.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "        self.previous_psnr = self.initial_psnr # 초기 PSNR 저장\n",
    "\n",
    "        # target_image_np와 result를 채널 차원(CH=8)으로 확장\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "        result_np = np.repeat(result.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "\n",
    "        self.result_before = result_np\n",
    "        self.target_image_np = target_image_np\n",
    "        #plt.imshow(target_image_np[1], cmap='gray')\n",
    "        #plt.title(f\"Target Image {self.current_file}\")\n",
    "        #plt.colorbar()\n",
    "        #plt.show()\n",
    "        #plt.imshow(result_np[1], cmap='gray')\n",
    "        #plt.title(f\"Simulation Result {self.current_file}\")\n",
    "        #plt.colorbar()\n",
    "        #plt.show()\n",
    "\n",
    "        # 모든 관찰값을 스택으로 결합\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )  # 최종 모양: [4, CH, IPS, IPS]\n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\033[92mInitial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\\033[0m\")\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "\n",
    "        return combined_observation, {\"state\": self.state}\n",
    "\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 행동 전 PSNR 계산\n",
    "        psnr_before = self.previous_psnr\n",
    "\n",
    "        # 행동을 기반으로 픽셀 좌표 계산Inv\n",
    "        channel = action // (IPS * IPS)\n",
    "        pixel_index = action % (IPS * IPS)\n",
    "        row = pixel_index // IPS\n",
    "        col = pixel_index % IPS\n",
    "\n",
    "        # 플립 전 모델 예측값 가져오기\n",
    "        pre_flip_value = self.observation[channel, row, col]\n",
    "\n",
    "        # 상태 변경\n",
    "        self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "        self.flip_count += 1  # 플립 증가\n",
    "\n",
    "        # 현재 상태로 새로운 시뮬레이션 수행\n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 시뮬레이션 결과를 NumPy로 변환\n",
    "        result_np = np.repeat(result_after.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        # Combined observation 생성 후 출력\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )\n",
    "\n",
    "        # PSNR 변화량 계산\n",
    "        psnr_change = psnr_after - psnr_before\n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "\n",
    "        # psnr_change가 음수인 경우 상태 롤백 수행\n",
    "        if psnr_change < 0:\n",
    "\n",
    "            failed_observation = combined_observation\n",
    "\n",
    "            failed_action = action\n",
    "            failed_reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "            # 이전 스텝의 누적 보상을 안전하게 초기화\n",
    "            previous_cumulative_reward = 0\n",
    "            if \"info\" in locals() and isinstance(info, dict):\n",
    "                previous_cumulative_reward = info.get(\"cumulative_reward\", 0)\n",
    "\n",
    "            # 플립된 픽셀을 원래대로 복구\n",
    "            self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "            self.flip_count -= 1\n",
    "\n",
    "            # 출력 추가 (100 스텝마다 출력)\n",
    "            if self.steps % 100 == 0:\n",
    "                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                print(\n",
    "                    f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                    f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                    f\"Reward: {failed_reward:.2f}, {current_time} \"\n",
    "                    f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                    f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                    f\"Flip Count={self.flip_count}, \"\n",
    "                    f\"Flip Pixcel={channel} * {row} * {col}\"\n",
    "                )\n",
    "\n",
    "            # 출력 추가 (1000 스텝마다 출력)\n",
    "            #if self.steps % 1000 == 0:\n",
    "                # Simulate 결과 시각화\n",
    "                #plt.imshow(result_np[1], cmap='gray')\n",
    "                #plt.title(f\"Simulation Result {self.steps}\")\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "\n",
    "            # 스텝 증가\n",
    "            self.steps += 1\n",
    "\n",
    "            # 실패 정보 생성\n",
    "            info = {\n",
    "                \"psnr_before\": psnr_before,\n",
    "                \"psnr_after\": psnr_after,\n",
    "                \"psnr_change\": psnr_change,\n",
    "                \"psnr_diff\": psnr_diff,\n",
    "                \"pre_flip_value\": pre_flip_value,\n",
    "                \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "                \"state_after\": None,  # 실패한 경우에는 상태를 업데이트하지 않음\n",
    "                \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "                \"observation_after\": None,  # 실패한 경우 관찰값 업데이트 없음\n",
    "                \"failed_action\": failed_action,  # 실패한 행동\n",
    "                \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "                \"reward\": failed_reward,\n",
    "                \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "                \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "                \"step\": self.steps  # 현재 스텝\n",
    "            }\n",
    "            return failed_observation, failed_reward, False, False, info\n",
    "\n",
    "        # 보상 계산\n",
    "        reward = psnr_change * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}, \"\n",
    "                f\"Flip Pixcel={channel} * {row} * {col}\"\n",
    "            )\n",
    "\n",
    "        # 출력 추가 (1000 스텝마다 출력)\n",
    "        #if self.steps % 1000 == 0:\n",
    "            # Simulate 결과 시각화\n",
    "            #plt.imshow(result_np[1], cmap='gray')\n",
    "            #plt.title(f\"Simulation Result {self.steps}\")\n",
    "            #plt.colorbar()\n",
    "            #plt.show()\n",
    "\n",
    "        self.previous_psnr = psnr_after\n",
    "\n",
    "        # 성공 종료 조건: PSNR >= T_PSNR 또는 PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}, \"\n",
    "                f\"Flip Pixcel={channel} * {row} * {col}\\033[0m\"\n",
    "            )\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  # 성공 에피소드 조건\n",
    "                # Simulate 결과 시각화\n",
    "                #plt.imshow(result_np[1], cmap='gray')\n",
    "                #plt.title(f\"Simulation Result sucess\")\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "                #result_before = self.result_before\n",
    "                #plt.imshow(result_before[1], cmap='gray')\n",
    "                #plt.title(f\"Simulation Result {self.current_file}\")\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "                #target_image_np = self.target_image_np\n",
    "                #plt.imshow(target_image_np[1], cmap='gray')\n",
    "                #plt.title(f\"Target Image {self.current_file}\")\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "\n",
    "                if self.steps <= 0:\n",
    "                    reward += 200\n",
    "                elif self.steps >= 4000:\n",
    "                    reward -= 200\n",
    "                else:\n",
    "                    # 0 < steps < 4000일 때 비례 보상 계산\n",
    "                    proportion = (4000 - self.steps) / 4000\n",
    "                    additional_reward = 200 * (2 * proportion - 1)  # 비례 보상 계산 (-200에서 +200 사이)\n",
    "                    reward += additional_reward\n",
    "\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"pre_flip_value\": pre_flip_value,\n",
    "            \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "            \"state_after\": self.state.copy() if psnr_change >= 0 else None,  # 행동 성공 시 상태\n",
    "            \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "            \"observation_after\": combined_observation if psnr_change >= 0 else None,  # 행동 성공 시 관찰값\n",
    "            \"failed_action\": action if psnr_change < 0 else None,  # 실패한 행동\n",
    "            \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "            \"reward\": reward,\n",
    "            \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "            \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "            \"action_coords\": (channel, row, col),  # 행동한 좌표\n",
    "            \"step\": self.steps  # 현재 스텝\n",
    "        }\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        return combined_observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = 'dataset/'\n",
    "#target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "#train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding) #랜덤크롭\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=False, padding=padding) #센터크롭\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    #max_steps = 10000,\n",
    "    #T_PSNR = 30,\n",
    "    #T_steps = 10\n",
    "    #T_PSNR_DIFF = 0.1\n",
    ")\n",
    "\n",
    "# 에피소드 보상 로깅 콜백\n",
    "class RewardLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(RewardLoggingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []  # 각 에피소드 보상을 저장\n",
    "        self.current_episode_reward = 0  # 현재 에피소드의 보상\n",
    "        self.episode_count = 0  # 에피소드 수를 추적\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 현재 스텝의 보상을 누적\n",
    "        reward = self.locals[\"rewards\"]\n",
    "        self.current_episode_reward += reward[0]  # 첫 번째 환경의 보상\n",
    "\n",
    "        # 에피소드 종료 처리\n",
    "        if self.locals[\"dones\"][0]:  # 첫 번째 환경에서 에피소드 종료 시\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_count += 1\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(f\"\\033[41mEpisode {self.episode_count}: Total Reward: {self.current_episode_reward:.2f}\\033[0m\")\n",
    "\n",
    "            # 현재 에피소드 보상을 초기화\n",
    "            self.current_episode_reward = 0\n",
    "\n",
    "        return True  # 학습 계속\n",
    "\n",
    "# 학습 종료 콜백\n",
    "class StopOnEpisodeCallback(BaseCallback):\n",
    "    def __init__(self, max_episodes, verbose=1):\n",
    "        super(StopOnEpisodeCallback, self).__init__(verbose)\n",
    "        self.max_episodes = max_episodes\n",
    "        self.episode_count = 0  # 에피소드 수를 추적\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # `dones`이 True일 때마다 에피소드 증가\n",
    "        if self.locals.get(\"dones\") is not None:\n",
    "            self.episode_count += np.sum(self.locals[\"dones\"])  # 에피소드 완료 횟수 추가\n",
    "\n",
    "        if self.episode_count >= self.max_episodes:  # 최대 에피소드 도달 시 학습 종료\n",
    "            print(f\"Stopping training at episode {self.episode_count}\")\n",
    "            return False  # 학습 중단\n",
    "        return True  # 학습 계속\n",
    "\n",
    "# 저장할 폴더 경로 설정\n",
    "save_dir = \"./ppo_MlpPolicy_models/\"  # 모델 저장 디렉토리\n",
    "os.makedirs(save_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# PPO 모델과 PSNRPredictor 경로 설정\n",
    "ppo_model_path = os.path.join(save_dir, \"0_ppo_MlpPolicy_latest.zip\")  # 최신 PPO 모델 저장 경로\n",
    "resume_training = True  # True로 설정하면 이전 모델에서 학습 재개\n",
    "\n",
    "# PPO 모델 로드 또는 새로 생성\n",
    "if resume_training and os.path.exists(ppo_model_path):\n",
    "    print(f\"Loading trained PPO model from {ppo_model_path}\")\n",
    "    ppo_model = PPO.load(ppo_model_path, env=env)\n",
    "else:\n",
    "    if resume_training:\n",
    "        print(f\"Warning: PPO model not found at {ppo_model_path}. Starting training from scratch.\")\n",
    "    print(\"Starting training from scratch.\")\n",
    "    ppo_model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=2,\n",
    "        n_steps=512,\n",
    "        batch_size=128,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.9,\n",
    "        learning_rate=1e-4,\n",
    "        clip_range=0.2,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        ent_coef=0.01,\n",
    "        tensorboard_log=\"./ppo_MlpPolicy/\"\n",
    "    )\n",
    "\n",
    "# 콜백 설정\n",
    "max_episodes = 2000  # 원하는 에피소드 수\n",
    "reward_logging_callback = RewardLoggingCallback(verbose=1)\n",
    "stop_callback = StopOnEpisodeCallback(max_episodes=max_episodes)\n",
    "callback = CallbackList([reward_logging_callback, stop_callback])\n",
    "\n",
    "# 학습 시작\n",
    "ppo_model.learn(total_timesteps=1000000000, callback=callback)\n",
    "\n",
    "# 모델 저장\n",
    "print(f\"Start model saving at {save_dir}\")\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "ppo_model_save_path = os.path.join(save_dir, f\"0_ppo_MlpPolicy_{current_date}.zip\")\n",
    "ppo_model.save(ppo_model_save_path)\n",
    "print(f\"PPO Model saved at {save_dir}\")\n",
    "\n",
    "# 최신 모델 업데이트\n",
    "print(f\"Start model updating at {save_dir}\")\n",
    "ppo_model_latest_path = os.path.join(save_dir, \"0_ppo_MlpPolicy_latest.zip\")\n",
    "\n",
    "# 최신 모델을 덮어쓰기 위해 기존 모델 파일 복사\n",
    "if os.path.exists(ppo_model_latest_path):\n",
    "    os.remove(ppo_model_latest_path)  # 기존 파일 삭제\n",
    "shutil.copyfile(ppo_model_save_path, ppo_model_latest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
