{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024, 1024])\n",
      "Using cuda device\n",
      "Initial MSE: 0.007286, Initial PSNR: 21.375124, 2024-12-20_08-45-57\n",
      "Logging to ./ppo_with_mask/PPO_38\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.007286, Initial PSNR: 21.375124, 2024-12-20_08-45-57\n",
      "Step: 1, MSE: 0.072330, PSNR: 11.406812, 2024-12-20_08-45-57\n",
      "Step: 2, MSE: 0.072434, PSNR: 11.400599, 2024-12-20_08-45-57\n",
      "Step: 3, MSE: 0.072388, PSNR: 11.403346, 2024-12-20_08-45-57\n",
      "Step: 4, MSE: 0.072374, PSNR: 11.404180, 2024-12-20_08-45-57\n",
      "Step: 5, MSE: 0.072297, PSNR: 11.408776, 2024-12-20_08-45-57\n",
      "Step: 6, MSE: 0.072249, PSNR: 11.411703, 2024-12-20_08-45-57\n",
      "Step: 7, MSE: 0.072407, PSNR: 11.402176, 2024-12-20_08-45-57\n",
      "Step: 8, MSE: 0.072251, PSNR: 11.411582, 2024-12-20_08-45-57\n",
      "Step: 9, MSE: 0.072256, PSNR: 11.411290, 2024-12-20_08-45-57\n",
      "Step: 10, MSE: 0.072414, PSNR: 11.401779, 2024-12-20_08-45-57\n",
      "Step: 11, MSE: 0.072348, PSNR: 11.405712, 2024-12-20_08-45-57\n",
      "Step: 12, MSE: 0.072352, PSNR: 11.405501, 2024-12-20_08-45-57\n",
      "Step: 13, MSE: 0.072220, PSNR: 11.413450, 2024-12-20_08-45-57\n",
      "Step: 14, MSE: 0.072372, PSNR: 11.404267, 2024-12-20_08-45-57\n",
      "Step: 15, MSE: 0.072439, PSNR: 11.400276, 2024-12-20_08-45-57\n",
      "Step: 16, MSE: 0.072347, PSNR: 11.405822, 2024-12-20_08-45-57\n",
      "Step: 17, MSE: 0.072216, PSNR: 11.413685, 2024-12-20_08-45-57\n",
      "Step: 18, MSE: 0.072224, PSNR: 11.413170, 2024-12-20_08-45-57\n",
      "Step: 19, MSE: 0.072420, PSNR: 11.401432, 2024-12-20_08-45-57\n",
      "Step: 20, MSE: 0.072363, PSNR: 11.404810, 2024-12-20_08-45-57\n",
      "Step: 21, MSE: 0.072340, PSNR: 11.406219, 2024-12-20_08-45-57\n",
      "Step: 22, MSE: 0.072303, PSNR: 11.408407, 2024-12-20_08-45-57\n",
      "Step: 23, MSE: 0.072356, PSNR: 11.405277, 2024-12-20_08-45-57\n",
      "Step: 24, MSE: 0.072248, PSNR: 11.411752, 2024-12-20_08-45-57\n",
      "Step: 25, MSE: 0.072377, PSNR: 11.404008, 2024-12-20_08-45-57\n",
      "Step: 26, MSE: 0.072397, PSNR: 11.402781, 2024-12-20_08-45-57\n",
      "Step: 27, MSE: 0.072312, PSNR: 11.407906, 2024-12-20_08-45-57\n",
      "Step: 28, MSE: 0.072329, PSNR: 11.406879, 2024-12-20_08-45-57\n",
      "Step: 29, MSE: 0.072357, PSNR: 11.405202, 2024-12-20_08-45-57\n",
      "Step: 30, MSE: 0.072394, PSNR: 11.402955, 2024-12-20_08-45-57\n",
      "Step: 31, MSE: 0.072297, PSNR: 11.408789, 2024-12-20_08-45-57\n",
      "Step: 32, MSE: 0.072330, PSNR: 11.406822, 2024-12-20_08-45-57\n",
      "Step: 33, MSE: 0.072347, PSNR: 11.405811, 2024-12-20_08-45-57\n",
      "Step: 34, MSE: 0.072379, PSNR: 11.403887, 2024-12-20_08-45-57\n",
      "Step: 35, MSE: 0.072401, PSNR: 11.402582, 2024-12-20_08-45-57\n",
      "Step: 36, MSE: 0.072205, PSNR: 11.414327, 2024-12-20_08-45-57\n",
      "Step: 37, MSE: 0.072369, PSNR: 11.404453, 2024-12-20_08-45-57\n",
      "Step: 38, MSE: 0.072324, PSNR: 11.407196, 2024-12-20_08-45-57\n",
      "Step: 39, MSE: 0.072340, PSNR: 11.406197, 2024-12-20_08-45-57\n",
      "Step: 40, MSE: 0.072360, PSNR: 11.404997, 2024-12-20_08-45-57\n",
      "Step: 41, MSE: 0.072276, PSNR: 11.410064, 2024-12-20_08-45-57\n",
      "Step: 42, MSE: 0.072311, PSNR: 11.407980, 2024-12-20_08-45-57\n",
      "Step: 43, MSE: 0.072222, PSNR: 11.413294, 2024-12-20_08-45-57\n",
      "Step: 44, MSE: 0.072269, PSNR: 11.410450, 2024-12-20_08-45-57\n",
      "Step: 45, MSE: 0.072276, PSNR: 11.410053, 2024-12-20_08-45-57\n",
      "Step: 46, MSE: 0.072250, PSNR: 11.411621, 2024-12-20_08-45-57\n",
      "Step: 47, MSE: 0.072396, PSNR: 11.402857, 2024-12-20_08-45-57\n",
      "Step: 48, MSE: 0.072315, PSNR: 11.407703, 2024-12-20_08-45-57\n",
      "Step: 49, MSE: 0.072323, PSNR: 11.407221, 2024-12-20_08-45-57\n",
      "Step: 50, MSE: 0.072407, PSNR: 11.402207, 2024-12-20_08-45-57\n",
      "Step: 51, MSE: 0.072345, PSNR: 11.405897, 2024-12-20_08-45-57\n",
      "Step: 52, MSE: 0.072321, PSNR: 11.407367, 2024-12-20_08-45-57\n",
      "Step: 53, MSE: 0.072335, PSNR: 11.406512, 2024-12-20_08-45-57\n",
      "Step: 54, MSE: 0.072381, PSNR: 11.403775, 2024-12-20_08-45-57\n",
      "Step: 55, MSE: 0.072227, PSNR: 11.413022, 2024-12-20_08-45-57\n",
      "Step: 56, MSE: 0.072281, PSNR: 11.409731, 2024-12-20_08-45-57\n",
      "Step: 57, MSE: 0.072309, PSNR: 11.408096, 2024-12-20_08-45-57\n",
      "Step: 58, MSE: 0.072337, PSNR: 11.406375, 2024-12-20_08-45-57\n",
      "Step: 59, MSE: 0.072216, PSNR: 11.413692, 2024-12-20_08-45-57\n",
      "Step: 60, MSE: 0.072300, PSNR: 11.408609, 2024-12-20_08-45-57\n",
      "Step: 61, MSE: 0.072412, PSNR: 11.401901, 2024-12-20_08-45-57\n",
      "Step: 62, MSE: 0.072383, PSNR: 11.403639, 2024-12-20_08-45-57\n",
      "Step: 63, MSE: 0.072327, PSNR: 11.407017, 2024-12-20_08-45-57\n",
      "Step: 64, MSE: 0.072334, PSNR: 11.406591, 2024-12-20_08-45-57\n",
      "Step: 65, MSE: 0.072249, PSNR: 11.411655, 2024-12-20_08-45-57\n",
      "Step: 66, MSE: 0.072367, PSNR: 11.404612, 2024-12-20_08-45-57\n",
      "Step: 67, MSE: 0.072347, PSNR: 11.405813, 2024-12-20_08-45-57\n",
      "Step: 68, MSE: 0.072274, PSNR: 11.410189, 2024-12-20_08-45-57\n",
      "Step: 69, MSE: 0.072236, PSNR: 11.412435, 2024-12-20_08-45-57\n",
      "Step: 70, MSE: 0.072284, PSNR: 11.409559, 2024-12-20_08-45-57\n",
      "Step: 71, MSE: 0.072332, PSNR: 11.406684, 2024-12-20_08-45-57\n",
      "Step: 72, MSE: 0.072477, PSNR: 11.398026, 2024-12-20_08-45-57\n",
      "Step: 73, MSE: 0.072337, PSNR: 11.406380, 2024-12-20_08-45-57\n",
      "Step: 74, MSE: 0.072300, PSNR: 11.408590, 2024-12-20_08-45-57\n",
      "Step: 75, MSE: 0.072282, PSNR: 11.409700, 2024-12-20_08-45-57\n",
      "Step: 76, MSE: 0.072311, PSNR: 11.407932, 2024-12-20_08-45-57\n",
      "Step: 77, MSE: 0.072330, PSNR: 11.406807, 2024-12-20_08-45-57\n",
      "Step: 78, MSE: 0.072370, PSNR: 11.404392, 2024-12-20_08-45-57\n",
      "Step: 79, MSE: 0.072184, PSNR: 11.415614, 2024-12-20_08-45-57\n",
      "Step: 80, MSE: 0.072309, PSNR: 11.408061, 2024-12-20_08-45-57\n",
      "Step: 81, MSE: 0.072529, PSNR: 11.394893, 2024-12-20_08-45-57\n",
      "Step: 82, MSE: 0.072322, PSNR: 11.407295, 2024-12-20_08-45-57\n",
      "Step: 83, MSE: 0.072345, PSNR: 11.405930, 2024-12-20_08-45-57\n",
      "Step: 84, MSE: 0.072316, PSNR: 11.407652, 2024-12-20_08-45-57\n",
      "Step: 85, MSE: 0.072164, PSNR: 11.416821, 2024-12-20_08-45-57\n",
      "Step: 86, MSE: 0.072335, PSNR: 11.406540, 2024-12-20_08-45-57\n",
      "Step: 87, MSE: 0.072430, PSNR: 11.400789, 2024-12-20_08-45-57\n",
      "Step: 88, MSE: 0.072310, PSNR: 11.408006, 2024-12-20_08-45-57\n",
      "Step: 89, MSE: 0.072319, PSNR: 11.407457, 2024-12-20_08-45-57\n",
      "Step: 90, MSE: 0.072229, PSNR: 11.412911, 2024-12-20_08-45-57\n",
      "Step: 91, MSE: 0.072364, PSNR: 11.404752, 2024-12-20_08-45-57\n",
      "Step: 92, MSE: 0.072335, PSNR: 11.406537, 2024-12-20_08-45-57\n",
      "Step: 93, MSE: 0.072324, PSNR: 11.407171, 2024-12-20_08-45-57\n",
      "Step: 94, MSE: 0.072385, PSNR: 11.403522, 2024-12-20_08-45-57\n",
      "Step: 95, MSE: 0.072311, PSNR: 11.407951, 2024-12-20_08-45-57\n",
      "Step: 96, MSE: 0.072395, PSNR: 11.402887, 2024-12-20_08-45-57\n",
      "Step: 97, MSE: 0.072364, PSNR: 11.404744, 2024-12-20_08-45-57\n",
      "Step: 98, MSE: 0.072334, PSNR: 11.406576, 2024-12-20_08-45-57\n",
      "Step: 99, MSE: 0.072290, PSNR: 11.409221, 2024-12-20_08-45-57\n",
      "Step: 100, MSE: 0.072305, PSNR: 11.408331, 2024-12-20_08-45-57\n",
      "Step: 101, MSE: 0.072398, PSNR: 11.402760, 2024-12-20_08-45-57\n",
      "Step: 102, MSE: 0.072245, PSNR: 11.411901, 2024-12-20_08-45-57\n",
      "Step: 103, MSE: 0.072479, PSNR: 11.397887, 2024-12-20_08-45-57\n",
      "Step: 104, MSE: 0.072322, PSNR: 11.407278, 2024-12-20_08-45-57\n",
      "Step: 105, MSE: 0.072416, PSNR: 11.401643, 2024-12-20_08-45-57\n",
      "Step: 106, MSE: 0.072248, PSNR: 11.411770, 2024-12-20_08-45-57\n",
      "Step: 107, MSE: 0.072277, PSNR: 11.410021, 2024-12-20_08-45-57\n",
      "Step: 108, MSE: 0.072372, PSNR: 11.404276, 2024-12-20_08-45-57\n",
      "Step: 109, MSE: 0.072275, PSNR: 11.410107, 2024-12-20_08-45-57\n",
      "Step: 110, MSE: 0.072170, PSNR: 11.416426, 2024-12-20_08-45-57\n",
      "Step: 111, MSE: 0.072356, PSNR: 11.405283, 2024-12-20_08-45-57\n",
      "Step: 112, MSE: 0.072420, PSNR: 11.401392, 2024-12-20_08-45-57\n",
      "Step: 113, MSE: 0.072227, PSNR: 11.413017, 2024-12-20_08-45-57\n",
      "Step: 114, MSE: 0.072302, PSNR: 11.408502, 2024-12-20_08-45-57\n",
      "Step: 115, MSE: 0.072274, PSNR: 11.410200, 2024-12-20_08-45-57\n",
      "Step: 116, MSE: 0.072311, PSNR: 11.407971, 2024-12-20_08-45-57\n",
      "Step: 117, MSE: 0.072348, PSNR: 11.405734, 2024-12-20_08-45-57\n",
      "Step: 118, MSE: 0.072439, PSNR: 11.400253, 2024-12-20_08-45-57\n",
      "Step: 119, MSE: 0.072276, PSNR: 11.410085, 2024-12-20_08-45-57\n",
      "Step: 120, MSE: 0.072270, PSNR: 11.410440, 2024-12-20_08-45-57\n",
      "Step: 121, MSE: 0.072327, PSNR: 11.406996, 2024-12-20_08-45-57\n",
      "Step: 122, MSE: 0.072378, PSNR: 11.403909, 2024-12-20_08-45-57\n",
      "Step: 123, MSE: 0.072475, PSNR: 11.398124, 2024-12-20_08-45-57\n",
      "Step: 124, MSE: 0.072280, PSNR: 11.409800, 2024-12-20_08-45-57\n",
      "Step: 125, MSE: 0.072298, PSNR: 11.408739, 2024-12-20_08-45-57\n",
      "Step: 126, MSE: 0.072352, PSNR: 11.405520, 2024-12-20_08-45-57\n",
      "Step: 127, MSE: 0.072258, PSNR: 11.411160, 2024-12-20_08-45-57\n",
      "Step: 128, MSE: 0.072301, PSNR: 11.408550, 2024-12-20_08-45-57\n",
      "Step: 129, MSE: 0.072307, PSNR: 11.408172, 2024-12-20_08-45-57\n",
      "Step: 130, MSE: 0.072298, PSNR: 11.408767, 2024-12-20_08-45-57\n",
      "Step: 131, MSE: 0.072411, PSNR: 11.401936, 2024-12-20_08-45-57\n",
      "Step: 132, MSE: 0.072298, PSNR: 11.408739, 2024-12-20_08-45-57\n",
      "Step: 133, MSE: 0.072239, PSNR: 11.412287, 2024-12-20_08-45-57\n",
      "Step: 134, MSE: 0.072332, PSNR: 11.406702, 2024-12-20_08-45-57\n",
      "Step: 135, MSE: 0.072281, PSNR: 11.409746, 2024-12-20_08-45-57\n",
      "Step: 136, MSE: 0.072235, PSNR: 11.412539, 2024-12-20_08-45-57\n",
      "Step: 137, MSE: 0.072349, PSNR: 11.405677, 2024-12-20_08-45-57\n",
      "Step: 138, MSE: 0.072396, PSNR: 11.402855, 2024-12-20_08-45-57\n",
      "Step: 139, MSE: 0.072305, PSNR: 11.408308, 2024-12-20_08-45-57\n",
      "Step: 140, MSE: 0.072321, PSNR: 11.407327, 2024-12-20_08-45-57\n",
      "Step: 141, MSE: 0.072347, PSNR: 11.405811, 2024-12-20_08-45-57\n",
      "Step: 142, MSE: 0.072260, PSNR: 11.411010, 2024-12-20_08-45-57\n",
      "Step: 143, MSE: 0.072259, PSNR: 11.411056, 2024-12-20_08-45-57\n",
      "Step: 144, MSE: 0.072308, PSNR: 11.408114, 2024-12-20_08-45-57\n",
      "Step: 145, MSE: 0.072262, PSNR: 11.410922, 2024-12-20_08-45-57\n",
      "Step: 146, MSE: 0.072435, PSNR: 11.400514, 2024-12-20_08-45-57\n",
      "Step: 147, MSE: 0.072391, PSNR: 11.403145, 2024-12-20_08-45-57\n",
      "Step: 148, MSE: 0.072324, PSNR: 11.407147, 2024-12-20_08-45-57\n",
      "Step: 149, MSE: 0.072293, PSNR: 11.409036, 2024-12-20_08-45-57\n",
      "Step: 150, MSE: 0.072216, PSNR: 11.413689, 2024-12-20_08-45-57\n",
      "Step: 151, MSE: 0.072369, PSNR: 11.404486, 2024-12-20_08-45-57\n",
      "Step: 152, MSE: 0.072229, PSNR: 11.412884, 2024-12-20_08-45-57\n",
      "Step: 153, MSE: 0.072414, PSNR: 11.401792, 2024-12-20_08-45-57\n",
      "Step: 154, MSE: 0.072391, PSNR: 11.403183, 2024-12-20_08-45-57\n",
      "Step: 155, MSE: 0.072239, PSNR: 11.412304, 2024-12-20_08-45-57\n",
      "Step: 156, MSE: 0.072379, PSNR: 11.403845, 2024-12-20_08-45-57\n",
      "Step: 157, MSE: 0.072376, PSNR: 11.404045, 2024-12-20_08-45-57\n",
      "Step: 158, MSE: 0.072226, PSNR: 11.413054, 2024-12-20_08-45-57\n",
      "Step: 159, MSE: 0.072358, PSNR: 11.405110, 2024-12-20_08-45-57\n",
      "Step: 160, MSE: 0.072332, PSNR: 11.406708, 2024-12-20_08-45-57\n",
      "Step: 161, MSE: 0.072330, PSNR: 11.406813, 2024-12-20_08-45-57\n",
      "Step: 162, MSE: 0.072231, PSNR: 11.412739, 2024-12-20_08-45-57\n",
      "Step: 163, MSE: 0.072266, PSNR: 11.410686, 2024-12-20_08-45-57\n",
      "Step: 164, MSE: 0.072323, PSNR: 11.407207, 2024-12-20_08-45-57\n",
      "Step: 165, MSE: 0.072451, PSNR: 11.399573, 2024-12-20_08-45-57\n",
      "Step: 166, MSE: 0.072347, PSNR: 11.405788, 2024-12-20_08-45-57\n",
      "Step: 167, MSE: 0.072315, PSNR: 11.407691, 2024-12-20_08-45-57\n",
      "Step: 168, MSE: 0.072351, PSNR: 11.405529, 2024-12-20_08-45-57\n",
      "Step: 169, MSE: 0.072316, PSNR: 11.407632, 2024-12-20_08-45-57\n",
      "Step: 170, MSE: 0.072319, PSNR: 11.407480, 2024-12-20_08-45-57\n",
      "Step: 171, MSE: 0.072414, PSNR: 11.401788, 2024-12-20_08-45-57\n",
      "Step: 172, MSE: 0.072281, PSNR: 11.409774, 2024-12-20_08-45-57\n",
      "Step: 173, MSE: 0.072352, PSNR: 11.405495, 2024-12-20_08-45-57\n",
      "Step: 174, MSE: 0.072375, PSNR: 11.404127, 2024-12-20_08-45-57\n",
      "Step: 175, MSE: 0.072317, PSNR: 11.407597, 2024-12-20_08-45-57\n",
      "Step: 176, MSE: 0.072243, PSNR: 11.412056, 2024-12-20_08-45-57\n",
      "Step: 177, MSE: 0.072351, PSNR: 11.405571, 2024-12-20_08-45-57\n",
      "Step: 178, MSE: 0.072337, PSNR: 11.406418, 2024-12-20_08-45-57\n",
      "Step: 179, MSE: 0.072231, PSNR: 11.412783, 2024-12-20_08-45-57\n",
      "Step: 180, MSE: 0.072241, PSNR: 11.412189, 2024-12-20_08-45-57\n",
      "Step: 181, MSE: 0.072333, PSNR: 11.406652, 2024-12-20_08-45-57\n",
      "Step: 182, MSE: 0.072217, PSNR: 11.413612, 2024-12-20_08-45-57\n",
      "Step: 183, MSE: 0.072364, PSNR: 11.404799, 2024-12-20_08-45-57\n",
      "Step: 184, MSE: 0.072209, PSNR: 11.414101, 2024-12-20_08-45-57\n",
      "Step: 185, MSE: 0.072443, PSNR: 11.400033, 2024-12-20_08-45-57\n",
      "Step: 186, MSE: 0.072350, PSNR: 11.405606, 2024-12-20_08-45-57\n",
      "Step: 187, MSE: 0.072166, PSNR: 11.416659, 2024-12-20_08-45-57\n",
      "Step: 188, MSE: 0.072355, PSNR: 11.405344, 2024-12-20_08-45-57\n",
      "Step: 189, MSE: 0.072248, PSNR: 11.411742, 2024-12-20_08-45-57\n",
      "Step: 190, MSE: 0.072282, PSNR: 11.409704, 2024-12-20_08-45-57\n",
      "Step: 191, MSE: 0.072335, PSNR: 11.406487, 2024-12-20_08-45-57\n",
      "Step: 192, MSE: 0.072304, PSNR: 11.408404, 2024-12-20_08-45-57\n",
      "Step: 193, MSE: 0.072240, PSNR: 11.412247, 2024-12-20_08-45-57\n",
      "Step: 194, MSE: 0.072256, PSNR: 11.411274, 2024-12-20_08-45-57\n",
      "Step: 195, MSE: 0.072481, PSNR: 11.397748, 2024-12-20_08-45-57\n",
      "Step: 196, MSE: 0.072417, PSNR: 11.401591, 2024-12-20_08-45-57\n",
      "Step: 197, MSE: 0.072319, PSNR: 11.407447, 2024-12-20_08-45-57\n",
      "Step: 198, MSE: 0.072273, PSNR: 11.410213, 2024-12-20_08-45-57\n",
      "Step: 199, MSE: 0.072332, PSNR: 11.406668, 2024-12-20_08-45-57\n",
      "Step: 200, MSE: 0.072459, PSNR: 11.399055, 2024-12-20_08-45-57\n",
      "Step: 201, MSE: 0.072303, PSNR: 11.408409, 2024-12-20_08-45-57\n",
      "Step: 202, MSE: 0.072431, PSNR: 11.400749, 2024-12-20_08-45-57\n",
      "Step: 203, MSE: 0.072207, PSNR: 11.414222, 2024-12-20_08-45-57\n",
      "Step: 204, MSE: 0.072261, PSNR: 11.410976, 2024-12-20_08-45-57\n",
      "Step: 205, MSE: 0.072300, PSNR: 11.408628, 2024-12-20_08-45-57\n",
      "Step: 206, MSE: 0.072365, PSNR: 11.404705, 2024-12-20_08-45-57\n",
      "Step: 207, MSE: 0.072208, PSNR: 11.414152, 2024-12-20_08-45-57\n",
      "Step: 208, MSE: 0.072342, PSNR: 11.406106, 2024-12-20_08-45-57\n",
      "Step: 209, MSE: 0.072394, PSNR: 11.402987, 2024-12-20_08-45-57\n",
      "Step: 210, MSE: 0.072264, PSNR: 11.410801, 2024-12-20_08-45-57\n",
      "Step: 211, MSE: 0.072327, PSNR: 11.406982, 2024-12-20_08-45-57\n",
      "Step: 212, MSE: 0.072250, PSNR: 11.411631, 2024-12-20_08-45-57\n",
      "Step: 213, MSE: 0.072320, PSNR: 11.407394, 2024-12-20_08-45-57\n",
      "Step: 214, MSE: 0.072413, PSNR: 11.401816, 2024-12-20_08-45-57\n",
      "Step: 215, MSE: 0.072281, PSNR: 11.409764, 2024-12-20_08-45-57\n",
      "Step: 216, MSE: 0.072374, PSNR: 11.404151, 2024-12-20_08-45-57\n",
      "Step: 217, MSE: 0.072327, PSNR: 11.406979, 2024-12-20_08-45-57\n",
      "Step: 218, MSE: 0.072312, PSNR: 11.407902, 2024-12-20_08-45-57\n",
      "Step: 219, MSE: 0.072261, PSNR: 11.410937, 2024-12-20_08-45-57\n",
      "Step: 220, MSE: 0.072289, PSNR: 11.409304, 2024-12-20_08-45-57\n",
      "Step: 221, MSE: 0.072277, PSNR: 11.410025, 2024-12-20_08-45-57\n",
      "Step: 222, MSE: 0.072291, PSNR: 11.409184, 2024-12-20_08-45-57\n",
      "Step: 223, MSE: 0.072253, PSNR: 11.411461, 2024-12-20_08-45-57\n",
      "Step: 224, MSE: 0.072263, PSNR: 11.410835, 2024-12-20_08-45-57\n",
      "Step: 225, MSE: 0.072352, PSNR: 11.405504, 2024-12-20_08-45-57\n",
      "Step: 226, MSE: 0.072337, PSNR: 11.406382, 2024-12-20_08-45-57\n",
      "Step: 227, MSE: 0.072391, PSNR: 11.403158, 2024-12-20_08-45-57\n",
      "Step: 228, MSE: 0.072365, PSNR: 11.404710, 2024-12-20_08-45-57\n",
      "Step: 229, MSE: 0.072427, PSNR: 11.401020, 2024-12-20_08-45-57\n",
      "Step: 230, MSE: 0.072323, PSNR: 11.407217, 2024-12-20_08-45-57\n",
      "Step: 231, MSE: 0.072336, PSNR: 11.406441, 2024-12-20_08-45-57\n",
      "Step: 232, MSE: 0.072240, PSNR: 11.412225, 2024-12-20_08-45-57\n",
      "Step: 233, MSE: 0.072373, PSNR: 11.404226, 2024-12-20_08-45-57\n",
      "Step: 234, MSE: 0.072186, PSNR: 11.415468, 2024-12-20_08-45-57\n",
      "Step: 235, MSE: 0.072270, PSNR: 11.410407, 2024-12-20_08-45-57\n",
      "Step: 236, MSE: 0.072321, PSNR: 11.407383, 2024-12-20_08-45-57\n",
      "Step: 237, MSE: 0.072365, PSNR: 11.404744, 2024-12-20_08-45-57\n",
      "Step: 238, MSE: 0.072431, PSNR: 11.400758, 2024-12-20_08-45-57\n",
      "Step: 239, MSE: 0.072134, PSNR: 11.418573, 2024-12-20_08-45-57\n",
      "Step: 240, MSE: 0.072265, PSNR: 11.410727, 2024-12-20_08-45-57\n",
      "Step: 241, MSE: 0.072382, PSNR: 11.403689, 2024-12-20_08-45-57\n",
      "Step: 242, MSE: 0.072421, PSNR: 11.401337, 2024-12-20_08-45-57\n",
      "Step: 243, MSE: 0.072304, PSNR: 11.408403, 2024-12-20_08-45-57\n",
      "Step: 244, MSE: 0.072397, PSNR: 11.402787, 2024-12-20_08-45-57\n",
      "Step: 245, MSE: 0.072326, PSNR: 11.407030, 2024-12-20_08-45-57\n",
      "Step: 246, MSE: 0.072217, PSNR: 11.413635, 2024-12-20_08-45-57\n",
      "Step: 247, MSE: 0.072327, PSNR: 11.407007, 2024-12-20_08-45-57\n",
      "Step: 248, MSE: 0.072382, PSNR: 11.403692, 2024-12-20_08-45-57\n",
      "Step: 249, MSE: 0.072208, PSNR: 11.414122, 2024-12-20_08-45-57\n",
      "Step: 250, MSE: 0.072382, PSNR: 11.403684, 2024-12-20_08-45-57\n",
      "Step: 251, MSE: 0.072370, PSNR: 11.404428, 2024-12-20_08-45-57\n",
      "Step: 252, MSE: 0.072220, PSNR: 11.413402, 2024-12-20_08-45-57\n",
      "Step: 253, MSE: 0.072286, PSNR: 11.409455, 2024-12-20_08-45-57\n",
      "Step: 254, MSE: 0.072406, PSNR: 11.402267, 2024-12-20_08-45-57\n",
      "Step: 255, MSE: 0.072363, PSNR: 11.404815, 2024-12-20_08-45-57\n",
      "Step: 256, MSE: 0.072282, PSNR: 11.409729, 2024-12-20_08-45-57\n",
      "Step: 257, MSE: 0.072367, PSNR: 11.404624, 2024-12-20_08-45-57\n",
      "Step: 258, MSE: 0.072417, PSNR: 11.401573, 2024-12-20_08-45-57\n",
      "Step: 259, MSE: 0.072307, PSNR: 11.408192, 2024-12-20_08-45-57\n",
      "Step: 260, MSE: 0.072327, PSNR: 11.406966, 2024-12-20_08-45-57\n",
      "Step: 261, MSE: 0.072350, PSNR: 11.405612, 2024-12-20_08-45-57\n",
      "Step: 262, MSE: 0.072336, PSNR: 11.406442, 2024-12-20_08-45-57\n",
      "Step: 263, MSE: 0.072301, PSNR: 11.408543, 2024-12-20_08-45-57\n",
      "Step: 264, MSE: 0.072269, PSNR: 11.410508, 2024-12-20_08-45-57\n",
      "Step: 265, MSE: 0.072372, PSNR: 11.404319, 2024-12-20_08-45-57\n",
      "Step: 266, MSE: 0.072353, PSNR: 11.405415, 2024-12-20_08-45-57\n",
      "Step: 267, MSE: 0.072351, PSNR: 11.405545, 2024-12-20_08-45-57\n",
      "Step: 268, MSE: 0.072347, PSNR: 11.405791, 2024-12-20_08-45-57\n",
      "Step: 269, MSE: 0.072273, PSNR: 11.410250, 2024-12-20_08-45-57\n",
      "Step: 270, MSE: 0.072315, PSNR: 11.407695, 2024-12-20_08-45-57\n",
      "Step: 271, MSE: 0.072251, PSNR: 11.411582, 2024-12-20_08-45-57\n",
      "Step: 272, MSE: 0.072325, PSNR: 11.407124, 2024-12-20_08-45-57\n",
      "Step: 273, MSE: 0.072393, PSNR: 11.403049, 2024-12-20_08-45-57\n",
      "Step: 274, MSE: 0.072374, PSNR: 11.404166, 2024-12-20_08-45-57\n",
      "Step: 275, MSE: 0.072406, PSNR: 11.402271, 2024-12-20_08-45-57\n",
      "Step: 276, MSE: 0.072242, PSNR: 11.412094, 2024-12-20_08-45-57\n",
      "Step: 277, MSE: 0.072253, PSNR: 11.411463, 2024-12-20_08-45-57\n",
      "Step: 278, MSE: 0.072291, PSNR: 11.409158, 2024-12-20_08-45-57\n",
      "Step: 279, MSE: 0.072319, PSNR: 11.407495, 2024-12-20_08-45-57\n",
      "Step: 280, MSE: 0.072276, PSNR: 11.410067, 2024-12-20_08-45-57\n",
      "Step: 281, MSE: 0.072407, PSNR: 11.402178, 2024-12-20_08-45-57\n",
      "Step: 282, MSE: 0.072200, PSNR: 11.414610, 2024-12-20_08-45-57\n",
      "Step: 283, MSE: 0.072245, PSNR: 11.411936, 2024-12-20_08-45-57\n",
      "Step: 284, MSE: 0.072224, PSNR: 11.413175, 2024-12-20_08-45-57\n",
      "Step: 285, MSE: 0.072262, PSNR: 11.410922, 2024-12-20_08-45-57\n",
      "Step: 286, MSE: 0.072474, PSNR: 11.398207, 2024-12-20_08-45-57\n",
      "Step: 287, MSE: 0.072398, PSNR: 11.402732, 2024-12-20_08-45-57\n",
      "Step: 288, MSE: 0.072252, PSNR: 11.411515, 2024-12-20_08-45-57\n",
      "Step: 289, MSE: 0.072154, PSNR: 11.417375, 2024-12-20_08-45-57\n",
      "Step: 290, MSE: 0.072259, PSNR: 11.411095, 2024-12-20_08-45-57\n",
      "Step: 291, MSE: 0.072285, PSNR: 11.409538, 2024-12-20_08-45-57\n",
      "Step: 292, MSE: 0.072233, PSNR: 11.412651, 2024-12-20_08-45-57\n",
      "Step: 293, MSE: 0.072258, PSNR: 11.411138, 2024-12-20_08-45-57\n",
      "Step: 294, MSE: 0.072293, PSNR: 11.409043, 2024-12-20_08-45-57\n",
      "Step: 295, MSE: 0.072279, PSNR: 11.409863, 2024-12-20_08-45-57\n",
      "Step: 296, MSE: 0.072266, PSNR: 11.410685, 2024-12-20_08-45-57\n",
      "Step: 297, MSE: 0.072270, PSNR: 11.410433, 2024-12-20_08-45-57\n",
      "Step: 298, MSE: 0.072333, PSNR: 11.406630, 2024-12-20_08-45-57\n",
      "Step: 299, MSE: 0.072313, PSNR: 11.407827, 2024-12-20_08-45-57\n",
      "Step: 300, MSE: 0.072254, PSNR: 11.411402, 2024-12-20_08-45-57\n",
      "Step: 301, MSE: 0.072379, PSNR: 11.403902, 2024-12-20_08-45-57\n",
      "Step: 302, MSE: 0.072265, PSNR: 11.410713, 2024-12-20_08-45-57\n",
      "Step: 303, MSE: 0.072287, PSNR: 11.409376, 2024-12-20_08-45-57\n",
      "Step: 304, MSE: 0.072320, PSNR: 11.407398, 2024-12-20_08-45-57\n",
      "Step: 305, MSE: 0.072428, PSNR: 11.400927, 2024-12-20_08-45-57\n",
      "Step: 306, MSE: 0.072309, PSNR: 11.408102, 2024-12-20_08-45-57\n",
      "Step: 307, MSE: 0.072324, PSNR: 11.407176, 2024-12-20_08-45-57\n",
      "Step: 308, MSE: 0.072339, PSNR: 11.406297, 2024-12-20_08-45-57\n",
      "Step: 309, MSE: 0.072382, PSNR: 11.403702, 2024-12-20_08-45-57\n",
      "Step: 310, MSE: 0.072223, PSNR: 11.413258, 2024-12-20_08-45-57\n",
      "Step: 311, MSE: 0.072177, PSNR: 11.416002, 2024-12-20_08-45-57\n",
      "Step: 312, MSE: 0.072283, PSNR: 11.409644, 2024-12-20_08-45-57\n",
      "Step: 313, MSE: 0.072361, PSNR: 11.404980, 2024-12-20_08-45-57\n",
      "Step: 314, MSE: 0.072327, PSNR: 11.406982, 2024-12-20_08-45-57\n",
      "Step: 315, MSE: 0.072261, PSNR: 11.410986, 2024-12-20_08-45-57\n",
      "Step: 316, MSE: 0.072258, PSNR: 11.411119, 2024-12-20_08-45-57\n",
      "Step: 317, MSE: 0.072362, PSNR: 11.404920, 2024-12-20_08-45-57\n",
      "Step: 318, MSE: 0.072416, PSNR: 11.401627, 2024-12-20_08-45-57\n",
      "Step: 319, MSE: 0.072211, PSNR: 11.413974, 2024-12-20_08-45-57\n",
      "Step: 320, MSE: 0.072296, PSNR: 11.408887, 2024-12-20_08-45-57\n",
      "Step: 321, MSE: 0.072302, PSNR: 11.408478, 2024-12-20_08-45-57\n",
      "Step: 322, MSE: 0.072295, PSNR: 11.408905, 2024-12-20_08-45-57\n",
      "Step: 323, MSE: 0.072319, PSNR: 11.407491, 2024-12-20_08-45-57\n",
      "Step: 324, MSE: 0.072387, PSNR: 11.403370, 2024-12-20_08-45-57\n",
      "Step: 325, MSE: 0.072369, PSNR: 11.404485, 2024-12-20_08-45-57\n",
      "Step: 326, MSE: 0.072279, PSNR: 11.409874, 2024-12-20_08-45-57\n",
      "Step: 327, MSE: 0.072274, PSNR: 11.410160, 2024-12-20_08-45-57\n",
      "Step: 328, MSE: 0.072287, PSNR: 11.409416, 2024-12-20_08-45-57\n",
      "Step: 329, MSE: 0.072394, PSNR: 11.402962, 2024-12-20_08-45-57\n",
      "Step: 330, MSE: 0.072284, PSNR: 11.409576, 2024-12-20_08-45-57\n",
      "Step: 331, MSE: 0.072351, PSNR: 11.405540, 2024-12-20_08-45-57\n",
      "Step: 332, MSE: 0.072369, PSNR: 11.404494, 2024-12-20_08-45-57\n",
      "Step: 333, MSE: 0.072441, PSNR: 11.400161, 2024-12-20_08-45-57\n",
      "Step: 334, MSE: 0.072336, PSNR: 11.406435, 2024-12-20_08-45-57\n",
      "Step: 335, MSE: 0.072419, PSNR: 11.401501, 2024-12-20_08-45-57\n",
      "Step: 336, MSE: 0.072219, PSNR: 11.413471, 2024-12-20_08-45-57\n",
      "Step: 337, MSE: 0.072321, PSNR: 11.407375, 2024-12-20_08-45-57\n",
      "Step: 338, MSE: 0.072334, PSNR: 11.406580, 2024-12-20_08-45-57\n",
      "Step: 339, MSE: 0.072327, PSNR: 11.407013, 2024-12-20_08-45-57\n",
      "Step: 340, MSE: 0.072369, PSNR: 11.404496, 2024-12-20_08-45-57\n",
      "Step: 341, MSE: 0.072312, PSNR: 11.407925, 2024-12-20_08-45-57\n",
      "Step: 342, MSE: 0.072282, PSNR: 11.409719, 2024-12-20_08-45-57\n",
      "Step: 343, MSE: 0.072261, PSNR: 11.410981, 2024-12-20_08-45-57\n",
      "Step: 344, MSE: 0.072259, PSNR: 11.411077, 2024-12-20_08-45-57\n",
      "Step: 345, MSE: 0.072370, PSNR: 11.404415, 2024-12-20_08-45-57\n",
      "Step: 346, MSE: 0.072356, PSNR: 11.405268, 2024-12-20_08-45-57\n",
      "Step: 347, MSE: 0.072408, PSNR: 11.402109, 2024-12-20_08-45-57\n",
      "Step: 348, MSE: 0.072209, PSNR: 11.414094, 2024-12-20_08-45-57\n",
      "Step: 349, MSE: 0.072415, PSNR: 11.401744, 2024-12-20_08-45-57\n",
      "Step: 350, MSE: 0.072325, PSNR: 11.407135, 2024-12-20_08-45-57\n",
      "Step: 351, MSE: 0.072251, PSNR: 11.411544, 2024-12-20_08-45-57\n",
      "Step: 352, MSE: 0.072311, PSNR: 11.407953, 2024-12-20_08-45-57\n",
      "Step: 353, MSE: 0.072461, PSNR: 11.398955, 2024-12-20_08-45-57\n",
      "Step: 354, MSE: 0.072401, PSNR: 11.402568, 2024-12-20_08-45-57\n",
      "Step: 355, MSE: 0.072227, PSNR: 11.412988, 2024-12-20_08-45-57\n",
      "Step: 356, MSE: 0.072442, PSNR: 11.400104, 2024-12-20_08-45-57\n",
      "Step: 357, MSE: 0.072280, PSNR: 11.409801, 2024-12-20_08-45-57\n",
      "Step: 358, MSE: 0.072313, PSNR: 11.407822, 2024-12-20_08-45-57\n",
      "Step: 359, MSE: 0.072266, PSNR: 11.410638, 2024-12-20_08-45-57\n",
      "Step: 360, MSE: 0.072320, PSNR: 11.407395, 2024-12-20_08-45-57\n",
      "Step: 361, MSE: 0.072429, PSNR: 11.400867, 2024-12-20_08-45-57\n",
      "Step: 362, MSE: 0.072257, PSNR: 11.411215, 2024-12-20_08-45-57\n",
      "Step: 363, MSE: 0.072270, PSNR: 11.410423, 2024-12-20_08-45-57\n",
      "Step: 364, MSE: 0.072225, PSNR: 11.413151, 2024-12-20_08-45-57\n",
      "Step: 365, MSE: 0.072339, PSNR: 11.406289, 2024-12-20_08-45-57\n",
      "Step: 366, MSE: 0.072322, PSNR: 11.407281, 2024-12-20_08-45-57\n",
      "Step: 367, MSE: 0.072378, PSNR: 11.403912, 2024-12-20_08-45-57\n",
      "Step: 368, MSE: 0.072264, PSNR: 11.410753, 2024-12-20_08-45-57\n",
      "Step: 369, MSE: 0.072417, PSNR: 11.401577, 2024-12-20_08-45-57\n",
      "Step: 370, MSE: 0.072353, PSNR: 11.405422, 2024-12-20_08-45-57\n",
      "Step: 371, MSE: 0.072363, PSNR: 11.404840, 2024-12-20_08-45-57\n",
      "Step: 372, MSE: 0.072290, PSNR: 11.409207, 2024-12-20_08-45-57\n",
      "Step: 373, MSE: 0.072411, PSNR: 11.401963, 2024-12-20_08-45-57\n",
      "Step: 374, MSE: 0.072412, PSNR: 11.401918, 2024-12-20_08-45-57\n",
      "Step: 375, MSE: 0.072438, PSNR: 11.400335, 2024-12-20_08-45-57\n",
      "Step: 376, MSE: 0.072362, PSNR: 11.404876, 2024-12-20_08-45-57\n",
      "Step: 377, MSE: 0.072415, PSNR: 11.401688, 2024-12-20_08-45-57\n",
      "Step: 378, MSE: 0.072326, PSNR: 11.407055, 2024-12-20_08-45-57\n",
      "Step: 379, MSE: 0.072309, PSNR: 11.408104, 2024-12-20_08-45-57\n",
      "Step: 380, MSE: 0.072310, PSNR: 11.407990, 2024-12-20_08-45-57\n",
      "Step: 381, MSE: 0.072306, PSNR: 11.408239, 2024-12-20_08-45-57\n",
      "Step: 382, MSE: 0.072381, PSNR: 11.403775, 2024-12-20_08-45-57\n",
      "Step: 383, MSE: 0.072396, PSNR: 11.402854, 2024-12-20_08-45-57\n",
      "Step: 384, MSE: 0.072258, PSNR: 11.411137, 2024-12-20_08-45-57\n",
      "Step: 385, MSE: 0.072302, PSNR: 11.408501, 2024-12-20_08-45-57\n",
      "Step: 386, MSE: 0.072239, PSNR: 11.412272, 2024-12-20_08-45-57\n",
      "Step: 387, MSE: 0.072288, PSNR: 11.409330, 2024-12-20_08-45-57\n",
      "Step: 388, MSE: 0.072346, PSNR: 11.405869, 2024-12-20_08-45-57\n",
      "Step: 389, MSE: 0.072351, PSNR: 11.405534, 2024-12-20_08-45-57\n",
      "Step: 390, MSE: 0.072323, PSNR: 11.407219, 2024-12-20_08-45-57\n",
      "Step: 391, MSE: 0.072402, PSNR: 11.402475, 2024-12-20_08-45-57\n",
      "Step: 392, MSE: 0.072215, PSNR: 11.413723, 2024-12-20_08-45-57\n",
      "Step: 393, MSE: 0.072208, PSNR: 11.414162, 2024-12-20_08-45-57\n",
      "Step: 394, MSE: 0.072400, PSNR: 11.402643, 2024-12-20_08-45-57\n",
      "Step: 395, MSE: 0.072305, PSNR: 11.408297, 2024-12-20_08-45-57\n",
      "Step: 396, MSE: 0.072262, PSNR: 11.410906, 2024-12-20_08-45-57\n",
      "Step: 397, MSE: 0.072292, PSNR: 11.409097, 2024-12-20_08-45-57\n",
      "Step: 398, MSE: 0.072507, PSNR: 11.396229, 2024-12-20_08-45-57\n",
      "Step: 399, MSE: 0.072320, PSNR: 11.407391, 2024-12-20_08-45-57\n",
      "Step: 400, MSE: 0.072238, PSNR: 11.412316, 2024-12-20_08-45-57\n",
      "Step: 401, MSE: 0.072249, PSNR: 11.411703, 2024-12-20_08-45-57\n",
      "Step: 402, MSE: 0.072319, PSNR: 11.407482, 2024-12-20_08-45-57\n",
      "Step: 403, MSE: 0.072234, PSNR: 11.412588, 2024-12-20_08-45-57\n",
      "Step: 404, MSE: 0.072397, PSNR: 11.402782, 2024-12-20_08-45-57\n",
      "Step: 405, MSE: 0.072337, PSNR: 11.406411, 2024-12-20_08-45-57\n",
      "Step: 406, MSE: 0.072310, PSNR: 11.408041, 2024-12-20_08-45-57\n",
      "Step: 407, MSE: 0.072211, PSNR: 11.413984, 2024-12-20_08-45-57\n",
      "Step: 408, MSE: 0.072372, PSNR: 11.404274, 2024-12-20_08-45-57\n",
      "Step: 409, MSE: 0.072264, PSNR: 11.410772, 2024-12-20_08-45-57\n",
      "Step: 410, MSE: 0.072427, PSNR: 11.401003, 2024-12-20_08-45-57\n",
      "Step: 411, MSE: 0.072280, PSNR: 11.409821, 2024-12-20_08-45-57\n",
      "Step: 412, MSE: 0.072238, PSNR: 11.412345, 2024-12-20_08-45-57\n",
      "Step: 413, MSE: 0.072282, PSNR: 11.409676, 2024-12-20_08-45-57\n",
      "Step: 414, MSE: 0.072271, PSNR: 11.410346, 2024-12-20_08-45-57\n",
      "Step: 415, MSE: 0.072320, PSNR: 11.407417, 2024-12-20_08-45-57\n",
      "Step: 416, MSE: 0.072255, PSNR: 11.411331, 2024-12-20_08-45-57\n",
      "Step: 417, MSE: 0.072448, PSNR: 11.399756, 2024-12-20_08-45-57\n",
      "Step: 418, MSE: 0.072306, PSNR: 11.408244, 2024-12-20_08-45-57\n",
      "Step: 419, MSE: 0.072372, PSNR: 11.404303, 2024-12-20_08-45-57\n",
      "Step: 420, MSE: 0.072296, PSNR: 11.408878, 2024-12-20_08-45-57\n",
      "Step: 421, MSE: 0.072308, PSNR: 11.408124, 2024-12-20_08-45-57\n",
      "Step: 422, MSE: 0.072398, PSNR: 11.402758, 2024-12-20_08-45-57\n",
      "Step: 423, MSE: 0.072360, PSNR: 11.405010, 2024-12-20_08-45-57\n",
      "Step: 424, MSE: 0.072280, PSNR: 11.409799, 2024-12-20_08-45-57\n",
      "Step: 425, MSE: 0.072476, PSNR: 11.398052, 2024-12-20_08-45-57\n",
      "Step: 426, MSE: 0.072423, PSNR: 11.401243, 2024-12-20_08-45-57\n",
      "Step: 427, MSE: 0.072259, PSNR: 11.411103, 2024-12-20_08-45-57\n",
      "Step: 428, MSE: 0.072382, PSNR: 11.403667, 2024-12-20_08-45-57\n",
      "Step: 429, MSE: 0.072384, PSNR: 11.403569, 2024-12-20_08-45-57\n",
      "Step: 430, MSE: 0.072334, PSNR: 11.406582, 2024-12-20_08-45-57\n",
      "Step: 431, MSE: 0.072213, PSNR: 11.413853, 2024-12-20_08-45-57\n",
      "Step: 432, MSE: 0.072344, PSNR: 11.405958, 2024-12-20_08-45-57\n",
      "Step: 433, MSE: 0.072287, PSNR: 11.409393, 2024-12-20_08-45-57\n",
      "Step: 434, MSE: 0.072318, PSNR: 11.407508, 2024-12-20_08-45-57\n",
      "Step: 435, MSE: 0.072299, PSNR: 11.408692, 2024-12-20_08-45-57\n",
      "Step: 436, MSE: 0.072338, PSNR: 11.406365, 2024-12-20_08-45-57\n",
      "Step: 437, MSE: 0.072225, PSNR: 11.413126, 2024-12-20_08-45-57\n",
      "Step: 438, MSE: 0.072299, PSNR: 11.408684, 2024-12-20_08-45-57\n",
      "Step: 439, MSE: 0.072313, PSNR: 11.407860, 2024-12-20_08-45-57\n",
      "Step: 440, MSE: 0.072273, PSNR: 11.410213, 2024-12-20_08-45-57\n",
      "Step: 441, MSE: 0.072270, PSNR: 11.410432, 2024-12-20_08-45-57\n",
      "Step: 442, MSE: 0.072281, PSNR: 11.409740, 2024-12-20_08-45-57\n",
      "Step: 443, MSE: 0.072385, PSNR: 11.403490, 2024-12-20_08-45-57\n",
      "Step: 444, MSE: 0.072242, PSNR: 11.412115, 2024-12-20_08-45-57\n",
      "Step: 445, MSE: 0.072496, PSNR: 11.396838, 2024-12-20_08-45-57\n",
      "Step: 446, MSE: 0.072207, PSNR: 11.414198, 2024-12-20_08-45-57\n",
      "Step: 447, MSE: 0.072451, PSNR: 11.399536, 2024-12-20_08-45-57\n",
      "Step: 448, MSE: 0.072294, PSNR: 11.408968, 2024-12-20_08-45-57\n",
      "Step: 449, MSE: 0.072450, PSNR: 11.399631, 2024-12-20_08-45-57\n",
      "Step: 450, MSE: 0.072312, PSNR: 11.407901, 2024-12-20_08-45-57\n",
      "Step: 451, MSE: 0.072398, PSNR: 11.402708, 2024-12-20_08-45-57\n",
      "Step: 452, MSE: 0.072246, PSNR: 11.411837, 2024-12-20_08-45-57\n",
      "Step: 453, MSE: 0.072287, PSNR: 11.409395, 2024-12-20_08-45-57\n",
      "Step: 454, MSE: 0.072259, PSNR: 11.411057, 2024-12-20_08-45-57\n",
      "Step: 455, MSE: 0.072394, PSNR: 11.403004, 2024-12-20_08-45-57\n",
      "Step: 456, MSE: 0.072193, PSNR: 11.415061, 2024-12-20_08-45-57\n",
      "Step: 457, MSE: 0.072270, PSNR: 11.410399, 2024-12-20_08-45-57\n",
      "Step: 458, MSE: 0.072413, PSNR: 11.401838, 2024-12-20_08-45-57\n",
      "Step: 459, MSE: 0.072271, PSNR: 11.410358, 2024-12-20_08-45-57\n",
      "Step: 460, MSE: 0.072305, PSNR: 11.408306, 2024-12-20_08-45-57\n",
      "Step: 461, MSE: 0.072205, PSNR: 11.414323, 2024-12-20_08-45-57\n",
      "Step: 462, MSE: 0.072229, PSNR: 11.412910, 2024-12-20_08-45-57\n",
      "Step: 463, MSE: 0.072283, PSNR: 11.409657, 2024-12-20_08-45-57\n",
      "Step: 464, MSE: 0.072258, PSNR: 11.411142, 2024-12-20_08-45-57\n",
      "Step: 465, MSE: 0.072269, PSNR: 11.410504, 2024-12-20_08-45-57\n",
      "Step: 466, MSE: 0.072433, PSNR: 11.400613, 2024-12-20_08-45-57\n",
      "Step: 467, MSE: 0.072227, PSNR: 11.412998, 2024-12-20_08-45-57\n",
      "Step: 468, MSE: 0.072323, PSNR: 11.407234, 2024-12-20_08-45-57\n",
      "Step: 469, MSE: 0.072217, PSNR: 11.413624, 2024-12-20_08-45-57\n",
      "Step: 470, MSE: 0.072311, PSNR: 11.407962, 2024-12-20_08-45-57\n",
      "Step: 471, MSE: 0.072235, PSNR: 11.412521, 2024-12-20_08-45-57\n",
      "Step: 472, MSE: 0.072419, PSNR: 11.401459, 2024-12-20_08-45-57\n",
      "Step: 473, MSE: 0.072387, PSNR: 11.403418, 2024-12-20_08-45-57\n",
      "Step: 474, MSE: 0.072369, PSNR: 11.404468, 2024-12-20_08-45-57\n",
      "Step: 475, MSE: 0.072395, PSNR: 11.402945, 2024-12-20_08-45-57\n",
      "Step: 476, MSE: 0.072420, PSNR: 11.401404, 2024-12-20_08-45-57\n",
      "Step: 477, MSE: 0.072253, PSNR: 11.411466, 2024-12-20_08-45-57\n",
      "Step: 478, MSE: 0.072333, PSNR: 11.406666, 2024-12-20_08-45-57\n",
      "Step: 479, MSE: 0.072372, PSNR: 11.404286, 2024-12-20_08-45-57\n",
      "Step: 480, MSE: 0.072254, PSNR: 11.411360, 2024-12-20_08-45-57\n",
      "Step: 481, MSE: 0.072268, PSNR: 11.410546, 2024-12-20_08-45-57\n",
      "Step: 482, MSE: 0.072203, PSNR: 11.414467, 2024-12-20_08-45-57\n",
      "Step: 483, MSE: 0.072302, PSNR: 11.408484, 2024-12-20_08-45-57\n",
      "Step: 484, MSE: 0.072280, PSNR: 11.409809, 2024-12-20_08-45-57\n",
      "Step: 485, MSE: 0.072344, PSNR: 11.405951, 2024-12-20_08-45-57\n",
      "Step: 486, MSE: 0.072310, PSNR: 11.408039, 2024-12-20_08-45-57\n",
      "Step: 487, MSE: 0.072321, PSNR: 11.407375, 2024-12-20_08-45-57\n",
      "Step: 488, MSE: 0.072401, PSNR: 11.402538, 2024-12-20_08-45-57\n",
      "Step: 489, MSE: 0.072369, PSNR: 11.404493, 2024-12-20_08-45-57\n",
      "Step: 490, MSE: 0.072367, PSNR: 11.404618, 2024-12-20_08-45-57\n",
      "Step: 491, MSE: 0.072312, PSNR: 11.407888, 2024-12-20_08-45-57\n",
      "Step: 492, MSE: 0.072278, PSNR: 11.409929, 2024-12-20_08-45-57\n",
      "Step: 493, MSE: 0.072434, PSNR: 11.400577, 2024-12-20_08-45-57\n",
      "Step: 494, MSE: 0.072363, PSNR: 11.404805, 2024-12-20_08-45-57\n",
      "Step: 495, MSE: 0.072391, PSNR: 11.403127, 2024-12-20_08-45-57\n",
      "Step: 496, MSE: 0.072428, PSNR: 11.400928, 2024-12-20_08-45-57\n",
      "Step: 497, MSE: 0.072278, PSNR: 11.409948, 2024-12-20_08-45-57\n",
      "Step: 498, MSE: 0.072401, PSNR: 11.402584, 2024-12-20_08-45-57\n",
      "Step: 499, MSE: 0.072272, PSNR: 11.410324, 2024-12-20_08-45-57\n",
      "Step: 500, MSE: 0.072388, PSNR: 11.403321, 2024-12-20_08-45-57\n",
      "Step: 501, MSE: 0.072409, PSNR: 11.402058, 2024-12-20_08-45-57\n",
      "Step: 502, MSE: 0.072445, PSNR: 11.399902, 2024-12-20_08-45-57\n",
      "Step: 503, MSE: 0.072241, PSNR: 11.412184, 2024-12-20_08-45-57\n",
      "Step: 504, MSE: 0.072237, PSNR: 11.412382, 2024-12-20_08-45-57\n",
      "Step: 505, MSE: 0.072197, PSNR: 11.414799, 2024-12-20_08-45-57\n",
      "Step: 506, MSE: 0.072329, PSNR: 11.406851, 2024-12-20_08-45-57\n",
      "Step: 507, MSE: 0.072245, PSNR: 11.411930, 2024-12-20_08-45-57\n",
      "Step: 508, MSE: 0.072253, PSNR: 11.411413, 2024-12-20_08-45-57\n",
      "Step: 509, MSE: 0.072176, PSNR: 11.416048, 2024-12-20_08-45-57\n",
      "Step: 510, MSE: 0.072391, PSNR: 11.403161, 2024-12-20_08-45-57\n",
      "Step: 511, MSE: 0.072240, PSNR: 11.412252, 2024-12-20_08-45-57\n",
      "Step: 512, MSE: 0.072391, PSNR: 11.403154, 2024-12-20_08-45-57\n",
      "Step: 513, MSE: 0.072301, PSNR: 11.408578, 2024-12-20_08-45-57\n",
      "Step: 514, MSE: 0.072283, PSNR: 11.409612, 2024-12-20_08-45-57\n",
      "Step: 515, MSE: 0.072410, PSNR: 11.402040, 2024-12-20_08-45-57\n",
      "Step: 516, MSE: 0.072235, PSNR: 11.412500, 2024-12-20_08-45-57\n",
      "Step: 517, MSE: 0.072415, PSNR: 11.401696, 2024-12-20_08-45-57\n",
      "Step: 518, MSE: 0.072344, PSNR: 11.405989, 2024-12-20_08-45-57\n",
      "Step: 519, MSE: 0.072379, PSNR: 11.403895, 2024-12-20_08-45-57\n",
      "Step: 520, MSE: 0.072227, PSNR: 11.412991, 2024-12-20_08-45-57\n",
      "Step: 521, MSE: 0.072284, PSNR: 11.409574, 2024-12-20_08-45-57\n",
      "Step: 522, MSE: 0.072301, PSNR: 11.408558, 2024-12-20_08-45-57\n",
      "Step: 523, MSE: 0.072242, PSNR: 11.412081, 2024-12-20_08-45-57\n",
      "Step: 524, MSE: 0.072369, PSNR: 11.404474, 2024-12-20_08-45-57\n",
      "Step: 525, MSE: 0.072401, PSNR: 11.402580, 2024-12-20_08-45-57\n",
      "Step: 526, MSE: 0.072467, PSNR: 11.398569, 2024-12-20_08-45-57\n",
      "Step: 527, MSE: 0.072275, PSNR: 11.410140, 2024-12-20_08-45-57\n",
      "Step: 528, MSE: 0.072223, PSNR: 11.413242, 2024-12-20_08-45-57\n",
      "Step: 529, MSE: 0.072284, PSNR: 11.409607, 2024-12-20_08-45-57\n",
      "Step: 530, MSE: 0.072288, PSNR: 11.409323, 2024-12-20_08-45-57\n",
      "Step: 531, MSE: 0.072203, PSNR: 11.414460, 2024-12-20_08-45-57\n",
      "Step: 532, MSE: 0.072350, PSNR: 11.405600, 2024-12-20_08-45-57\n",
      "Step: 533, MSE: 0.072372, PSNR: 11.404324, 2024-12-20_08-45-57\n",
      "Step: 534, MSE: 0.072304, PSNR: 11.408358, 2024-12-20_08-45-57\n",
      "Step: 535, MSE: 0.072338, PSNR: 11.406325, 2024-12-20_08-45-57\n",
      "Step: 536, MSE: 0.072295, PSNR: 11.408911, 2024-12-20_08-45-57\n",
      "Step: 537, MSE: 0.072417, PSNR: 11.401621, 2024-12-20_08-45-57\n",
      "Step: 538, MSE: 0.072294, PSNR: 11.408993, 2024-12-20_08-45-57\n",
      "Step: 539, MSE: 0.072249, PSNR: 11.411694, 2024-12-20_08-45-57\n",
      "Step: 540, MSE: 0.072480, PSNR: 11.397811, 2024-12-20_08-45-57\n",
      "Step: 541, MSE: 0.072286, PSNR: 11.409437, 2024-12-20_08-45-57\n",
      "Step: 542, MSE: 0.072225, PSNR: 11.413098, 2024-12-20_08-45-57\n",
      "Step: 543, MSE: 0.072352, PSNR: 11.405499, 2024-12-20_08-45-57\n",
      "Step: 544, MSE: 0.072334, PSNR: 11.406579, 2024-12-20_08-45-57\n",
      "Step: 545, MSE: 0.072431, PSNR: 11.400743, 2024-12-20_08-45-57\n",
      "Step: 546, MSE: 0.072277, PSNR: 11.410016, 2024-12-20_08-45-57\n",
      "Step: 547, MSE: 0.072338, PSNR: 11.406355, 2024-12-20_08-45-57\n",
      "Step: 548, MSE: 0.072282, PSNR: 11.409675, 2024-12-20_08-45-57\n",
      "Step: 549, MSE: 0.072345, PSNR: 11.405887, 2024-12-20_08-45-57\n",
      "Step: 550, MSE: 0.072307, PSNR: 11.408216, 2024-12-20_08-45-57\n",
      "Step: 551, MSE: 0.072244, PSNR: 11.412000, 2024-12-20_08-45-57\n",
      "Step: 552, MSE: 0.072440, PSNR: 11.400224, 2024-12-20_08-45-57\n",
      "Step: 553, MSE: 0.072271, PSNR: 11.410360, 2024-12-20_08-45-57\n",
      "Step: 554, MSE: 0.072324, PSNR: 11.407169, 2024-12-20_08-45-57\n",
      "Step: 555, MSE: 0.072304, PSNR: 11.408353, 2024-12-20_08-45-57\n",
      "Step: 556, MSE: 0.072399, PSNR: 11.402668, 2024-12-20_08-45-57\n",
      "Step: 557, MSE: 0.072285, PSNR: 11.409527, 2024-12-20_08-45-57\n",
      "Step: 558, MSE: 0.072314, PSNR: 11.407801, 2024-12-20_08-45-57\n",
      "Step: 559, MSE: 0.072271, PSNR: 11.410385, 2024-12-20_08-45-57\n",
      "Step: 560, MSE: 0.072425, PSNR: 11.401117, 2024-12-20_08-45-57\n",
      "Step: 561, MSE: 0.072286, PSNR: 11.409454, 2024-12-20_08-45-57\n",
      "Step: 562, MSE: 0.072409, PSNR: 11.402069, 2024-12-20_08-45-57\n",
      "Step: 563, MSE: 0.072412, PSNR: 11.401911, 2024-12-20_08-45-57\n",
      "Step: 564, MSE: 0.072328, PSNR: 11.406936, 2024-12-20_08-45-57\n",
      "Step: 565, MSE: 0.072362, PSNR: 11.404878, 2024-12-20_08-45-57\n",
      "Step: 566, MSE: 0.072263, PSNR: 11.410814, 2024-12-20_08-45-57\n",
      "Step: 567, MSE: 0.072416, PSNR: 11.401670, 2024-12-20_08-45-57\n",
      "Step: 568, MSE: 0.072404, PSNR: 11.402367, 2024-12-20_08-45-57\n",
      "Step: 569, MSE: 0.072272, PSNR: 11.410315, 2024-12-20_08-45-57\n",
      "Step: 570, MSE: 0.072276, PSNR: 11.410054, 2024-12-20_08-45-57\n",
      "Step: 571, MSE: 0.072202, PSNR: 11.414508, 2024-12-20_08-45-57\n",
      "Step: 572, MSE: 0.072363, PSNR: 11.404839, 2024-12-20_08-45-57\n",
      "Step: 573, MSE: 0.072363, PSNR: 11.404843, 2024-12-20_08-45-57\n",
      "Step: 574, MSE: 0.072334, PSNR: 11.406555, 2024-12-20_08-45-57\n",
      "Step: 575, MSE: 0.072415, PSNR: 11.401711, 2024-12-20_08-45-57\n",
      "Step: 576, MSE: 0.072332, PSNR: 11.406710, 2024-12-20_08-45-57\n",
      "Step: 577, MSE: 0.072434, PSNR: 11.400602, 2024-12-20_08-45-57\n",
      "Step: 578, MSE: 0.072292, PSNR: 11.409120, 2024-12-20_08-45-57\n",
      "Step: 579, MSE: 0.072309, PSNR: 11.408105, 2024-12-20_08-45-57\n",
      "Step: 580, MSE: 0.072272, PSNR: 11.410309, 2024-12-20_08-45-57\n",
      "Step: 581, MSE: 0.072278, PSNR: 11.409952, 2024-12-20_08-45-57\n",
      "Step: 582, MSE: 0.072353, PSNR: 11.405457, 2024-12-20_08-45-57\n",
      "Step: 583, MSE: 0.072378, PSNR: 11.403909, 2024-12-20_08-45-57\n",
      "Step: 584, MSE: 0.072241, PSNR: 11.412158, 2024-12-20_08-45-57\n",
      "Step: 585, MSE: 0.072328, PSNR: 11.406923, 2024-12-20_08-45-57\n",
      "Step: 586, MSE: 0.072326, PSNR: 11.407079, 2024-12-20_08-45-57\n",
      "Step: 587, MSE: 0.072386, PSNR: 11.403426, 2024-12-20_08-45-57\n",
      "Step: 588, MSE: 0.072232, PSNR: 11.412714, 2024-12-20_08-45-57\n",
      "Step: 589, MSE: 0.072197, PSNR: 11.414795, 2024-12-20_08-45-57\n",
      "Step: 590, MSE: 0.072313, PSNR: 11.407861, 2024-12-20_08-45-57\n",
      "Step: 591, MSE: 0.072400, PSNR: 11.402588, 2024-12-20_08-45-57\n",
      "Step: 592, MSE: 0.072256, PSNR: 11.411249, 2024-12-20_08-45-57\n",
      "Step: 593, MSE: 0.072383, PSNR: 11.403642, 2024-12-20_08-45-57\n",
      "Step: 594, MSE: 0.072325, PSNR: 11.407139, 2024-12-20_08-45-57\n",
      "Step: 595, MSE: 0.072365, PSNR: 11.404714, 2024-12-20_08-45-57\n",
      "Step: 596, MSE: 0.072419, PSNR: 11.401462, 2024-12-20_08-45-57\n",
      "Step: 597, MSE: 0.072356, PSNR: 11.405235, 2024-12-20_08-45-57\n",
      "Step: 598, MSE: 0.072319, PSNR: 11.407466, 2024-12-20_08-45-57\n",
      "Step: 599, MSE: 0.072288, PSNR: 11.409319, 2024-12-20_08-45-57\n",
      "Step: 600, MSE: 0.072250, PSNR: 11.411605, 2024-12-20_08-45-57\n",
      "Step: 601, MSE: 0.072245, PSNR: 11.411898, 2024-12-20_08-45-57\n",
      "Step: 602, MSE: 0.072345, PSNR: 11.405918, 2024-12-20_08-45-57\n",
      "Step: 603, MSE: 0.072426, PSNR: 11.401043, 2024-12-20_08-45-57\n",
      "Step: 604, MSE: 0.072275, PSNR: 11.410142, 2024-12-20_08-45-57\n",
      "Step: 605, MSE: 0.072446, PSNR: 11.399865, 2024-12-20_08-45-57\n",
      "Step: 606, MSE: 0.072253, PSNR: 11.411462, 2024-12-20_08-45-57\n",
      "Step: 607, MSE: 0.072383, PSNR: 11.403618, 2024-12-20_08-45-57\n",
      "Step: 608, MSE: 0.072289, PSNR: 11.409268, 2024-12-20_08-45-57\n",
      "Step: 609, MSE: 0.072346, PSNR: 11.405828, 2024-12-20_08-45-57\n",
      "Step: 610, MSE: 0.072341, PSNR: 11.406146, 2024-12-20_08-45-57\n",
      "Step: 611, MSE: 0.072382, PSNR: 11.403709, 2024-12-20_08-45-57\n",
      "Step: 612, MSE: 0.072209, PSNR: 11.414107, 2024-12-20_08-45-57\n",
      "Step: 613, MSE: 0.072301, PSNR: 11.408535, 2024-12-20_08-45-57\n",
      "Step: 614, MSE: 0.072282, PSNR: 11.409674, 2024-12-20_08-45-57\n",
      "Step: 615, MSE: 0.072260, PSNR: 11.411020, 2024-12-20_08-45-57\n",
      "Step: 616, MSE: 0.072406, PSNR: 11.402272, 2024-12-20_08-45-57\n",
      "Step: 617, MSE: 0.072211, PSNR: 11.413960, 2024-12-20_08-45-57\n",
      "Step: 618, MSE: 0.072299, PSNR: 11.408648, 2024-12-20_08-45-57\n",
      "Step: 619, MSE: 0.072396, PSNR: 11.402876, 2024-12-20_08-45-57\n",
      "Step: 620, MSE: 0.072297, PSNR: 11.408820, 2024-12-20_08-45-57\n",
      "Step: 621, MSE: 0.072543, PSNR: 11.394029, 2024-12-20_08-45-57\n",
      "Step: 622, MSE: 0.072338, PSNR: 11.406344, 2024-12-20_08-45-57\n",
      "Step: 623, MSE: 0.072221, PSNR: 11.413363, 2024-12-20_08-45-57\n",
      "Step: 624, MSE: 0.072345, PSNR: 11.405906, 2024-12-20_08-45-57\n",
      "Step: 625, MSE: 0.072288, PSNR: 11.409312, 2024-12-20_08-45-57\n",
      "Step: 626, MSE: 0.072397, PSNR: 11.402802, 2024-12-20_08-45-57\n",
      "Step: 627, MSE: 0.072159, PSNR: 11.417101, 2024-12-20_08-45-57\n",
      "Step: 628, MSE: 0.072268, PSNR: 11.410541, 2024-12-20_08-45-57\n",
      "Step: 629, MSE: 0.072361, PSNR: 11.404971, 2024-12-20_08-45-57\n",
      "Step: 630, MSE: 0.072414, PSNR: 11.401773, 2024-12-20_08-45-57\n",
      "Step: 631, MSE: 0.072415, PSNR: 11.401697, 2024-12-20_08-45-57\n",
      "Step: 632, MSE: 0.072346, PSNR: 11.405865, 2024-12-20_08-45-57\n",
      "Step: 633, MSE: 0.072204, PSNR: 11.414384, 2024-12-20_08-45-57\n",
      "Step: 634, MSE: 0.072344, PSNR: 11.405983, 2024-12-20_08-45-57\n",
      "Step: 635, MSE: 0.072313, PSNR: 11.407847, 2024-12-20_08-45-57\n",
      "Step: 636, MSE: 0.072247, PSNR: 11.411804, 2024-12-20_08-45-57\n",
      "Step: 637, MSE: 0.072358, PSNR: 11.405117, 2024-12-20_08-45-57\n",
      "Step: 638, MSE: 0.072412, PSNR: 11.401892, 2024-12-20_08-45-57\n",
      "Step: 639, MSE: 0.072380, PSNR: 11.403831, 2024-12-20_08-45-57\n",
      "Step: 640, MSE: 0.072345, PSNR: 11.405916, 2024-12-20_08-45-57\n",
      "Step: 641, MSE: 0.072277, PSNR: 11.410018, 2024-12-20_08-45-57\n",
      "Step: 642, MSE: 0.072286, PSNR: 11.409456, 2024-12-20_08-45-57\n",
      "Step: 643, MSE: 0.072348, PSNR: 11.405741, 2024-12-20_08-45-57\n",
      "Step: 644, MSE: 0.072359, PSNR: 11.405083, 2024-12-20_08-45-57\n",
      "Step: 645, MSE: 0.072268, PSNR: 11.410551, 2024-12-20_08-45-57\n",
      "Step: 646, MSE: 0.072322, PSNR: 11.407307, 2024-12-20_08-45-57\n",
      "Step: 647, MSE: 0.072391, PSNR: 11.403127, 2024-12-20_08-45-57\n",
      "Step: 648, MSE: 0.072448, PSNR: 11.399729, 2024-12-20_08-45-57\n",
      "Step: 649, MSE: 0.072275, PSNR: 11.410089, 2024-12-20_08-45-57\n",
      "Step: 650, MSE: 0.072227, PSNR: 11.413013, 2024-12-20_08-45-57\n",
      "Step: 651, MSE: 0.072221, PSNR: 11.413380, 2024-12-20_08-45-57\n",
      "Step: 652, MSE: 0.072290, PSNR: 11.409203, 2024-12-20_08-45-57\n",
      "Step: 653, MSE: 0.072318, PSNR: 11.407565, 2024-12-20_08-45-57\n",
      "Step: 654, MSE: 0.072321, PSNR: 11.407346, 2024-12-20_08-45-57\n",
      "Step: 655, MSE: 0.072291, PSNR: 11.409134, 2024-12-20_08-45-57\n",
      "Step: 656, MSE: 0.072308, PSNR: 11.408142, 2024-12-20_08-45-57\n",
      "Step: 657, MSE: 0.072331, PSNR: 11.406744, 2024-12-20_08-45-57\n",
      "Step: 658, MSE: 0.072214, PSNR: 11.413779, 2024-12-20_08-45-57\n",
      "Step: 659, MSE: 0.072312, PSNR: 11.407901, 2024-12-20_08-45-57\n",
      "Step: 660, MSE: 0.072372, PSNR: 11.404287, 2024-12-20_08-45-57\n",
      "Step: 661, MSE: 0.072275, PSNR: 11.410112, 2024-12-20_08-45-57\n",
      "Step: 662, MSE: 0.072320, PSNR: 11.407391, 2024-12-20_08-45-57\n",
      "Step: 663, MSE: 0.072236, PSNR: 11.412448, 2024-12-20_08-45-57\n",
      "Step: 664, MSE: 0.072318, PSNR: 11.407524, 2024-12-20_08-45-57\n",
      "Step: 665, MSE: 0.072371, PSNR: 11.404360, 2024-12-20_08-45-57\n",
      "Step: 666, MSE: 0.072328, PSNR: 11.406941, 2024-12-20_08-45-57\n",
      "Step: 667, MSE: 0.072283, PSNR: 11.409637, 2024-12-20_08-45-57\n",
      "Step: 668, MSE: 0.072349, PSNR: 11.405691, 2024-12-20_08-45-57\n",
      "Step: 669, MSE: 0.072321, PSNR: 11.407335, 2024-12-20_08-45-57\n",
      "Step: 670, MSE: 0.072355, PSNR: 11.405317, 2024-12-20_08-45-57\n",
      "Step: 671, MSE: 0.072265, PSNR: 11.410707, 2024-12-20_08-45-57\n",
      "Step: 672, MSE: 0.072429, PSNR: 11.400892, 2024-12-20_08-45-57\n",
      "Step: 673, MSE: 0.072248, PSNR: 11.411742, 2024-12-20_08-45-57\n",
      "Step: 674, MSE: 0.072310, PSNR: 11.408019, 2024-12-20_08-45-57\n",
      "Step: 675, MSE: 0.072247, PSNR: 11.411823, 2024-12-20_08-45-57\n",
      "Step: 676, MSE: 0.072368, PSNR: 11.404530, 2024-12-20_08-45-57\n",
      "Step: 677, MSE: 0.072339, PSNR: 11.406277, 2024-12-20_08-45-57\n",
      "Step: 678, MSE: 0.072291, PSNR: 11.409178, 2024-12-20_08-45-57\n",
      "Step: 679, MSE: 0.072290, PSNR: 11.409199, 2024-12-20_08-45-57\n",
      "Step: 680, MSE: 0.072332, PSNR: 11.406723, 2024-12-20_08-45-57\n",
      "Step: 681, MSE: 0.072314, PSNR: 11.407778, 2024-12-20_08-45-57\n",
      "Step: 682, MSE: 0.072232, PSNR: 11.412727, 2024-12-20_08-45-57\n",
      "Step: 683, MSE: 0.072405, PSNR: 11.402297, 2024-12-20_08-45-57\n",
      "Step: 684, MSE: 0.072373, PSNR: 11.404253, 2024-12-20_08-45-57\n",
      "Step: 685, MSE: 0.072281, PSNR: 11.409767, 2024-12-20_08-45-57\n",
      "Step: 686, MSE: 0.072455, PSNR: 11.399307, 2024-12-20_08-45-57\n",
      "Step: 687, MSE: 0.072431, PSNR: 11.400781, 2024-12-20_08-45-57\n",
      "Step: 688, MSE: 0.072198, PSNR: 11.414720, 2024-12-20_08-45-57\n",
      "Step: 689, MSE: 0.072314, PSNR: 11.407775, 2024-12-20_08-45-57\n",
      "Step: 690, MSE: 0.072255, PSNR: 11.411331, 2024-12-20_08-45-57\n",
      "Step: 691, MSE: 0.072273, PSNR: 11.410244, 2024-12-20_08-45-57\n",
      "Step: 692, MSE: 0.072371, PSNR: 11.404332, 2024-12-20_08-45-57\n",
      "Step: 693, MSE: 0.072244, PSNR: 11.411973, 2024-12-20_08-45-57\n",
      "Step: 694, MSE: 0.072328, PSNR: 11.406952, 2024-12-20_08-45-57\n",
      "Step: 695, MSE: 0.072441, PSNR: 11.400183, 2024-12-20_08-45-57\n",
      "Step: 696, MSE: 0.072292, PSNR: 11.409120, 2024-12-20_08-45-57\n",
      "Step: 697, MSE: 0.072256, PSNR: 11.411238, 2024-12-20_08-45-57\n",
      "Step: 698, MSE: 0.072291, PSNR: 11.409185, 2024-12-20_08-45-57\n",
      "Step: 699, MSE: 0.072226, PSNR: 11.413041, 2024-12-20_08-45-57\n",
      "Step: 700, MSE: 0.072382, PSNR: 11.403676, 2024-12-20_08-45-57\n",
      "Step: 701, MSE: 0.072457, PSNR: 11.399221, 2024-12-20_08-45-57\n",
      "Step: 702, MSE: 0.072341, PSNR: 11.406163, 2024-12-20_08-45-57\n",
      "Step: 703, MSE: 0.072202, PSNR: 11.414522, 2024-12-20_08-45-57\n",
      "Step: 704, MSE: 0.072298, PSNR: 11.408763, 2024-12-20_08-45-57\n",
      "Step: 705, MSE: 0.072328, PSNR: 11.406937, 2024-12-20_08-45-57\n",
      "Step: 706, MSE: 0.072230, PSNR: 11.412846, 2024-12-20_08-45-57\n",
      "Step: 707, MSE: 0.072273, PSNR: 11.410269, 2024-12-20_08-45-57\n",
      "Step: 708, MSE: 0.072281, PSNR: 11.409781, 2024-12-20_08-45-57\n",
      "Step: 709, MSE: 0.072274, PSNR: 11.410158, 2024-12-20_08-45-57\n",
      "Step: 710, MSE: 0.072227, PSNR: 11.413011, 2024-12-20_08-45-57\n",
      "Step: 711, MSE: 0.072291, PSNR: 11.409140, 2024-12-20_08-45-57\n",
      "Step: 712, MSE: 0.072318, PSNR: 11.407510, 2024-12-20_08-45-57\n",
      "Step: 713, MSE: 0.072265, PSNR: 11.410696, 2024-12-20_08-45-57\n",
      "Step: 714, MSE: 0.072321, PSNR: 11.407361, 2024-12-20_08-45-57\n",
      "Step: 715, MSE: 0.072309, PSNR: 11.408048, 2024-12-20_08-45-57\n",
      "Step: 716, MSE: 0.072320, PSNR: 11.407391, 2024-12-20_08-45-57\n",
      "Step: 717, MSE: 0.072278, PSNR: 11.409951, 2024-12-20_08-45-57\n",
      "Step: 718, MSE: 0.072301, PSNR: 11.408566, 2024-12-20_08-45-57\n",
      "Step: 719, MSE: 0.072429, PSNR: 11.400858, 2024-12-20_08-45-57\n",
      "Step: 720, MSE: 0.072388, PSNR: 11.403307, 2024-12-20_08-45-57\n",
      "Step: 721, MSE: 0.072247, PSNR: 11.411780, 2024-12-20_08-45-57\n",
      "Step: 722, MSE: 0.072317, PSNR: 11.407575, 2024-12-20_08-45-57\n",
      "Step: 723, MSE: 0.072406, PSNR: 11.402277, 2024-12-20_08-45-57\n",
      "Step: 724, MSE: 0.072268, PSNR: 11.410551, 2024-12-20_08-45-57\n",
      "Step: 725, MSE: 0.072280, PSNR: 11.409796, 2024-12-20_08-45-57\n",
      "Step: 726, MSE: 0.072319, PSNR: 11.407489, 2024-12-20_08-45-57\n",
      "Step: 727, MSE: 0.072379, PSNR: 11.403893, 2024-12-20_08-45-57\n",
      "Step: 728, MSE: 0.072476, PSNR: 11.398044, 2024-12-20_08-45-57\n",
      "Step: 729, MSE: 0.072384, PSNR: 11.403574, 2024-12-20_08-45-57\n",
      "Step: 730, MSE: 0.072262, PSNR: 11.410925, 2024-12-20_08-45-57\n",
      "Step: 731, MSE: 0.072360, PSNR: 11.405005, 2024-12-20_08-45-57\n",
      "Step: 732, MSE: 0.072303, PSNR: 11.408439, 2024-12-20_08-45-57\n",
      "Step: 733, MSE: 0.072241, PSNR: 11.412139, 2024-12-20_08-45-57\n",
      "Step: 734, MSE: 0.072326, PSNR: 11.407053, 2024-12-20_08-45-57\n",
      "Step: 735, MSE: 0.072338, PSNR: 11.406341, 2024-12-20_08-45-57\n",
      "Step: 736, MSE: 0.072343, PSNR: 11.406006, 2024-12-20_08-45-57\n",
      "Step: 737, MSE: 0.072398, PSNR: 11.402704, 2024-12-20_08-45-57\n",
      "Step: 738, MSE: 0.072371, PSNR: 11.404338, 2024-12-20_08-45-57\n",
      "Step: 739, MSE: 0.072338, PSNR: 11.406365, 2024-12-20_08-45-57\n",
      "Step: 740, MSE: 0.072255, PSNR: 11.411315, 2024-12-20_08-45-57\n",
      "Step: 741, MSE: 0.072262, PSNR: 11.410887, 2024-12-20_08-45-57\n",
      "Step: 742, MSE: 0.072363, PSNR: 11.404810, 2024-12-20_08-45-57\n",
      "Step: 743, MSE: 0.072313, PSNR: 11.407842, 2024-12-20_08-45-57\n",
      "Step: 744, MSE: 0.072322, PSNR: 11.407322, 2024-12-20_08-45-57\n",
      "Step: 745, MSE: 0.072332, PSNR: 11.406693, 2024-12-20_08-45-57\n",
      "Step: 746, MSE: 0.072303, PSNR: 11.408425, 2024-12-20_08-45-57\n",
      "Step: 747, MSE: 0.072234, PSNR: 11.412601, 2024-12-20_08-45-57\n",
      "Step: 748, MSE: 0.072309, PSNR: 11.408075, 2024-12-20_08-45-57\n",
      "Step: 749, MSE: 0.072391, PSNR: 11.403140, 2024-12-20_08-45-57\n",
      "Step: 750, MSE: 0.072276, PSNR: 11.410078, 2024-12-20_08-45-57\n",
      "Step: 751, MSE: 0.072208, PSNR: 11.414143, 2024-12-20_08-45-57\n",
      "Step: 752, MSE: 0.072333, PSNR: 11.406641, 2024-12-20_08-45-57\n",
      "Step: 753, MSE: 0.072407, PSNR: 11.402168, 2024-12-20_08-45-57\n",
      "Step: 754, MSE: 0.072419, PSNR: 11.401471, 2024-12-20_08-45-57\n",
      "Step: 755, MSE: 0.072235, PSNR: 11.412521, 2024-12-20_08-45-57\n",
      "Step: 756, MSE: 0.072394, PSNR: 11.402980, 2024-12-20_08-45-57\n",
      "Step: 757, MSE: 0.072376, PSNR: 11.404053, 2024-12-20_08-45-57\n",
      "Step: 758, MSE: 0.072331, PSNR: 11.406727, 2024-12-20_08-45-57\n",
      "Step: 759, MSE: 0.072374, PSNR: 11.404152, 2024-12-20_08-45-57\n",
      "Step: 760, MSE: 0.072417, PSNR: 11.401583, 2024-12-20_08-45-57\n",
      "Step: 761, MSE: 0.072344, PSNR: 11.405950, 2024-12-20_08-45-57\n",
      "Step: 762, MSE: 0.072271, PSNR: 11.410380, 2024-12-20_08-45-57\n",
      "Step: 763, MSE: 0.072267, PSNR: 11.410570, 2024-12-20_08-45-57\n",
      "Step: 764, MSE: 0.072279, PSNR: 11.409874, 2024-12-20_08-45-57\n",
      "Step: 765, MSE: 0.072268, PSNR: 11.410528, 2024-12-20_08-45-57\n",
      "Step: 766, MSE: 0.072339, PSNR: 11.406254, 2024-12-20_08-45-57\n",
      "Step: 767, MSE: 0.072388, PSNR: 11.403360, 2024-12-20_08-45-57\n",
      "Step: 768, MSE: 0.072213, PSNR: 11.413859, 2024-12-20_08-45-57\n",
      "Step: 769, MSE: 0.072399, PSNR: 11.402664, 2024-12-20_08-45-57\n",
      "Step: 770, MSE: 0.072261, PSNR: 11.410975, 2024-12-20_08-45-57\n",
      "Step: 771, MSE: 0.072371, PSNR: 11.404333, 2024-12-20_08-45-57\n",
      "Step: 772, MSE: 0.072181, PSNR: 11.415791, 2024-12-20_08-45-57\n",
      "Step: 773, MSE: 0.072261, PSNR: 11.410971, 2024-12-20_08-45-57\n",
      "Step: 774, MSE: 0.072334, PSNR: 11.406597, 2024-12-20_08-45-57\n",
      "Step: 775, MSE: 0.072343, PSNR: 11.406041, 2024-12-20_08-45-57\n",
      "Step: 776, MSE: 0.072245, PSNR: 11.411940, 2024-12-20_08-45-57\n",
      "Step: 777, MSE: 0.072261, PSNR: 11.410938, 2024-12-20_08-45-57\n",
      "Step: 778, MSE: 0.072359, PSNR: 11.405047, 2024-12-20_08-45-57\n",
      "Step: 779, MSE: 0.072332, PSNR: 11.406704, 2024-12-20_08-45-57\n",
      "Step: 780, MSE: 0.072285, PSNR: 11.409496, 2024-12-20_08-45-57\n",
      "Step: 781, MSE: 0.072313, PSNR: 11.407842, 2024-12-20_08-45-57\n",
      "Step: 782, MSE: 0.072255, PSNR: 11.411325, 2024-12-20_08-45-57\n",
      "Step: 783, MSE: 0.072223, PSNR: 11.413274, 2024-12-20_08-45-57\n",
      "Step: 784, MSE: 0.072175, PSNR: 11.416124, 2024-12-20_08-45-57\n",
      "Step: 785, MSE: 0.072223, PSNR: 11.413241, 2024-12-20_08-45-57\n",
      "Step: 786, MSE: 0.072471, PSNR: 11.398387, 2024-12-20_08-45-57\n",
      "Step: 787, MSE: 0.072313, PSNR: 11.407822, 2024-12-20_08-45-57\n",
      "Step: 788, MSE: 0.072275, PSNR: 11.410110, 2024-12-20_08-45-57\n",
      "Step: 789, MSE: 0.072407, PSNR: 11.402172, 2024-12-20_08-45-57\n",
      "Step: 790, MSE: 0.072296, PSNR: 11.408865, 2024-12-20_08-45-57\n",
      "Step: 791, MSE: 0.072376, PSNR: 11.404034, 2024-12-20_08-45-57\n",
      "Step: 792, MSE: 0.072444, PSNR: 11.399998, 2024-12-20_08-45-57\n",
      "Step: 793, MSE: 0.072406, PSNR: 11.402231, 2024-12-20_08-45-57\n",
      "Step: 794, MSE: 0.072378, PSNR: 11.403948, 2024-12-20_08-45-57\n",
      "Step: 795, MSE: 0.072366, PSNR: 11.404667, 2024-12-20_08-45-57\n",
      "Step: 796, MSE: 0.072242, PSNR: 11.412123, 2024-12-20_08-45-57\n",
      "Step: 797, MSE: 0.072246, PSNR: 11.411884, 2024-12-20_08-45-57\n",
      "Step: 798, MSE: 0.072209, PSNR: 11.414062, 2024-12-20_08-45-57\n",
      "Step: 799, MSE: 0.072286, PSNR: 11.409462, 2024-12-20_08-45-57\n",
      "Step: 800, MSE: 0.072315, PSNR: 11.407743, 2024-12-20_08-45-57\n",
      "Step: 801, MSE: 0.072461, PSNR: 11.398928, 2024-12-20_08-45-57\n",
      "Step: 802, MSE: 0.072231, PSNR: 11.412786, 2024-12-20_08-45-57\n",
      "Step: 803, MSE: 0.072273, PSNR: 11.410229, 2024-12-20_08-45-57\n",
      "Step: 804, MSE: 0.072376, PSNR: 11.404036, 2024-12-20_08-45-57\n",
      "Step: 805, MSE: 0.072399, PSNR: 11.402665, 2024-12-20_08-45-57\n",
      "Step: 806, MSE: 0.072219, PSNR: 11.413507, 2024-12-20_08-45-57\n",
      "Step: 807, MSE: 0.072393, PSNR: 11.403061, 2024-12-20_08-45-57\n",
      "Step: 808, MSE: 0.072312, PSNR: 11.407877, 2024-12-20_08-45-57\n",
      "Step: 809, MSE: 0.072391, PSNR: 11.403171, 2024-12-20_08-45-57\n",
      "Step: 810, MSE: 0.072387, PSNR: 11.403378, 2024-12-20_08-45-57\n",
      "Step: 811, MSE: 0.072321, PSNR: 11.407351, 2024-12-20_08-45-57\n",
      "Step: 812, MSE: 0.072298, PSNR: 11.408738, 2024-12-20_08-45-57\n",
      "Step: 813, MSE: 0.072371, PSNR: 11.404327, 2024-12-20_08-45-57\n",
      "Step: 814, MSE: 0.072285, PSNR: 11.409494, 2024-12-20_08-45-57\n",
      "Step: 815, MSE: 0.072411, PSNR: 11.401925, 2024-12-20_08-45-57\n",
      "Step: 816, MSE: 0.072281, PSNR: 11.409781, 2024-12-20_08-45-57\n",
      "Step: 817, MSE: 0.072272, PSNR: 11.410298, 2024-12-20_08-45-57\n",
      "Step: 818, MSE: 0.072265, PSNR: 11.410709, 2024-12-20_08-45-57\n",
      "Step: 819, MSE: 0.072388, PSNR: 11.403342, 2024-12-20_08-45-57\n",
      "Step: 820, MSE: 0.072183, PSNR: 11.415658, 2024-12-20_08-45-57\n",
      "Step: 821, MSE: 0.072292, PSNR: 11.409116, 2024-12-20_08-45-57\n",
      "Step: 822, MSE: 0.072208, PSNR: 11.414127, 2024-12-20_08-45-57\n",
      "Step: 823, MSE: 0.072401, PSNR: 11.402546, 2024-12-20_08-45-57\n",
      "Step: 824, MSE: 0.072378, PSNR: 11.403940, 2024-12-20_08-45-57\n",
      "Step: 825, MSE: 0.072431, PSNR: 11.400767, 2024-12-20_08-45-57\n",
      "Step: 826, MSE: 0.072276, PSNR: 11.410080, 2024-12-20_08-45-57\n",
      "Step: 827, MSE: 0.072353, PSNR: 11.405440, 2024-12-20_08-45-57\n",
      "Step: 828, MSE: 0.072401, PSNR: 11.402554, 2024-12-20_08-45-57\n",
      "Step: 829, MSE: 0.072272, PSNR: 11.410299, 2024-12-20_08-45-57\n",
      "Step: 830, MSE: 0.072295, PSNR: 11.408896, 2024-12-20_08-45-57\n",
      "Step: 831, MSE: 0.072394, PSNR: 11.402952, 2024-12-20_08-45-57\n",
      "Step: 832, MSE: 0.072208, PSNR: 11.414139, 2024-12-20_08-45-57\n",
      "Step: 833, MSE: 0.072376, PSNR: 11.404060, 2024-12-20_08-45-57\n",
      "Step: 834, MSE: 0.072326, PSNR: 11.407037, 2024-12-20_08-45-57\n",
      "Step: 835, MSE: 0.072375, PSNR: 11.404121, 2024-12-20_08-45-57\n",
      "Step: 836, MSE: 0.072219, PSNR: 11.413471, 2024-12-20_08-45-57\n",
      "Step: 837, MSE: 0.072260, PSNR: 11.411019, 2024-12-20_08-45-57\n",
      "Step: 838, MSE: 0.072263, PSNR: 11.410847, 2024-12-20_08-45-57\n",
      "Step: 839, MSE: 0.072328, PSNR: 11.406923, 2024-12-20_08-45-57\n",
      "Step: 840, MSE: 0.072190, PSNR: 11.415215, 2024-12-20_08-45-57\n",
      "Step: 841, MSE: 0.072216, PSNR: 11.413641, 2024-12-20_08-45-57\n",
      "Step: 842, MSE: 0.072365, PSNR: 11.404736, 2024-12-20_08-45-57\n",
      "Step: 843, MSE: 0.072409, PSNR: 11.402046, 2024-12-20_08-45-57\n",
      "Step: 844, MSE: 0.072209, PSNR: 11.414089, 2024-12-20_08-45-57\n",
      "Step: 845, MSE: 0.072466, PSNR: 11.398640, 2024-12-20_08-45-57\n",
      "Step: 846, MSE: 0.072285, PSNR: 11.409544, 2024-12-20_08-45-57\n",
      "Step: 847, MSE: 0.072255, PSNR: 11.411351, 2024-12-20_08-45-57\n",
      "Step: 848, MSE: 0.072282, PSNR: 11.409722, 2024-12-20_08-45-57\n",
      "Step: 849, MSE: 0.072346, PSNR: 11.405874, 2024-12-20_08-45-57\n",
      "Step: 850, MSE: 0.072373, PSNR: 11.404237, 2024-12-20_08-45-57\n",
      "Step: 851, MSE: 0.072418, PSNR: 11.401552, 2024-12-20_08-45-57\n",
      "Step: 852, MSE: 0.072423, PSNR: 11.401223, 2024-12-20_08-45-57\n",
      "Step: 853, MSE: 0.072233, PSNR: 11.412624, 2024-12-20_08-45-57\n",
      "Step: 854, MSE: 0.072328, PSNR: 11.406906, 2024-12-20_08-45-57\n",
      "Step: 855, MSE: 0.072333, PSNR: 11.406611, 2024-12-20_08-45-57\n",
      "Step: 856, MSE: 0.072226, PSNR: 11.413083, 2024-12-20_08-45-57\n",
      "Step: 857, MSE: 0.072271, PSNR: 11.410334, 2024-12-20_08-45-57\n",
      "Step: 858, MSE: 0.072348, PSNR: 11.405746, 2024-12-20_08-45-57\n",
      "Step: 859, MSE: 0.072269, PSNR: 11.410484, 2024-12-20_08-45-57\n",
      "Step: 860, MSE: 0.072262, PSNR: 11.410913, 2024-12-20_08-45-57\n",
      "Step: 861, MSE: 0.072267, PSNR: 11.410628, 2024-12-20_08-45-57\n",
      "Step: 862, MSE: 0.072147, PSNR: 11.417791, 2024-12-20_08-45-57\n",
      "Step: 863, MSE: 0.072331, PSNR: 11.406750, 2024-12-20_08-45-57\n",
      "Step: 864, MSE: 0.072424, PSNR: 11.401188, 2024-12-20_08-45-57\n",
      "Step: 865, MSE: 0.072343, PSNR: 11.406013, 2024-12-20_08-45-57\n",
      "Step: 866, MSE: 0.072336, PSNR: 11.406451, 2024-12-20_08-45-57\n",
      "Step: 867, MSE: 0.072330, PSNR: 11.406823, 2024-12-20_08-45-57\n",
      "Step: 868, MSE: 0.072337, PSNR: 11.406403, 2024-12-20_08-45-57\n",
      "Step: 869, MSE: 0.072393, PSNR: 11.403041, 2024-12-20_08-45-57\n",
      "Step: 870, MSE: 0.072319, PSNR: 11.407486, 2024-12-20_08-45-57\n",
      "Step: 871, MSE: 0.072345, PSNR: 11.405901, 2024-12-20_08-45-57\n",
      "Step: 872, MSE: 0.072203, PSNR: 11.414456, 2024-12-20_08-45-57\n",
      "Step: 873, MSE: 0.072256, PSNR: 11.411289, 2024-12-20_08-45-57\n",
      "Step: 874, MSE: 0.072375, PSNR: 11.404119, 2024-12-20_08-45-57\n",
      "Step: 875, MSE: 0.072281, PSNR: 11.409746, 2024-12-20_08-45-57\n",
      "Step: 876, MSE: 0.072403, PSNR: 11.402422, 2024-12-20_08-45-57\n",
      "Step: 877, MSE: 0.072238, PSNR: 11.412361, 2024-12-20_08-45-57\n",
      "Step: 878, MSE: 0.072291, PSNR: 11.409185, 2024-12-20_08-45-57\n",
      "Step: 879, MSE: 0.072353, PSNR: 11.405434, 2024-12-20_08-45-57\n",
      "Step: 880, MSE: 0.072293, PSNR: 11.409035, 2024-12-20_08-45-57\n",
      "Step: 881, MSE: 0.072281, PSNR: 11.409787, 2024-12-20_08-45-57\n",
      "Step: 882, MSE: 0.072328, PSNR: 11.406936, 2024-12-20_08-45-57\n",
      "Step: 883, MSE: 0.072265, PSNR: 11.410725, 2024-12-20_08-45-57\n",
      "Step: 884, MSE: 0.072299, PSNR: 11.408682, 2024-12-20_08-45-57\n",
      "Step: 885, MSE: 0.072334, PSNR: 11.406547, 2024-12-20_08-45-57\n",
      "Step: 886, MSE: 0.072417, PSNR: 11.401594, 2024-12-20_08-45-57\n",
      "Step: 887, MSE: 0.072300, PSNR: 11.408609, 2024-12-20_08-45-57\n",
      "Step: 888, MSE: 0.072356, PSNR: 11.405272, 2024-12-20_08-45-57\n",
      "Step: 889, MSE: 0.072302, PSNR: 11.408525, 2024-12-20_08-45-57\n",
      "Step: 890, MSE: 0.072337, PSNR: 11.406389, 2024-12-20_08-45-57\n",
      "Step: 891, MSE: 0.072357, PSNR: 11.405216, 2024-12-20_08-45-57\n",
      "Step: 892, MSE: 0.072222, PSNR: 11.413311, 2024-12-20_08-45-57\n",
      "Step: 893, MSE: 0.072375, PSNR: 11.404119, 2024-12-20_08-45-57\n",
      "Step: 894, MSE: 0.072323, PSNR: 11.407221, 2024-12-20_08-45-57\n",
      "Step: 895, MSE: 0.072176, PSNR: 11.416054, 2024-12-20_08-45-57\n",
      "Step: 896, MSE: 0.072260, PSNR: 11.410994, 2024-12-20_08-45-57\n",
      "Step: 897, MSE: 0.072182, PSNR: 11.415701, 2024-12-20_08-45-57\n",
      "Step: 898, MSE: 0.072304, PSNR: 11.408405, 2024-12-20_08-45-57\n",
      "Step: 899, MSE: 0.072310, PSNR: 11.408000, 2024-12-20_08-45-57\n",
      "Step: 900, MSE: 0.072332, PSNR: 11.406685, 2024-12-20_08-45-57\n",
      "Step: 901, MSE: 0.072356, PSNR: 11.405241, 2024-12-20_08-45-57\n",
      "Step: 902, MSE: 0.072254, PSNR: 11.411361, 2024-12-20_08-45-57\n",
      "Step: 903, MSE: 0.072202, PSNR: 11.414503, 2024-12-20_08-45-57\n",
      "Step: 904, MSE: 0.072351, PSNR: 11.405535, 2024-12-20_08-45-57\n",
      "Step: 905, MSE: 0.072382, PSNR: 11.403666, 2024-12-20_08-45-57\n",
      "Step: 906, MSE: 0.072234, PSNR: 11.412588, 2024-12-20_08-45-57\n",
      "Step: 907, MSE: 0.072381, PSNR: 11.403730, 2024-12-20_08-45-57\n",
      "Step: 908, MSE: 0.072406, PSNR: 11.402230, 2024-12-20_08-45-57\n",
      "Step: 909, MSE: 0.072141, PSNR: 11.418162, 2024-12-20_08-45-57\n",
      "Step: 910, MSE: 0.072286, PSNR: 11.409449, 2024-12-20_08-45-57\n",
      "Step: 911, MSE: 0.072327, PSNR: 11.407005, 2024-12-20_08-45-57\n",
      "Step: 912, MSE: 0.072261, PSNR: 11.410937, 2024-12-20_08-45-57\n",
      "Step: 913, MSE: 0.072243, PSNR: 11.412039, 2024-12-20_08-45-57\n",
      "Step: 914, MSE: 0.072316, PSNR: 11.407671, 2024-12-20_08-45-57\n",
      "Step: 915, MSE: 0.072426, PSNR: 11.401044, 2024-12-20_08-45-57\n",
      "Step: 916, MSE: 0.072182, PSNR: 11.415712, 2024-12-20_08-45-57\n",
      "Step: 917, MSE: 0.072323, PSNR: 11.407217, 2024-12-20_08-45-57\n",
      "Step: 918, MSE: 0.072388, PSNR: 11.403313, 2024-12-20_08-45-57\n",
      "Step: 919, MSE: 0.072386, PSNR: 11.403445, 2024-12-20_08-45-57\n",
      "Step: 920, MSE: 0.072355, PSNR: 11.405321, 2024-12-20_08-45-57\n",
      "Step: 921, MSE: 0.072304, PSNR: 11.408376, 2024-12-20_08-45-57\n",
      "Step: 922, MSE: 0.072192, PSNR: 11.415089, 2024-12-20_08-45-57\n",
      "Step: 923, MSE: 0.072195, PSNR: 11.414948, 2024-12-20_08-45-57\n",
      "Step: 924, MSE: 0.072264, PSNR: 11.410780, 2024-12-20_08-45-57\n",
      "Step: 925, MSE: 0.072346, PSNR: 11.405857, 2024-12-20_08-45-57\n",
      "Step: 926, MSE: 0.072373, PSNR: 11.404205, 2024-12-20_08-45-57\n",
      "Step: 927, MSE: 0.072253, PSNR: 11.411453, 2024-12-20_08-45-57\n",
      "Step: 928, MSE: 0.072197, PSNR: 11.414838, 2024-12-20_08-45-57\n",
      "Step: 929, MSE: 0.072455, PSNR: 11.399344, 2024-12-20_08-45-57\n",
      "Step: 930, MSE: 0.072299, PSNR: 11.408652, 2024-12-20_08-45-57\n",
      "Step: 931, MSE: 0.072375, PSNR: 11.404133, 2024-12-20_08-45-57\n",
      "Step: 932, MSE: 0.072190, PSNR: 11.415216, 2024-12-20_08-45-57\n",
      "Step: 933, MSE: 0.072205, PSNR: 11.414312, 2024-12-20_08-45-57\n",
      "Step: 934, MSE: 0.072301, PSNR: 11.408582, 2024-12-20_08-45-57\n",
      "Step: 935, MSE: 0.072269, PSNR: 11.410496, 2024-12-20_08-45-57\n",
      "Step: 936, MSE: 0.072328, PSNR: 11.406942, 2024-12-20_08-45-57\n",
      "Step: 937, MSE: 0.072355, PSNR: 11.405343, 2024-12-20_08-45-57\n",
      "Step: 938, MSE: 0.072424, PSNR: 11.401162, 2024-12-20_08-45-57\n",
      "Step: 939, MSE: 0.072207, PSNR: 11.414208, 2024-12-20_08-45-57\n",
      "Step: 940, MSE: 0.072336, PSNR: 11.406461, 2024-12-20_08-45-57\n",
      "Step: 941, MSE: 0.072286, PSNR: 11.409445, 2024-12-20_08-45-57\n",
      "Step: 942, MSE: 0.072355, PSNR: 11.405331, 2024-12-20_08-45-57\n",
      "Step: 943, MSE: 0.072298, PSNR: 11.408722, 2024-12-20_08-45-57\n",
      "Step: 944, MSE: 0.072334, PSNR: 11.406575, 2024-12-20_08-45-57\n",
      "Step: 945, MSE: 0.072409, PSNR: 11.402087, 2024-12-20_08-45-57\n",
      "Step: 946, MSE: 0.072249, PSNR: 11.411665, 2024-12-20_08-45-57\n",
      "Step: 947, MSE: 0.072314, PSNR: 11.407785, 2024-12-20_08-45-57\n",
      "Step: 948, MSE: 0.072370, PSNR: 11.404402, 2024-12-20_08-45-57\n",
      "Step: 949, MSE: 0.072328, PSNR: 11.406936, 2024-12-20_08-45-57\n",
      "Step: 950, MSE: 0.072281, PSNR: 11.409780, 2024-12-20_08-45-57\n",
      "Step: 951, MSE: 0.072306, PSNR: 11.408267, 2024-12-20_08-45-57\n",
      "Step: 952, MSE: 0.072235, PSNR: 11.412537, 2024-12-20_08-45-57\n",
      "Step: 953, MSE: 0.072325, PSNR: 11.407127, 2024-12-20_08-45-57\n",
      "Step: 954, MSE: 0.072280, PSNR: 11.409816, 2024-12-20_08-45-57\n",
      "Step: 955, MSE: 0.072384, PSNR: 11.403563, 2024-12-20_08-45-57\n",
      "Step: 956, MSE: 0.072237, PSNR: 11.412395, 2024-12-20_08-45-57\n",
      "Step: 957, MSE: 0.072317, PSNR: 11.407606, 2024-12-20_08-45-57\n",
      "Step: 958, MSE: 0.072320, PSNR: 11.407387, 2024-12-20_08-45-57\n",
      "Step: 959, MSE: 0.072288, PSNR: 11.409367, 2024-12-20_08-45-57\n",
      "Step: 960, MSE: 0.072297, PSNR: 11.408817, 2024-12-20_08-45-57\n",
      "Step: 961, MSE: 0.072236, PSNR: 11.412457, 2024-12-20_08-45-57\n",
      "Step: 962, MSE: 0.072298, PSNR: 11.408722, 2024-12-20_08-45-57\n",
      "Step: 963, MSE: 0.072244, PSNR: 11.411980, 2024-12-20_08-45-57\n",
      "Step: 964, MSE: 0.072369, PSNR: 11.404468, 2024-12-20_08-45-57\n",
      "Step: 965, MSE: 0.072382, PSNR: 11.403685, 2024-12-20_08-45-57\n",
      "Step: 966, MSE: 0.072344, PSNR: 11.406004, 2024-12-20_08-45-57\n",
      "Step: 967, MSE: 0.072293, PSNR: 11.409008, 2024-12-20_08-45-57\n",
      "Step: 968, MSE: 0.072319, PSNR: 11.407479, 2024-12-20_08-45-57\n",
      "Step: 969, MSE: 0.072293, PSNR: 11.409016, 2024-12-20_08-45-57\n",
      "Step: 970, MSE: 0.072301, PSNR: 11.408538, 2024-12-20_08-45-57\n",
      "Step: 971, MSE: 0.072438, PSNR: 11.400357, 2024-12-20_08-45-57\n",
      "Step: 972, MSE: 0.072295, PSNR: 11.408935, 2024-12-20_08-45-57\n",
      "Step: 973, MSE: 0.072391, PSNR: 11.403171, 2024-12-20_08-45-57\n",
      "Step: 974, MSE: 0.072312, PSNR: 11.407890, 2024-12-20_08-45-57\n",
      "Step: 975, MSE: 0.072381, PSNR: 11.403725, 2024-12-20_08-45-57\n",
      "Step: 976, MSE: 0.072242, PSNR: 11.412096, 2024-12-20_08-45-57\n",
      "Step: 977, MSE: 0.072335, PSNR: 11.406542, 2024-12-20_08-45-57\n",
      "Step: 978, MSE: 0.072432, PSNR: 11.400688, 2024-12-20_08-45-57\n",
      "Step: 979, MSE: 0.072351, PSNR: 11.405552, 2024-12-20_08-45-57\n",
      "Step: 980, MSE: 0.072260, PSNR: 11.411000, 2024-12-20_08-45-57\n",
      "Step: 981, MSE: 0.072294, PSNR: 11.408972, 2024-12-20_08-45-57\n",
      "Step: 982, MSE: 0.072337, PSNR: 11.406378, 2024-12-20_08-45-57\n",
      "Step: 983, MSE: 0.072263, PSNR: 11.410830, 2024-12-20_08-45-57\n",
      "Step: 984, MSE: 0.072295, PSNR: 11.408890, 2024-12-20_08-45-57\n",
      "Step: 985, MSE: 0.072377, PSNR: 11.404022, 2024-12-20_08-45-57\n",
      "Step: 986, MSE: 0.072257, PSNR: 11.411180, 2024-12-20_08-45-57\n",
      "Step: 987, MSE: 0.072357, PSNR: 11.405207, 2024-12-20_08-45-57\n",
      "Step: 988, MSE: 0.072268, PSNR: 11.410518, 2024-12-20_08-45-57\n",
      "Step: 989, MSE: 0.072349, PSNR: 11.405683, 2024-12-20_08-45-57\n",
      "Step: 990, MSE: 0.072259, PSNR: 11.411087, 2024-12-20_08-45-57\n",
      "Step: 991, MSE: 0.072115, PSNR: 11.419763, 2024-12-20_08-45-57\n",
      "Step: 992, MSE: 0.072300, PSNR: 11.408590, 2024-12-20_08-45-57\n",
      "Step: 993, MSE: 0.072285, PSNR: 11.409542, 2024-12-20_08-45-57\n",
      "Step: 994, MSE: 0.072238, PSNR: 11.412365, 2024-12-20_08-45-57\n",
      "Step: 995, MSE: 0.072259, PSNR: 11.411100, 2024-12-20_08-45-57\n",
      "Step: 996, MSE: 0.072266, PSNR: 11.410664, 2024-12-20_08-45-57\n",
      "Step: 997, MSE: 0.072277, PSNR: 11.409980, 2024-12-20_08-45-57\n",
      "Step: 998, MSE: 0.072383, PSNR: 11.403657, 2024-12-20_08-45-57\n",
      "Step: 999, MSE: 0.072279, PSNR: 11.409904, 2024-12-20_08-45-57\n",
      "Step: 1000, MSE: 0.072260, PSNR: 11.411046, 2024-12-20_08-45-57\n",
      "Step: 1001, MSE: 0.072297, PSNR: 11.408773, 2024-12-20_08-45-57\n",
      "Step: 1002, MSE: 0.072307, PSNR: 11.408169, 2024-12-20_08-45-57\n",
      "Step: 1003, MSE: 0.072457, PSNR: 11.399206, 2024-12-20_08-45-57\n",
      "Step: 1004, MSE: 0.072226, PSNR: 11.413050, 2024-12-20_08-45-57\n",
      "Step: 1005, MSE: 0.072386, PSNR: 11.403463, 2024-12-20_08-45-57\n",
      "Step: 1006, MSE: 0.072388, PSNR: 11.403334, 2024-12-20_08-45-57\n",
      "Step: 1007, MSE: 0.072389, PSNR: 11.403283, 2024-12-20_08-45-57\n",
      "Step: 1008, MSE: 0.072411, PSNR: 11.401973, 2024-12-20_08-45-57\n",
      "Step: 1009, MSE: 0.072341, PSNR: 11.406155, 2024-12-20_08-45-57\n",
      "Step: 1010, MSE: 0.072115, PSNR: 11.419722, 2024-12-20_08-45-57\n",
      "Step: 1011, MSE: 0.072276, PSNR: 11.410053, 2024-12-20_08-45-57\n",
      "Step: 1012, MSE: 0.072252, PSNR: 11.411505, 2024-12-20_08-45-57\n",
      "Step: 1013, MSE: 0.072246, PSNR: 11.411867, 2024-12-20_08-45-57\n",
      "Step: 1014, MSE: 0.072308, PSNR: 11.408161, 2024-12-20_08-45-57\n",
      "Step: 1015, MSE: 0.072370, PSNR: 11.404438, 2024-12-20_08-45-57\n",
      "Step: 1016, MSE: 0.072360, PSNR: 11.405037, 2024-12-20_08-45-57\n",
      "Step: 1017, MSE: 0.072367, PSNR: 11.404612, 2024-12-20_08-45-57\n",
      "Step: 1018, MSE: 0.072325, PSNR: 11.407120, 2024-12-20_08-45-57\n",
      "Step: 1019, MSE: 0.072360, PSNR: 11.405030, 2024-12-20_08-45-57\n",
      "Step: 1020, MSE: 0.072339, PSNR: 11.406246, 2024-12-20_08-45-57\n",
      "Step: 1021, MSE: 0.072262, PSNR: 11.410914, 2024-12-20_08-45-57\n",
      "Step: 1022, MSE: 0.072373, PSNR: 11.404232, 2024-12-20_08-45-57\n",
      "Step: 1023, MSE: 0.072291, PSNR: 11.409180, 2024-12-20_08-45-57\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 271  |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.67 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 429\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# PPO 학습\u001b[39;00m\n\u001b[1;32m    418\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    420\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    432\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.67 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 1024, 1024).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(1024)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((1024, 1024))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 1024 or target.shape[-2] < 1024:\n",
    "            target = torchvision.transforms.Resize(1024)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "#BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=100000, T_PSNR=30, T_steps=1000):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 1024, 1024)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: MultiBinary 데이터\n",
    "        self.action_space = spaces.MultiBinary(1 * 8 * 1024 * 1024)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        # 학습 상태\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            self.target_image = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image = next(self.data_iter)\n",
    "    \n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 첫 스텝에서 초기 상태와 동일한 행동 적용\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1  # 스텝 증가\n",
    "            # reset과 동일한 로직을 호출해 초기 상태 생성\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        # 행동(action)을 이진 상태(state)와 동일한 형식으로 변환\n",
    "        action = np.reshape(action, (1, 8, 1024, 1024)).astype(np.int8)\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, action).astype(np.float32)\n",
    "\n",
    "        # 이진화된 새로운 상태를 torch 텐서로 변환\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "        reward = -mse\n",
    "\n",
    "        # 출력 추가\n",
    "        print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        # 상태 업데이트\n",
    "        self.state = new_state\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "\n",
    "        # 종료 조건\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 행동 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        관찰값이 0~0.2인 경우 -> 행동 0으로 고정.\n",
    "        관찰값이 0.8~1인 경우 -> 행동 1로 고정.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "        return mask\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=100000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=1000\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
