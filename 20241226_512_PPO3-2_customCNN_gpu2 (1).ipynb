{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 메시지는 콘솔과 파일에 동시에 기록됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "로깅 메시지입니다.\n",
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n",
      "Using cuda device\n",
      "\u001b[91mResetting environment. Consecutive episode failures: 0, Max consecutive episode failures: 0\u001b[0m\n",
      "\u001b[92mInitial MSE: 0.001446, Initial PSNR: 27.761915, 11:10:43\u001b[0m\n",
      "Logging to ./ppo_custom_policy/run_4\n",
      "\u001b[94mStep: 5, PSNR Before: 27.761915, PSNR After: 27.762030, PSNR Change: 0.000114, PSNR Diff: 0.000114 (New Max), Reward: 0.09, 11:10:49 Pre-flip Model Output=0.016652, New State Value=1, Flip Count=1\u001b[0m\n",
      "\u001b[94mStep: 34, PSNR Before: 27.762030, PSNR After: 27.762035, PSNR Change: 0.000006, PSNR Diff: 0.000120 (New Max), Reward: 0.10, 11:11:13 Pre-flip Model Output=0.255861, New State Value=1, Flip Count=3\u001b[0m\n",
      "\u001b[94mStep: 47, PSNR Before: 27.762035, PSNR After: 27.762066, PSNR Change: 0.000031, PSNR Diff: 0.000151 (New Max), Reward: 0.12, 11:11:24 Pre-flip Model Output=0.379479, New State Value=1, Flip Count=4\u001b[0m\n",
      "\u001b[94mStep: 49, PSNR Before: 27.762066, PSNR After: 27.762150, PSNR Change: 0.000084, PSNR Diff: 0.000235 (New Max), Reward: 0.19, 11:11:26 Pre-flip Model Output=0.197126, New State Value=1, Flip Count=5\u001b[0m\n",
      "\u001b[94mStep: 59, PSNR Before: 27.762150, PSNR After: 27.762220, PSNR Change: 0.000071, PSNR Diff: 0.000305 (New Max), Reward: 0.24, 11:11:35 Pre-flip Model Output=0.631642, New State Value=0, Flip Count=6\u001b[0m\n",
      "\u001b[94mStep: 68, PSNR Before: 27.762220, PSNR After: 27.762224, PSNR Change: 0.000004, PSNR Diff: 0.000309 (New Max), Reward: 0.25, 11:11:42 Pre-flip Model Output=0.457970, New State Value=1, Flip Count=7\u001b[0m\n",
      "\u001b[94mStep: 70, PSNR Before: 27.762224, PSNR After: 27.762314, PSNR Change: 0.000090, PSNR Diff: 0.000399 (New Max), Reward: 0.32, 11:11:45 Pre-flip Model Output=0.051114, New State Value=1, Flip Count=8\u001b[0m\n",
      "\u001b[94mStep: 78, PSNR Before: 27.762314, PSNR After: 27.762363, PSNR Change: 0.000050, PSNR Diff: 0.000448 (New Max), Reward: 0.36, 11:11:51 Pre-flip Model Output=0.208790, New State Value=1, Flip Count=9\u001b[0m\n",
      "\u001b[94mStep: 102, PSNR Before: 27.762363, PSNR After: 27.762367, PSNR Change: 0.000004, PSNR Diff: 0.000452 (New Max), Reward: 0.36, 11:12:11 Pre-flip Model Output=0.013477, New State Value=1, Flip Count=10\u001b[0m\n",
      "\u001b[94mStep: 103, PSNR Before: 27.762367, PSNR After: 27.762390, PSNR Change: 0.000023, PSNR Diff: 0.000475 (New Max), Reward: 0.38, 11:12:12 Pre-flip Model Output=0.052870, New State Value=1, Flip Count=11\u001b[0m\n",
      "\u001b[94mStep: 104, PSNR Before: 27.762390, PSNR After: 27.762482, PSNR Change: 0.000092, PSNR Diff: 0.000566 (New Max), Reward: 0.45, 11:12:14 Pre-flip Model Output=0.260683, New State Value=1, Flip Count=12\u001b[0m\n",
      "\u001b[94mStep: 110, PSNR Before: 27.762482, PSNR After: 27.762489, PSNR Change: 0.000008, PSNR Diff: 0.000574 (New Max), Reward: 0.46, 11:12:19 Pre-flip Model Output=0.505107, New State Value=0, Flip Count=13\u001b[0m\n",
      "\u001b[94mStep: 114, PSNR Before: 27.762489, PSNR After: 27.762543, PSNR Change: 0.000053, PSNR Diff: 0.000628 (New Max), Reward: 0.50, 11:12:23 Pre-flip Model Output=0.720982, New State Value=0, Flip Count=14\u001b[0m\n",
      "\u001b[94mStep: 115, PSNR Before: 27.762543, PSNR After: 27.762602, PSNR Change: 0.000059, PSNR Diff: 0.000687 (New Max), Reward: 0.55, 11:12:24 Pre-flip Model Output=0.366622, New State Value=1, Flip Count=15\u001b[0m\n",
      "\u001b[94mStep: 116, PSNR Before: 27.762602, PSNR After: 27.762663, PSNR Change: 0.000061, PSNR Diff: 0.000748 (New Max), Reward: 0.60, 11:12:26 Pre-flip Model Output=0.964682, New State Value=0, Flip Count=16\u001b[0m\n",
      "\u001b[94mStep: 123, PSNR Before: 27.762663, PSNR After: 27.762665, PSNR Change: 0.000002, PSNR Diff: 0.000750 (New Max), Reward: 0.60, 11:12:32 Pre-flip Model Output=0.005878, New State Value=1, Flip Count=17\u001b[0m\n",
      "\u001b[94mStep: 129, PSNR Before: 27.762665, PSNR After: 27.762671, PSNR Change: 0.000006, PSNR Diff: 0.000755 (New Max), Reward: 0.60, 11:12:37 Pre-flip Model Output=0.193320, New State Value=1, Flip Count=18\u001b[0m\n",
      "\u001b[94mStep: 131, PSNR Before: 27.762671, PSNR After: 27.762714, PSNR Change: 0.000044, PSNR Diff: 0.000799 (New Max), Reward: 0.64, 11:12:39 Pre-flip Model Output=0.572280, New State Value=0, Flip Count=19\u001b[0m\n",
      "\u001b[94mStep: 133, PSNR Before: 27.762714, PSNR After: 27.762802, PSNR Change: 0.000088, PSNR Diff: 0.000887 (New Max), Reward: 0.71, 11:12:42 Pre-flip Model Output=0.224055, New State Value=1, Flip Count=20\u001b[0m\n",
      "\u001b[94mStep: 134, PSNR Before: 27.762802, PSNR After: 27.762829, PSNR Change: 0.000027, PSNR Diff: 0.000914 (New Max), Reward: 0.73, 11:12:43 Pre-flip Model Output=0.420442, New State Value=1, Flip Count=21\u001b[0m\n",
      "\u001b[94mStep: 137, PSNR Before: 27.762829, PSNR After: 27.762831, PSNR Change: 0.000002, PSNR Diff: 0.000916 (New Max), Reward: 0.73, 11:12:46 Pre-flip Model Output=0.175416, New State Value=1, Flip Count=22\u001b[0m\n",
      "\u001b[94mStep: 138, PSNR Before: 27.762831, PSNR After: 27.762842, PSNR Change: 0.000011, PSNR Diff: 0.000927 (New Max), Reward: 0.74, 11:12:47 Pre-flip Model Output=0.565723, New State Value=0, Flip Count=23\u001b[0m\n",
      "\u001b[94mStep: 142, PSNR Before: 27.762842, PSNR After: 27.762848, PSNR Change: 0.000006, PSNR Diff: 0.000933 (New Max), Reward: 0.75, 11:12:51 Pre-flip Model Output=0.000362, New State Value=1, Flip Count=24\u001b[0m\n",
      "\u001b[94mStep: 153, PSNR Before: 27.762848, PSNR After: 27.762951, PSNR Change: 0.000103, PSNR Diff: 0.001036 (New Max), Reward: 0.83, 11:13:00 Pre-flip Model Output=0.553865, New State Value=0, Flip Count=25\u001b[0m\n",
      "\u001b[94mStep: 162, PSNR Before: 27.762951, PSNR After: 27.762995, PSNR Change: 0.000044, PSNR Diff: 0.001080 (New Max), Reward: 0.86, 11:13:08 Pre-flip Model Output=0.517570, New State Value=0, Flip Count=26\u001b[0m\n",
      "\u001b[94mStep: 164, PSNR Before: 27.762995, PSNR After: 27.763039, PSNR Change: 0.000044, PSNR Diff: 0.001123 (New Max), Reward: 0.90, 11:13:10 Pre-flip Model Output=0.447938, New State Value=1, Flip Count=27\u001b[0m\n",
      "\u001b[94mStep: 166, PSNR Before: 27.763039, PSNR After: 27.763046, PSNR Change: 0.000008, PSNR Diff: 0.001131 (New Max), Reward: 0.90, 11:13:12 Pre-flip Model Output=0.092206, New State Value=1, Flip Count=28\u001b[0m\n",
      "\u001b[94mStep: 174, PSNR Before: 27.763046, PSNR After: 27.763054, PSNR Change: 0.000008, PSNR Diff: 0.001139 (New Max), Reward: 0.91, 11:13:19 Pre-flip Model Output=0.525554, New State Value=0, Flip Count=29\u001b[0m\n",
      "\u001b[94mStep: 189, PSNR Before: 27.763054, PSNR After: 27.763062, PSNR Change: 0.000008, PSNR Diff: 0.001146 (New Max), Reward: 0.92, 11:13:32 Pre-flip Model Output=0.203475, New State Value=1, Flip Count=30\u001b[0m\n",
      "\u001b[94mStep: 193, PSNR Before: 27.763062, PSNR After: 27.763073, PSNR Change: 0.000011, PSNR Diff: 0.001158 (New Max), Reward: 0.93, 11:13:35 Pre-flip Model Output=0.749223, New State Value=0, Flip Count=31\u001b[0m\n",
      "\u001b[94mStep: 202, PSNR Before: 27.763073, PSNR After: 27.763145, PSNR Change: 0.000072, PSNR Diff: 0.001230 (New Max), Reward: 0.98, 11:13:43 Pre-flip Model Output=0.669168, New State Value=0, Flip Count=32\u001b[0m\n",
      "\u001b[94mStep: 203, PSNR Before: 27.763145, PSNR After: 27.763214, PSNR Change: 0.000069, PSNR Diff: 0.001299 (New Max), Reward: 1.04, 11:13:44 Pre-flip Model Output=0.489645, New State Value=1, Flip Count=33\u001b[0m\n",
      "\u001b[94mStep: 209, PSNR Before: 27.763214, PSNR After: 27.763254, PSNR Change: 0.000040, PSNR Diff: 0.001339 (New Max), Reward: 1.07, 11:13:50 Pre-flip Model Output=0.001836, New State Value=1, Flip Count=34\u001b[0m\n",
      "\u001b[94mStep: 210, PSNR Before: 27.763254, PSNR After: 27.763262, PSNR Change: 0.000008, PSNR Diff: 0.001347 (New Max), Reward: 1.08, 11:13:51 Pre-flip Model Output=0.223674, New State Value=1, Flip Count=35\u001b[0m\n",
      "\u001b[94mStep: 221, PSNR Before: 27.763262, PSNR After: 27.763292, PSNR Change: 0.000031, PSNR Diff: 0.001377 (New Max), Reward: 1.10, 11:14:00 Pre-flip Model Output=0.538811, New State Value=0, Flip Count=36\u001b[0m\n",
      "\u001b[94mStep: 247, PSNR Before: 27.763292, PSNR After: 27.763294, PSNR Change: 0.000002, PSNR Diff: 0.001379 (New Max), Reward: 1.10, 11:14:22 Pre-flip Model Output=0.990148, New State Value=0, Flip Count=38\u001b[0m\n",
      "\u001b[94mStep: 253, PSNR Before: 27.763294, PSNR After: 27.763435, PSNR Change: 0.000141, PSNR Diff: 0.001520 (New Max), Reward: 1.22, 11:14:27 Pre-flip Model Output=0.690620, New State Value=0, Flip Count=39\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacity of 23.65 GiB of which 160.69 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.77 GiB is allocated by PyTorch, and 273.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 931\u001b[0m\n\u001b[1;32m    915\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m CustomPPO(\n\u001b[1;32m    916\u001b[0m     CustomCNNPolicy,  \u001b[38;5;66;03m# Use the custom policy\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_custom_policy/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[0;32m--> 931\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    934\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 870\u001b[0m, in \u001b[0;36mCustomPPO.learn\u001b[0;34m(self, total_timesteps, log_interval)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device_train):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(device_train)  \u001b[38;5;66;03m# 학습 전 정책 네트워크를 GPU 1으로 이동\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(device_env)  \u001b[38;5;66;03m# 학습 후 다시 정책 네트워크를 GPU 0으로 이동\u001b[39;00m\n\u001b[1;32m    873\u001b[0m timesteps_since_eval \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:213\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 213\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 821\u001b[0m, in \u001b[0;36mCustomCNNPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, actions):\n\u001b[0;32m--> 821\u001b[0m     logits, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[1;32m    823\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "Cell \u001b[0;32mIn[1], line 782\u001b[0m, in \u001b[0;36mCustomCNNPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    776\u001b[0m simulation \u001b[38;5;241m=\u001b[39m simulation\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Debugging shapes after squeeze\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;66;03m#print(f\"[DEBUG] After Squeeze: state shape={state.shape}, obs_out shape={obs_out.shape}, target shape={target.shape}, simulation shape={simulation.shape}\")\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Process each component using respective CNNs\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m state_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m obs_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_cnn(obs_out)\n\u001b[1;32m    784\u001b[0m target_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_cnn(target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py:104\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 1 has a total capacity of 23.65 GiB of which 160.69 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.77 GiB is allocated by PyTorch, and 273.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 로그를 저장할 디렉토리 설정\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "\n",
    "# 현재 파일 이름과 실행 시간 가져오기\n",
    "if '__file__' in globals():\n",
    "    current_file = os.path.splitext(os.path.basename(__file__))[0]  # 현재 파일 이름(확장자 제거)\n",
    "else:\n",
    "    current_file = \"interactive\"  # 인터프리터나 노트북 환경에서 기본 파일 이름 사용\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # 현재 시간\n",
    "log_filename = os.path.join(log_dir, f\"{current_file}_{current_datetime}.log\")  # log 폴더에 파일 저장\n",
    "\n",
    "# 로그 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),  # 동적으로 생성된 파일 이름 사용\n",
    "        logging.StreamHandler()  # 콘솔 출력\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, data):\n",
    "        for file in self.files:\n",
    "            file.write(data)\n",
    "            file.flush()  # 실시간 저장\n",
    "\n",
    "    def flush(self):\n",
    "        for file in self.files:\n",
    "            file.flush()\n",
    "\n",
    "\n",
    "# stdout을 파일과 콘솔로 동시에 출력\n",
    "log_file = open(log_filename, \"a\")\n",
    "sys.stdout = Tee(sys.stdout, log_file)\n",
    "\n",
    "# 테스트 출력\n",
    "print(\"이 메시지는 콘솔과 파일에 동시에 기록됩니다.\")\n",
    "logging.info(\"로깅 메시지입니다.\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "IPS = 512\n",
    "CH = 8\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=CH, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, IPS, IPS).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(IPS)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((IPS, IPS))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < IPS or target.shape[-2] < IPS:\n",
    "            target = torchvision.transforms.Resize(IPS)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "# BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=10000, T_PSNR=30, T_steps=10, T_PSNR_DIFF=0.1, max_allowed_changes=1):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        T_PSNR_DIFF: PSNR 차이의 임계값.\n",
    "        max_allowed_changes: 한 번에 변경 가능한 최대 픽셀 수 (기본값: 1).\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간: (1, 채널, 픽셀, 픽셀)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4, CH, IPS, IPS), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: 픽셀 하나를 선택하는 인덱스 (채널 * 픽셀 *픽셀)\n",
    "        self.num_pixels = CH * IPS * IPS\n",
    "        self.action_space = spaces.Discrete(self.num_pixels)\n",
    "\n",
    "        # 타겟 함수와 데이터 로더 설정\n",
    "        self.target_function = target_function\n",
    "        self.trainloader = trainloader\n",
    "\n",
    "        # 환경 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "        self.T_PSNR_DIFF = T_PSNR_DIFF\n",
    "        self.max_allowed_changes = max_allowed_changes  # 추가된 속성\n",
    "\n",
    "        # 학습 상태 초기화\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 데이터 로더에서 첫 배치 설정\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "        # 실패한 경우 반복 여부\n",
    "        self.retry_current_target = False  # 현재 데이터셋 반복 여부\n",
    "\n",
    "        # 연속 실패 관련 변수\n",
    "        self.consecutive_fail_count = 0  # 연속 실패 횟수\n",
    "        self.max_consecutive_failures = 0  # 최대 연속 실패 횟수 기록\n",
    "\n",
    "        # 최고 PSNR_DIFF 추적 변수\n",
    "        self.max_psnr_diff = float('-inf')  # 가장 높은 PSNR_DIFF를 추적\n",
    "\n",
    "        self.flip_count = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경 초기화 함수.\n",
    "        데이터셋에서 새로운 이미지를 가져오고 초기 상태를 설정합니다.\n",
    "        - 데이터셋의 다음 이미지를 불러옵니다. \n",
    "        - BinaryNet을 사용해 초기 관찰값을 생성합니다.\n",
    "        - 초기 상태(state)는 관찰값을 이진화한 결과입니다.\n",
    "        - 초기 PSNR과 MSE를 계산하고 출력합니다.\n",
    "        - 실패 시 이전 데이터를 다시 불러옵니다.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): 랜덤 시드 값. Default는 None.\n",
    "            options (dict, optional): 추가 옵션. Default는 None.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.retry_current_target:  # 이전 에피소드에서 실패한 경우\n",
    "            self.consecutive_fail_count += 1\n",
    "        else:\n",
    "            self.consecutive_fail_count = 0  # 성공적인 에피소드로 연속 실패 초기화\n",
    "\n",
    "        self.max_consecutive_failures = max(self.max_consecutive_failures, self.consecutive_fail_count)\n",
    "\n",
    "        if not self.retry_current_target:  # 실패한 경우 현재 데이터를 다시 사용\n",
    "            try:\n",
    "                self.target_image = next(self.data_iter)\n",
    "            except StopIteration:\n",
    "                self.data_iter = iter(self.trainloader)\n",
    "                self.target_image = next(self.data_iter)\n",
    "\n",
    "        # 매 에피소드마다 최대 PSNR 차이 초기화\n",
    "        self.max_psnr_diff = float('-inf')\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "\n",
    "        # 타겟 이미지 형식 출력\n",
    "        #print(f\"[DEBUG]Target image shape: {self.target_image.shape}, dtype: {self.target_image.dtype}\")\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 512, 512)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # Ensure observation shape is (채널, 픽셀, 픽셀)\n",
    "        self.observation = model_output.squeeze(0).cpu().numpy()  # (채널, 픽셀, 픽셀)\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # Binary state\n",
    "\n",
    "        # 시뮬레이션 전 binary 형상을 (1, 채널, 픽셀, 픽셀)로 복원\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()  # (1, 채널, 픽셀, 픽셀)\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        self.initial_psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)  # 초기 PSNR 저장\n",
    "\n",
    "        # target_image_np와 result를 채널 차원(CH=8)으로 확장\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "        result_np = np.repeat(result.squeeze(0).cpu().numpy(), CH, axis=0)  # 모양: [8, 512, 512]\n",
    "\n",
    "        #print(f\"[DEBUG]reset\")\n",
    "        #print(f\"[DEBUG]self.state shape: {self.state.shape}, type: {type(self.state)}\")\n",
    "        #print(f\"[DEBUG]self.observation shape: {self.observation.shape}, type: {type(self.observation)}\")\n",
    "        #print(f\"[DEBUG]target_image_np shape: {target_image_np.shape}, type: {type(target_image_np)}\")\n",
    "        #print(f\"[DEBUG]result_np shape: {result_np.shape}, type: {type(result_np)}\")\n",
    "\n",
    "\n",
    "        # 모든 관찰값을 스택으로 결합\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )  # 최종 모양: [4, 8, 512, 512]\n",
    "\n",
    "        #print(f\"[DEBUG]Combined observation shape: {combined_observation.shape}, type: {type(combined_observation)}\")\n",
    "\n",
    "        print(f\"\\033[91mResetting environment. Consecutive episode failures: {self.consecutive_fail_count}, Max consecutive episode failures: {self.max_consecutive_failures}\\033[0m\")\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\033[92mInitial MSE: {mse:.6f}, Initial PSNR: {self.initial_psnr:.6f}, {current_time}\\033[0m\")\n",
    "\n",
    "\n",
    "        self.retry_current_target = False  # 초기화 후 데이터 반복 플래그 해제\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        \n",
    "        #print(f\"[DEBUG] Reset Observation: combined_observation shape={combined_observation.shape}, type={type(combined_observation)}\")\n",
    "        return combined_observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def initialize_state(self, z=2e-3): #아마도 사용하지 않는 레거시 코드드\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "\n",
    "        Args:\n",
    "            z (float): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 초기 관찰값.\n",
    "            dict: 초기 상태와 행동 마스크.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 모델로 초기 관찰값 생성\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # 관찰값을 numpy 배열로 변환\n",
    "\n",
    "        # 초기 상태는 이진화된 값으로 설정\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()  # 상태를 Torch 텐서로 변환\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # 메타 정보 추가\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # 초기 MSE와 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 초기 값 출력\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {datetime.now()}\")\n",
    "\n",
    "        # 시뮬레이션 결과를 별도로 저장\n",
    "        self.simulation_result = result.detach().cpu().numpy()\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        # 관찰값(초기 모델 출력)을 반환\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        - 관찰값이 0.2 ~ 0.8 범위에 해당하는 픽셀만 변경 가능.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): 관찰값.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 가능한 행동에 대해 1, 불가능한 행동에 대해 0.\n",
    "        \"\"\"\n",
    "        # 모든 픽셀을 고려한 기본 마스크\n",
    "        mask = np.zeros(self.num_pixels, dtype=np.int8)\n",
    "\n",
    "        # (1, 채널, 픽셀, 픽셀) -> (채널, 픽셀, 픽셀)로 변환\n",
    "        obs = self.observation.squeeze()\n",
    "\n",
    "        # 조건을 만족하는 위치에 대해 마스크 설정\n",
    "        for channel in range(obs.shape[0]):\n",
    "            indices = np.where((obs[channel] > 0) & (obs[channel] < 1))\n",
    "            for row, col in zip(*indices):\n",
    "                pixel_idx = channel * IPS * IPS + row * IPS + col\n",
    "                mask[pixel_idx] = 1  # 가능한 행동으로 설정\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        \"\"\"\n",
    "        환경의 한 타임스텝을 진행합니다.\n",
    "        - 주어진 행동(action)을 적용하고, 새로운 상태를 계산합니다.\n",
    "        - 보상은 행동 전후 PSNR 변화량(psnr_change)을 기반으로 계산합니다.\n",
    "        - psnr_change가 0보다 작을 경우 잘못된 행동으로 처리됩니다.\n",
    "\n",
    "        Args:\n",
    "            action (np.ndarray): 에이전트가 수행한 행동.\n",
    "            lr (float, optional): 학습률. Default는 1e-4.\n",
    "            z (float, optional): 시뮬레이션 거리. Default는 2e-3.\n",
    "\n",
    "        Returns:\n",
    "            observation (np.ndarray): 새로운 관찰값.\n",
    "            float: 보상 값.\n",
    "            bool: 종료 여부.\n",
    "            bool: Truncated 여부.\n",
    "            dict: 추가 정보 (MSE, PSNR, PSNR_DIFF, 행동 마스크 등).\n",
    "        \"\"\"\n",
    "        #if self.steps == 0:\n",
    "        #    print(\"Executing reset logic for the first step\")\n",
    "        #    self.steps += 1\n",
    "        #    observation, info = self.initialize_state(z)\n",
    "        #    return observation, 0.0, False, False, info\n",
    "\n",
    "        # 행동 마스크를 적용\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        if mask.flatten()[action] == 0:\n",
    "            # 잘못된 행동 시 패널티와 함께 상태 유지\n",
    "            #print(f\"Invalid action taken at step {self.steps}, action: {action}\")\n",
    "            return self.observation, -10.0, False, False, {\"mask\": mask}\n",
    "\n",
    "        # 행동 전 PSNR 계산\n",
    "        binary_before = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_before = tt.Tensor(binary_before, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_before = tt.simulate(binary_before, z).abs()**2\n",
    "        result_before = torch.mean(sim_before, dim=1, keepdim=True)\n",
    "        psnr_before = tt.relativeLoss(result_before, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 행동을 기반으로 픽셀 좌표 계산\n",
    "        channel = action // (IPS * IPS)\n",
    "        pixel_index = action % (IPS * IPS)\n",
    "        row = pixel_index // IPS\n",
    "        col = pixel_index % IPS\n",
    "\n",
    "        # 플립 전 모델 예측값 가져오기\n",
    "        pre_flip_value = self.observation[channel, row, col]\n",
    "\n",
    "        # 상태 변경\n",
    "        self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "        self.flip_count += 1  # 플립 증가\n",
    "\n",
    "        # 현재 상태로 새로운 시뮬레이션 수행\n",
    "        binary_after = torch.tensor(self.state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        binary_after = tt.Tensor(binary_after, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})\n",
    "        sim_after = tt.simulate(binary_after, z).abs()**2\n",
    "        result_after = torch.mean(sim_after, dim=1, keepdim=True)\n",
    "        psnr_after = tt.relativeLoss(result_after, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        # 시뮬레이션 결과를 NumPy로 변환\n",
    "        result_np = np.repeat(result_after.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        target_image_np = np.repeat(self.target_image.squeeze(0).cpu().numpy(), CH, axis=0)\n",
    "\n",
    "        #print(f\"[DEBUG]step\")\n",
    "        #print(f\"[DEBUG]self.state shape: {self.state.shape}, type: {type(self.state)}\")\n",
    "        #print(f\"[DEBUG]self.observation shape: {self.observation.shape}, type: {type(self.observation)}\")\n",
    "        #print(f\"[DEBUG]target_image_np shape: {target_image_np.shape}, type: {type(target_image_np)}\")\n",
    "        #print(f\"[DEBUG]result_np shape: {result_np.shape}, type: {type(result_np)}\")\n",
    "\n",
    "        # Combined observation 생성 후 출력\n",
    "        combined_observation = np.stack(\n",
    "            [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "        )\n",
    "\n",
    "        #print(f\"[DEBUG]Combined observation shape: {combined_observation.shape}, type: {type(combined_observation)}\")\n",
    "\n",
    "        # PSNR 변화량 계산\n",
    "        psnr_change = psnr_after - psnr_before\n",
    "\n",
    "        # 기존 PSNR_DIFF 계산\n",
    "        psnr_diff = psnr_after - self.initial_psnr\n",
    "        is_max_psnr_diff = psnr_diff > self.max_psnr_diff  # 최고 PSNR_DIFF 확인\n",
    "        self.max_psnr_diff = max(self.max_psnr_diff, psnr_diff)  # 최고 PSNR_DIFF 업데이트\n",
    "\n",
    "        # psnr_change가 음수인 경우 상태 롤백만 수행\n",
    "        if psnr_change < 0:\n",
    "\n",
    "            # print(f\"[DEBUG]rollback\")\n",
    "            # print(f\"[DEBUG]self.state shape: {self.state.shape}, type: {type(self.state)}\")\n",
    "            # print(f\"[DEBUG]self.observation shape: {self.observation.shape}, type: {type(self.observation)}\")\n",
    "            # print(f\"[DEBUG]target_image_np shape: {target_image_np.shape}, type: {type(target_image_np)}\")\n",
    "            # print(f\"[DEBUG]result_np shape: {result_np.shape}, type: {type(result_np)}\")\n",
    "\n",
    "            failed_observation = np.stack(\n",
    "                [self.state, self.observation, target_image_np, result_np], axis=0\n",
    "            )  # 최종 모양: [4, 8, 512, 512]\n",
    "\n",
    "            # print(f\"[DEBUG]failed_observationshape: {failed_observation.shape}, type: {type(failed_observation)}\")\n",
    "\n",
    "            failed_action = action\n",
    "            failed_reward = -10  # 실패에 대한 보상\n",
    "\n",
    "            # 이전 스텝의 누적 보상을 안전하게 초기화\n",
    "            previous_cumulative_reward = 0\n",
    "            if \"info\" in locals() and isinstance(info, dict):\n",
    "                previous_cumulative_reward = info.get(\"cumulative_reward\", 0)\n",
    "\n",
    "            # 실패 정보 생성\n",
    "            info = {\n",
    "                \"psnr_before\": psnr_before,\n",
    "                \"psnr_after\": psnr_after,\n",
    "                \"psnr_change\": psnr_change,\n",
    "                \"psnr_diff\": psnr_diff,\n",
    "                \"mask\": mask,\n",
    "                \"pre_flip_value\": pre_flip_value,\n",
    "                \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "                \"state_after\": None,  # 실패한 경우에는 상태를 업데이트하지 않음\n",
    "                \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "                \"observation_after\": None,  # 실패한 경우 관찰값 업데이트 없음\n",
    "                \"failed_action\": failed_action,  # 실패한 행동\n",
    "                \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "                \"reward\": failed_reward,\n",
    "                \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "                \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "                \"step\": self.steps  # 현재 스텝\n",
    "            }\n",
    "\n",
    "            # 플립된 픽셀을 원래대로 복구\n",
    "            self.state[channel, row, col] = 1 - self.state[channel, row, col]\n",
    "            self.flip_count -= 1\n",
    "\n",
    "            # 스텝 증가\n",
    "            self.steps += 1\n",
    "\n",
    "            return failed_observation, failed_reward, False, False, info\n",
    "\n",
    "\n",
    "        # PSNR_CHANGE가 0보다 작을 경우 잘못된 행동으로 처리\n",
    "        #if psnr_change < 0:\n",
    "            #print(f\"Invalid action: PSNR Change {psnr_change:.6f} < 0 at step {self.steps}\")\n",
    "        #    return self.observation, -10.0, False, False, {\"psnr_before\": psnr_before, \"psnr_after\": psnr_after, \"psnr_change\": psnr_change, \"mask\": mask}\n",
    "\n",
    "        # 보상 계산\n",
    "        reward = psnr_diff * 800  # PSNR 변화량(psnr_change)에 기반한 보상\n",
    "\n",
    "        # 실패 조건 확인\n",
    "        if psnr_diff < -0.01:\n",
    "            print(f\"\\033[91mEpisode failed: PSNR Diff {psnr_diff:.6f} < -0.01 at step {self.steps}\\033[0m\")\n",
    "            self.retry_current_target = True  # 실패 시 반복 플래그 활성화\n",
    "            return self.observation, -100.0, True, False, {\"psnr_diff\": psnr_diff, \"mask\": None}\n",
    "\n",
    "        # 최고 PSNR_DIFF일 때 출력\n",
    "        if is_max_psnr_diff:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"\\033[94mStep: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\\033[0m\"\n",
    "            )\n",
    "\n",
    "        # 출력 추가 (100 스텝마다 출력)\n",
    "        if self.steps % 100 == 0:\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(\n",
    "                f\"Step: {self.steps}, PSNR Before: {psnr_before:.6f}, PSNR After: {psnr_after:.6f}, \"\n",
    "                f\"PSNR Change: {psnr_change:.6f}, PSNR Diff: {psnr_diff:.6f} (New Max), \"\n",
    "                f\"Reward: {reward:.2f}, {current_time} \"\n",
    "                f\"Pre-flip Model Output={pre_flip_value:.6f}, \"\n",
    "                f\"New State Value={self.state[channel, row, col]}, \"\n",
    "                f\"Flip Count={self.flip_count}\"\n",
    "            )\n",
    "\n",
    "        # 성공 종료 조건: PSNR >= T_PSNR 또는 PSNR_DIFF >= T_PSNR_DIFF\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr_after >= self.T_PSNR or psnr_diff >= self.T_PSNR_DIFF:\n",
    "            self.psnr_sustained_steps += 1\n",
    "            if self.psnr_sustained_steps >= self.T_steps:  # 성공 에피소드 조건\n",
    "                reward += 100  # 에피소드 성공 시 추가 보상\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\n",
    "            \"psnr_before\": psnr_before,\n",
    "            \"psnr_after\": psnr_after,\n",
    "            \"psnr_change\": psnr_change,\n",
    "            \"psnr_diff\": psnr_diff,\n",
    "            \"mask\": mask,\n",
    "            \"pre_flip_value\": pre_flip_value,\n",
    "            \"state_before\": self.state.copy(),  # 행동 이전 상태\n",
    "            \"state_after\": self.state.copy() if psnr_change >= 0 else None,  # 행동 성공 시 상태\n",
    "            \"observation_before\": self.observation.copy(),  # 행동 이전 관찰값\n",
    "            \"observation_after\": combined_observation if psnr_change >= 0 else None,  # 행동 성공 시 관찰값\n",
    "            \"failed_action\": action if psnr_change < 0 else None,  # 실패한 행동\n",
    "            \"flip_count\": self.flip_count,  # 현재까지의 플립 횟수\n",
    "            \"reward\": reward,\n",
    "            \"target_image\": self.target_image.cpu().numpy(),  # 타겟 이미지\n",
    "            \"simulation_result\": result_np,  # 현재 시뮬레이션 결과\n",
    "            \"action_coords\": (channel, row, col),  # 행동한 좌표\n",
    "            \"step\": self.steps  # 현재 스텝\n",
    "        }\n",
    "\n",
    "        del binary_before, binary_after, sim_before, sim_after, result_before, result_after\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        #print(f\"[DEBUG] Step Observation: combined_observation shape={combined_observation.shape}, type={type(combined_observation)}\")\n",
    "\n",
    "        return combined_observation, reward, terminated, truncated, info\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class CustomCNNPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, *args, **kwargs):\n",
    "        super(CustomCNNPolicy, self).__init__(observation_space, action_space, lr_schedule, *args, **kwargs)\n",
    "\n",
    "        # Define CNN modules for each part of the observation\n",
    "        self.state_cnn = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.obs_cnn = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.target_cnn = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.simulation_cnn = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Calculate the total flattened features\n",
    "        dummy_input = torch.zeros(1, 8, 512, 512)\n",
    "        state_features = self.state_cnn(dummy_input).shape[1]\n",
    "        obs_features = self.obs_cnn(dummy_input).shape[1]\n",
    "        target_features = self.target_cnn(dummy_input).shape[1]\n",
    "        simulation_features = self.simulation_cnn(dummy_input).shape[1]\n",
    "\n",
    "        total_features = state_features + obs_features + target_features + simulation_features\n",
    "\n",
    "        # Actor (policy network)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(total_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.action_space.n)\n",
    "        )\n",
    "\n",
    "        # Critic (value network)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(total_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        #print(f\"[DEBUG] Policy Forward Input: obs shape={obs.shape}, type={type(obs)}\")\n",
    "\n",
    "        # Split the observation into components along dim=1\n",
    "        state, obs_out, target, simulation = torch.chunk(obs, 4, dim=1)\n",
    "\n",
    "        # Remove the extra dimension (squeeze dim=1)\n",
    "        state = state.squeeze(1)  # From [1, 8, 512, 512] -> [8, 512, 512]\n",
    "        obs_out = obs_out.squeeze(1)\n",
    "        target = target.squeeze(1)\n",
    "        simulation = simulation.squeeze(1)\n",
    "\n",
    "        # Debugging shapes after squeeze\n",
    "        #print(f\"[DEBUG] After Squeeze: state shape={state.shape}, obs_out shape={obs_out.shape}, target shape={target.shape}, simulation shape={simulation.shape}\")\n",
    "\n",
    "        # Process each component using respective CNNs\n",
    "        state_features = self.state_cnn(state)\n",
    "        obs_features = self.obs_cnn(obs_out)\n",
    "        target_features = self.target_cnn(target)\n",
    "        simulation_features = self.simulation_cnn(simulation)\n",
    "\n",
    "        # Debugging CNN output shapes\n",
    "        #print(f\"[DEBUG] CNN Outputs: state_features shape={state_features.shape}, obs_features shape={obs_features.shape}, target_features shape={target_features.shape}, simulation_features shape={simulation_features.shape}\")\n",
    "\n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat([state_features, obs_features, target_features, simulation_features], dim=1)\n",
    "\n",
    "        # Compute policy logits and value\n",
    "        logits = self.actor(combined_features)\n",
    "        values = self.critic(combined_features)\n",
    "\n",
    "        # Create distribution from logits\n",
    "        distribution = self.action_dist.proba_distribution(logits)\n",
    "\n",
    "        # Sample actions\n",
    "        if deterministic:\n",
    "            actions = distribution.mode()\n",
    "        else:\n",
    "            actions = distribution.sample()\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "\n",
    "        return actions, values, log_probs\n",
    "\n",
    "\n",
    "    def _predict(self, obs, deterministic=False):\n",
    "        logits, _ = self.forward(obs, deterministic)\n",
    "        return logits.argmax(dim=1) if deterministic else torch.multinomial(torch.softmax(logits, dim=1), num_samples=1)\n",
    "\n",
    "    def get_distribution(self, obs):\n",
    "        logits, _ = self.forward(obs)\n",
    "        return self.action_dist.proba_distribution(logits=logits)\n",
    "\n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        distribution = self.action_dist.proba_distribution(logits=logits)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        return log_prob, values, distribution.entropy()\n",
    "\n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "\n",
    "class DummyCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    기본 동작을 제공하는 더미 콜백.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DummyCallback, self).__init__()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 매 스텝마다 호출되며, 기본적으로 계속 학습하도록 True를 반환\n",
    "        return True\n",
    "\n",
    "class CustomPPO(PPO):\n",
    "    def learn(self, total_timesteps, log_interval=10):\n",
    "        device_env = torch.device(\"cuda:0\")  # 환경과 행동 생성용\n",
    "        device_train = torch.device(\"cuda:1\")  # 학습 연산용\n",
    "\n",
    "        # 환경에서 사용될 정책 네트워크를 GPU 0으로 이동\n",
    "        self.policy.to(device_env)\n",
    "\n",
    "        # 학습 설정 초기화\n",
    "        self._setup_learn(total_timesteps)\n",
    "\n",
    "        # 기본 콜백 설정\n",
    "        callback = CallbackList([DummyCallback()])\n",
    "        callback.init_callback(self)  # 콜백에 모델 정보를 전달\n",
    "\n",
    "        timesteps_since_eval = 0\n",
    "        for step in range(0, total_timesteps, self.n_steps):\n",
    "            # 데이터 수집은 GPU 0에서 수행\n",
    "            with torch.cuda.device(device_env):\n",
    "                self.collect_rollouts(\n",
    "                    env=self.env,\n",
    "                    rollout_buffer=self.rollout_buffer,\n",
    "                    n_rollout_steps=self.n_steps,\n",
    "                    callback=callback  # 더미 콜백 사용\n",
    "                )\n",
    "\n",
    "            # 학습은 GPU 1에서 수행\n",
    "            with torch.cuda.device(device_train):\n",
    "                self.policy.to(device_train)  # 학습 전 정책 네트워크를 GPU 1으로 이동\n",
    "                self.train()\n",
    "                self.policy.to(device_env)  # 학습 후 다시 정책 네트워크를 GPU 0으로 이동\n",
    "\n",
    "            timesteps_since_eval += self.n_steps\n",
    "            if step % log_interval == 0:\n",
    "                print(f\"Step {step}/{total_timesteps}: Training complete for this batch.\")\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002/2024-12-19 20:37:52.499731_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader\n",
    ")\n",
    "env = ActionMasker(env, mask_fn)\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 모델 초기화\n",
    "ppo_model = CustomPPO(\n",
    "    CustomCNNPolicy,  # Use the custom policy\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-4,  # Learning rate\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping\n",
    "    tensorboard_log=\"./ppo_custom_policy/\"\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "ppo_model.learn(total_timesteps=1000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    #max_steps=10000,\n",
    "    #T_PSNR=30,\n",
    "    #T_steps=10\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "#obs = venv.reset()\n",
    "#print(f\"[DEBUG] Venv Reset Observation: obs shape={obs.shape}, type={type(obs)}\")\n",
    "\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "#obs = venv.reset()\n",
    "#print(f\"[DEBUG] After VecNormalize: obs shape={obs.shape}, type={type(obs)}\")\n",
    "\n",
    "ppo_model = PPO(\n",
    "    CustomCNNPolicy,  # Use the custom policy\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-4,  # Learning rate\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping\n",
    "    tensorboard_log=\"./ppo_custom_policy/\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=256,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=1e-4,  # 학습률 감소\n",
    "    clip_range=0.2,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.2,  # Gradient clipping 추가\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
