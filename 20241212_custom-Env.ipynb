{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7d6156-1a3c-4d16-8a80-26491908ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, max_steps=1000000, T_PSNR=30, T_steps=100, window_size=3):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        window_size: 주변 픽셀 정보를 포함하는 윈도우 크기 (홀수).\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 주변 정보 설정\n",
    "        self.window_size = window_size\n",
    "        self.pad = window_size // 2  # 패딩 크기 계산\n",
    "\n",
    "        # Observation space: 주변 정보 포함 (연속형 데이터)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(1024, 1024, 8 + window_size**2 - 1),  # 8채널 + 추가 윈도우 정보\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Action space: 1024x1024x8 이진 데이터 (출력)\n",
    "        self.action_space = spaces.MultiBinary(1024 * 1024 * 8)\n",
    "\n",
    "        # 목표 함수\n",
    "        self.target_function = target_function\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        # 학습 상태\n",
    "        self.state = None\n",
    "        self.binary_state = None  # 초기 이진 상태\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0  # PSNR 목표 유지 스텝 수\n",
    "\n",
    "    def _extract_window_features(self, state):\n",
    "        \"\"\"\n",
    "        주변 픽셀 정보를 포함한 윈도우 생성.\n",
    "        \"\"\"\n",
    "        # 주변 픽셀 정보를 포함하기 위해 패딩 추가\n",
    "        padded_state = np.pad(state, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)), mode=\"reflect\")\n",
    "        window_features = np.zeros((1024, 1024, self.window_size**2 - 1), dtype=np.float32)\n",
    "\n",
    "        # 주변 정보를 추가 채널로 생성\n",
    "        idx = 0\n",
    "        for dy in range(-self.pad, self.pad + 1):\n",
    "            for dx in range(-self.pad, self.pad + 1):\n",
    "                if dy == 0 and dx == 0:\n",
    "                    continue  # 중심 픽셀은 제외\n",
    "                window_features[:, :, idx] = padded_state[\n",
    "                    self.pad + dy : self.pad + dy + 1024,\n",
    "                    self.pad + dx : self.pad + dx + 1024,\n",
    "                    0,  # 첫 번째 채널 기준으로 주변 정보 추출\n",
    "                ]\n",
    "                idx += 1\n",
    "        return window_features\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        환경 상태를 초기화.\n",
    "        \"\"\"\n",
    "        # 연속형 데이터 초기화 (1024x1024x8 크기)\n",
    "        self.state = np.random.rand(1024, 1024, 8).astype(np.float32)\n",
    "\n",
    "        # 초기 이진 상태를 0.5 기준으로 설정\n",
    "        self.binary_state = (self.state >= 0.5).astype(np.float32)\n",
    "\n",
    "        # 주변 정보 포함 상태 생성\n",
    "        window_features = self._extract_window_features(self.state[:, :, 0:1])\n",
    "        extended_state = np.concatenate([self.state, window_features], axis=-1)\n",
    "\n",
    "        # 초기 상태 설정\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        return extended_state, {}\n",
    "\n",
    "    def calculate_psnr(self, mse):\n",
    "        \"\"\"\n",
    "        MSE로부터 PSNR 계산.\n",
    "        \"\"\"\n",
    "        if mse == 0:\n",
    "            return np.inf\n",
    "        return 10 * np.log10(1.0 / mse)  # MAX 신호 값(1.0) 기준\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: 1024x1024x8 배열로 출력된 바이너리 홀로그램 (0 또는 1).\n",
    "        \"\"\"\n",
    "        # Action을 3D 배열로 변환\n",
    "        action = np.reshape(action, (1024, 1024, 8))\n",
    "\n",
    "        # 타겟 계산 (타겟은 연속형 데이터의 평균값으로 가정)\n",
    "        target = self.state.mean(axis=-1, keepdims=True)  # 타겟 이미지 (평균값)\n",
    "\n",
    "        # MSE 계산 (이진 출력의 평균과 타겟 비교)\n",
    "        mse = np.mean((action.mean(axis=-1) - target.squeeze()) ** 2)\n",
    "\n",
    "        # PSNR 계산\n",
    "        psnr = self.calculate_psnr(mse)\n",
    "\n",
    "        # 보상 설계 (MSE 기반)\n",
    "        reward = -mse\n",
    "\n",
    "        # 종료 조건 초기화\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        self.steps += 1\n",
    "\n",
    "        # 시간 초과 조건\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        # 성능 기반 종료 조건 (PSNR 유지)\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 목표 PSNR을 일정 시간 유지하면 종료\n",
    "        if self.psnr_sustained_steps >= self.T_steps:\n",
    "            terminated = True\n",
    "\n",
    "        # 주변 정보 포함 상태 생성\n",
    "        window_features = self._extract_window_features(self.state[:, :, 0:1])\n",
    "        extended_state = np.concatenate([self.state, window_features], axis=-1)\n",
    "\n",
    "        # 추가 정보 반환\n",
    "        info = {\n",
    "            \"mse\": mse,\n",
    "            \"psnr\": psnr,\n",
    "            \"psnr_sustained_steps\": self.psnr_sustained_steps,\n",
    "        }\n",
    "\n",
    "        return extended_state, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9cb06c-737b-463f-bdcf-33be55567292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/env_checker.py:54: UserWarning: It seems that your observation  is an image but its `dtype` is (float32) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/env_checker.py:62: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "def target_function(state, action):\n",
    "    target = state.mean(axis=-1, keepdims=True)\n",
    "    return np.mean((action.mean(axis=-1) - target.squeeze()) ** 2)\n",
    "\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=target_function,\n",
    "    max_steps=1000000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=100,\n",
    "    window_size=3\n",
    ")\n",
    "\n",
    "# 환경 유효성 확인\n",
    "check_env(env, warn=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed95b01-acaa-4898-82bb-05d549d2de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# PPO 모델 학습\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:171\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_kl \u001b[38;5;241m=\u001b[39m target_kl\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:174\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range \u001b[38;5;241m=\u001b[39m get_schedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:135\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class \u001b[38;5;241m=\u001b[39m RolloutBuffer\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps,\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_kwargs,\n\u001b[1;32m    134\u001b[0m )\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Warn when not using CPU with MlpPolicy\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:535\u001b[0m, in \u001b[0;36mActorCriticPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Action distribution\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist \u001b[38;5;241m=\u001b[39m make_proba_distribution(action_space, use_sde\u001b[38;5;241m=\u001b[39muse_sde, dist_kwargs\u001b[38;5;241m=\u001b[39mdist_kwargs)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:631\u001b[0m, in \u001b[0;36mActorCriticPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    628\u001b[0m         module_gains[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf_features_extractor] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module, gain \u001b[38;5;129;01min\u001b[39;00m module_gains\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 631\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Setup optimizer with initial learning rate\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:895\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 895\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:895\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 895\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:896\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    895\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m--> 896\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:314\u001b[0m, in \u001b[0;36mBasePolicy.init_weights\u001b[0;34m(module, gain)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mOrthogonal initialization (used in PPO and A2C)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (nn\u001b[38;5;241m.\u001b[39mLinear, nn\u001b[38;5;241m.\u001b[39mConv2d)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morthogonal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py:544\u001b[0m, in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain, generator)\u001b[0m\n\u001b[1;32m    541\u001b[0m     flattened\u001b[38;5;241m.\u001b[39mt_()\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Compute the qr factorization\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m q, r \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\u001b[39;00m\n\u001b[1;32m    546\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(r, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# PPO 모델 학습\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
