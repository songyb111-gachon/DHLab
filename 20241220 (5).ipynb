{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024, 1024])\n",
      "Using cuda device\n",
      "Initial MSE: 0.002616, Initial PSNR: 25.824223, 09:48:01\n",
      "Logging to ./ppo_with_mask/PPO_48\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002616, Initial PSNR: 25.824223, 2024-12-20_09-47-22\n",
      "Step: 1, MSE: 0.069524, PSNR: 11.578655, 09:48:02\n",
      "Step: 2, MSE: 0.069664, PSNR: 11.569899, 09:48:02\n",
      "Step: 3, MSE: 0.069539, PSNR: 11.577697, 09:48:03\n",
      "Step: 4, MSE: 0.069604, PSNR: 11.573668, 09:48:03\n",
      "Step: 5, MSE: 0.069639, PSNR: 11.571465, 09:48:03\n",
      "Step: 6, MSE: 0.069723, PSNR: 11.566217, 09:48:04\n",
      "Step: 7, MSE: 0.069658, PSNR: 11.570302, 09:48:04\n",
      "Step: 8, MSE: 0.069535, PSNR: 11.577948, 09:48:05\n",
      "Step: 9, MSE: 0.069514, PSNR: 11.579299, 09:48:05\n",
      "Step: 10, MSE: 0.069568, PSNR: 11.575879, 09:48:05\n",
      "Step: 11, MSE: 0.069616, PSNR: 11.572896, 09:48:06\n",
      "Step: 12, MSE: 0.069669, PSNR: 11.569599, 09:48:06\n",
      "Step: 13, MSE: 0.069625, PSNR: 11.572322, 09:48:06\n",
      "Step: 14, MSE: 0.069441, PSNR: 11.583848, 09:48:07\n",
      "Step: 15, MSE: 0.069639, PSNR: 11.571480, 09:48:07\n",
      "Step: 16, MSE: 0.069480, PSNR: 11.581416, 09:48:07\n",
      "Step: 17, MSE: 0.069650, PSNR: 11.570776, 09:48:08\n",
      "Step: 18, MSE: 0.069536, PSNR: 11.577881, 09:48:08\n",
      "Step: 19, MSE: 0.069687, PSNR: 11.568491, 09:48:08\n",
      "Step: 20, MSE: 0.069753, PSNR: 11.564358, 09:48:09\n",
      "Step: 21, MSE: 0.069791, PSNR: 11.562021, 09:48:09\n",
      "Step: 22, MSE: 0.069530, PSNR: 11.578283, 09:48:09\n",
      "Step: 23, MSE: 0.069839, PSNR: 11.559008, 09:48:10\n",
      "Step: 24, MSE: 0.069609, PSNR: 11.573359, 09:48:10\n",
      "Step: 25, MSE: 0.069411, PSNR: 11.585727, 09:48:11\n",
      "Step: 26, MSE: 0.069537, PSNR: 11.577845, 09:48:11\n",
      "Step: 27, MSE: 0.069670, PSNR: 11.569551, 09:48:11\n",
      "Step: 28, MSE: 0.069502, PSNR: 11.579998, 09:48:12\n",
      "Step: 29, MSE: 0.069726, PSNR: 11.566028, 09:48:12\n",
      "Step: 30, MSE: 0.069455, PSNR: 11.582937, 09:48:12\n",
      "Step: 31, MSE: 0.069526, PSNR: 11.578517, 09:48:13\n",
      "Step: 32, MSE: 0.069591, PSNR: 11.574488, 09:48:13\n",
      "Step: 33, MSE: 0.069751, PSNR: 11.564484, 09:48:14\n",
      "Step: 34, MSE: 0.069542, PSNR: 11.577526, 09:48:14\n",
      "Step: 35, MSE: 0.069631, PSNR: 11.571990, 09:48:14\n",
      "Step: 36, MSE: 0.069570, PSNR: 11.575803, 09:48:15\n",
      "Step: 37, MSE: 0.069580, PSNR: 11.575151, 09:48:15\n",
      "Step: 38, MSE: 0.069679, PSNR: 11.568964, 09:48:15\n",
      "Step: 39, MSE: 0.069674, PSNR: 11.569310, 09:48:16\n",
      "Step: 40, MSE: 0.069388, PSNR: 11.587152, 09:48:16\n",
      "Step: 41, MSE: 0.069525, PSNR: 11.578571, 09:48:16\n",
      "Step: 42, MSE: 0.069748, PSNR: 11.564673, 09:48:17\n",
      "Step: 43, MSE: 0.069603, PSNR: 11.573691, 09:48:17\n",
      "Step: 44, MSE: 0.069653, PSNR: 11.570621, 09:48:17\n",
      "Step: 45, MSE: 0.069556, PSNR: 11.576662, 09:48:18\n",
      "Step: 46, MSE: 0.069708, PSNR: 11.567192, 09:48:18\n",
      "Step: 47, MSE: 0.069698, PSNR: 11.567810, 09:48:19\n",
      "Step: 48, MSE: 0.069534, PSNR: 11.577998, 09:48:19\n",
      "Step: 49, MSE: 0.069547, PSNR: 11.577236, 09:48:19\n",
      "Step: 50, MSE: 0.069557, PSNR: 11.576571, 09:48:20\n",
      "Step: 51, MSE: 0.069557, PSNR: 11.576571, 09:48:20\n",
      "Step: 52, MSE: 0.069531, PSNR: 11.578216, 09:48:20\n",
      "Step: 53, MSE: 0.069687, PSNR: 11.568499, 09:48:21\n",
      "Step: 54, MSE: 0.069730, PSNR: 11.565790, 09:48:21\n",
      "Step: 55, MSE: 0.069446, PSNR: 11.583504, 09:48:21\n",
      "Step: 56, MSE: 0.069475, PSNR: 11.581694, 09:48:22\n",
      "Step: 57, MSE: 0.069647, PSNR: 11.570996, 09:48:22\n",
      "Step: 58, MSE: 0.069653, PSNR: 11.570621, 09:48:22\n",
      "Step: 59, MSE: 0.069609, PSNR: 11.573339, 09:48:23\n",
      "Step: 60, MSE: 0.069606, PSNR: 11.573532, 09:48:23\n",
      "Step: 61, MSE: 0.069543, PSNR: 11.577436, 09:48:23\n",
      "Step: 62, MSE: 0.069627, PSNR: 11.572207, 09:48:24\n",
      "Step: 63, MSE: 0.069637, PSNR: 11.571627, 09:48:24\n",
      "Step: 64, MSE: 0.069445, PSNR: 11.583574, 09:48:24\n",
      "Step: 65, MSE: 0.069579, PSNR: 11.575190, 09:48:25\n",
      "Step: 66, MSE: 0.069430, PSNR: 11.584506, 09:48:25\n",
      "Step: 67, MSE: 0.069441, PSNR: 11.583835, 09:48:26\n",
      "Step: 68, MSE: 0.069749, PSNR: 11.564626, 09:48:26\n",
      "Step: 69, MSE: 0.069524, PSNR: 11.578653, 09:48:26\n",
      "Step: 70, MSE: 0.069584, PSNR: 11.574884, 09:48:27\n",
      "Step: 71, MSE: 0.069500, PSNR: 11.580122, 09:48:27\n",
      "Step: 72, MSE: 0.069504, PSNR: 11.579874, 09:48:27\n",
      "Step: 73, MSE: 0.069745, PSNR: 11.564844, 09:48:28\n",
      "Step: 74, MSE: 0.069458, PSNR: 11.582785, 09:48:28\n",
      "Step: 75, MSE: 0.069678, PSNR: 11.569017, 09:48:28\n",
      "Step: 76, MSE: 0.069587, PSNR: 11.574739, 09:48:29\n",
      "Step: 77, MSE: 0.069703, PSNR: 11.567513, 09:48:29\n",
      "Step: 78, MSE: 0.069548, PSNR: 11.577137, 09:48:29\n",
      "Step: 79, MSE: 0.069679, PSNR: 11.569008, 09:48:30\n",
      "Step: 80, MSE: 0.069594, PSNR: 11.574296, 09:48:30\n",
      "Step: 81, MSE: 0.069477, PSNR: 11.581572, 09:48:30\n",
      "Step: 82, MSE: 0.069632, PSNR: 11.571938, 09:48:31\n",
      "Step: 83, MSE: 0.069543, PSNR: 11.577474, 09:48:31\n",
      "Step: 84, MSE: 0.069791, PSNR: 11.562017, 09:48:31\n",
      "Step: 85, MSE: 0.069531, PSNR: 11.578218, 09:48:32\n",
      "Step: 86, MSE: 0.069655, PSNR: 11.570493, 09:48:32\n",
      "Step: 87, MSE: 0.069598, PSNR: 11.574039, 09:48:33\n",
      "Step: 88, MSE: 0.069420, PSNR: 11.585167, 09:48:33\n",
      "Step: 89, MSE: 0.069697, PSNR: 11.567854, 09:48:33\n",
      "Step: 90, MSE: 0.069752, PSNR: 11.564408, 09:48:34\n",
      "Step: 91, MSE: 0.069556, PSNR: 11.576641, 09:48:34\n",
      "Step: 92, MSE: 0.069500, PSNR: 11.580153, 09:48:34\n",
      "Step: 93, MSE: 0.069606, PSNR: 11.573545, 09:48:35\n",
      "Step: 94, MSE: 0.069710, PSNR: 11.567055, 09:48:35\n",
      "Step: 95, MSE: 0.069606, PSNR: 11.573555, 09:48:35\n",
      "Step: 96, MSE: 0.069553, PSNR: 11.576857, 09:48:36\n",
      "Step: 97, MSE: 0.069718, PSNR: 11.566555, 09:48:36\n",
      "Step: 98, MSE: 0.069546, PSNR: 11.577293, 09:48:36\n",
      "Step: 99, MSE: 0.069598, PSNR: 11.574010, 09:48:37\n",
      "Step: 100, MSE: 0.069503, PSNR: 11.579991, 09:48:37\n",
      "Step: 101, MSE: 0.069441, PSNR: 11.583836, 09:48:37\n",
      "Step: 102, MSE: 0.069636, PSNR: 11.571689, 09:48:38\n",
      "Step: 103, MSE: 0.069627, PSNR: 11.572252, 09:48:38\n",
      "Step: 104, MSE: 0.069726, PSNR: 11.566067, 09:48:39\n",
      "Step: 105, MSE: 0.069426, PSNR: 11.584797, 09:48:39\n",
      "Step: 106, MSE: 0.069551, PSNR: 11.576969, 09:48:39\n",
      "Step: 107, MSE: 0.069490, PSNR: 11.580782, 09:48:40\n",
      "Step: 108, MSE: 0.069384, PSNR: 11.587391, 09:48:40\n",
      "Step: 109, MSE: 0.069729, PSNR: 11.565886, 09:48:40\n",
      "Step: 110, MSE: 0.069424, PSNR: 11.584875, 09:48:41\n",
      "Step: 111, MSE: 0.069741, PSNR: 11.565114, 09:48:41\n",
      "Step: 112, MSE: 0.069443, PSNR: 11.583747, 09:48:41\n",
      "Step: 113, MSE: 0.069429, PSNR: 11.584598, 09:48:42\n",
      "Step: 114, MSE: 0.069645, PSNR: 11.571079, 09:48:42\n",
      "Step: 115, MSE: 0.069633, PSNR: 11.571866, 09:48:42\n",
      "Step: 116, MSE: 0.069505, PSNR: 11.579848, 09:48:43\n",
      "Step: 117, MSE: 0.069326, PSNR: 11.591045, 09:48:43\n",
      "Step: 118, MSE: 0.069558, PSNR: 11.576505, 09:48:43\n",
      "Step: 119, MSE: 0.069559, PSNR: 11.576481, 09:48:44\n",
      "Step: 120, MSE: 0.069665, PSNR: 11.569862, 09:48:44\n",
      "Step: 121, MSE: 0.069793, PSNR: 11.561892, 09:48:45\n",
      "Step: 122, MSE: 0.069687, PSNR: 11.568455, 09:48:45\n",
      "Step: 123, MSE: 0.069564, PSNR: 11.576132, 09:48:45\n",
      "Step: 124, MSE: 0.069480, PSNR: 11.581382, 09:48:46\n",
      "Step: 125, MSE: 0.069753, PSNR: 11.564374, 09:48:46\n",
      "Step: 126, MSE: 0.069610, PSNR: 11.573261, 09:48:46\n",
      "Step: 127, MSE: 0.069650, PSNR: 11.570765, 09:48:47\n",
      "Step: 128, MSE: 0.069582, PSNR: 11.575013, 09:48:47\n",
      "Step: 129, MSE: 0.069564, PSNR: 11.576151, 09:48:47\n",
      "Step: 130, MSE: 0.069532, PSNR: 11.578174, 09:48:48\n",
      "Step: 131, MSE: 0.069507, PSNR: 11.579722, 09:48:48\n",
      "Step: 132, MSE: 0.069484, PSNR: 11.581155, 09:48:48\n",
      "Step: 133, MSE: 0.069770, PSNR: 11.563302, 09:48:49\n",
      "Step: 134, MSE: 0.069526, PSNR: 11.578541, 09:48:49\n",
      "Step: 135, MSE: 0.069628, PSNR: 11.572149, 09:48:49\n",
      "Step: 136, MSE: 0.069834, PSNR: 11.559320, 09:48:50\n",
      "Step: 137, MSE: 0.069657, PSNR: 11.570377, 09:48:50\n",
      "Step: 138, MSE: 0.069647, PSNR: 11.570946, 09:48:50\n",
      "Step: 139, MSE: 0.069571, PSNR: 11.575689, 09:48:51\n",
      "Step: 140, MSE: 0.069583, PSNR: 11.574955, 09:48:51\n",
      "Step: 141, MSE: 0.069593, PSNR: 11.574333, 09:48:51\n",
      "Step: 142, MSE: 0.069599, PSNR: 11.573956, 09:48:52\n",
      "Step: 143, MSE: 0.069723, PSNR: 11.566240, 09:48:52\n",
      "Step: 144, MSE: 0.069538, PSNR: 11.577763, 09:48:52\n",
      "Step: 145, MSE: 0.069593, PSNR: 11.574371, 09:48:53\n",
      "Step: 146, MSE: 0.069541, PSNR: 11.577572, 09:48:53\n",
      "Step: 147, MSE: 0.069546, PSNR: 11.577306, 09:48:53\n",
      "Step: 148, MSE: 0.069610, PSNR: 11.573256, 09:48:54\n",
      "Step: 149, MSE: 0.069707, PSNR: 11.567238, 09:48:54\n",
      "Step: 150, MSE: 0.069528, PSNR: 11.578392, 09:48:55\n",
      "Step: 151, MSE: 0.069600, PSNR: 11.573883, 09:48:55\n",
      "Step: 152, MSE: 0.069692, PSNR: 11.568188, 09:48:55\n",
      "Step: 153, MSE: 0.069567, PSNR: 11.575937, 09:48:56\n",
      "Step: 154, MSE: 0.069496, PSNR: 11.580402, 09:48:56\n",
      "Step: 155, MSE: 0.069671, PSNR: 11.569489, 09:48:56\n",
      "Step: 156, MSE: 0.069515, PSNR: 11.579237, 09:48:57\n",
      "Step: 157, MSE: 0.069609, PSNR: 11.573349, 09:48:57\n",
      "Step: 158, MSE: 0.069693, PSNR: 11.568109, 09:48:57\n",
      "Step: 159, MSE: 0.069311, PSNR: 11.591964, 09:48:58\n",
      "Step: 160, MSE: 0.069659, PSNR: 11.570200, 09:48:58\n",
      "Step: 161, MSE: 0.069750, PSNR: 11.564565, 09:48:58\n",
      "Step: 162, MSE: 0.069477, PSNR: 11.581600, 09:48:59\n",
      "Step: 163, MSE: 0.069777, PSNR: 11.562877, 09:48:59\n",
      "Step: 164, MSE: 0.069634, PSNR: 11.571808, 09:48:59\n",
      "Step: 165, MSE: 0.069666, PSNR: 11.569798, 09:49:00\n",
      "Step: 166, MSE: 0.069612, PSNR: 11.573167, 09:49:00\n",
      "Step: 167, MSE: 0.069652, PSNR: 11.570652, 09:49:01\n",
      "Step: 168, MSE: 0.069685, PSNR: 11.568600, 09:49:01\n",
      "Step: 169, MSE: 0.069709, PSNR: 11.567123, 09:49:01\n",
      "Step: 170, MSE: 0.069611, PSNR: 11.573206, 09:49:02\n",
      "Step: 171, MSE: 0.069647, PSNR: 11.570999, 09:49:02\n",
      "Step: 172, MSE: 0.069431, PSNR: 11.584435, 09:49:02\n",
      "Step: 173, MSE: 0.069579, PSNR: 11.575249, 09:49:03\n",
      "Step: 174, MSE: 0.069591, PSNR: 11.574490, 09:49:03\n",
      "Step: 175, MSE: 0.069684, PSNR: 11.568693, 09:49:03\n",
      "Step: 176, MSE: 0.069596, PSNR: 11.574134, 09:49:04\n",
      "Step: 177, MSE: 0.069609, PSNR: 11.573371, 09:49:04\n",
      "Step: 178, MSE: 0.069391, PSNR: 11.586957, 09:49:04\n",
      "Step: 179, MSE: 0.069329, PSNR: 11.590881, 09:49:05\n",
      "Step: 180, MSE: 0.069742, PSNR: 11.565066, 09:49:05\n",
      "Step: 181, MSE: 0.069498, PSNR: 11.580305, 09:49:05\n",
      "Step: 182, MSE: 0.069462, PSNR: 11.582498, 09:49:06\n",
      "Step: 183, MSE: 0.069473, PSNR: 11.581868, 09:49:06\n",
      "Step: 184, MSE: 0.069681, PSNR: 11.568828, 09:49:07\n",
      "Step: 185, MSE: 0.069640, PSNR: 11.571408, 09:49:07\n",
      "Step: 186, MSE: 0.069483, PSNR: 11.581200, 09:49:07\n",
      "Step: 187, MSE: 0.069597, PSNR: 11.574089, 09:49:08\n",
      "Step: 188, MSE: 0.069586, PSNR: 11.574802, 09:49:08\n",
      "Step: 189, MSE: 0.069619, PSNR: 11.572721, 09:49:08\n",
      "Step: 190, MSE: 0.069538, PSNR: 11.577777, 09:49:09\n",
      "Step: 191, MSE: 0.069607, PSNR: 11.573466, 09:49:09\n",
      "Step: 192, MSE: 0.069691, PSNR: 11.568208, 09:49:09\n",
      "Step: 193, MSE: 0.069520, PSNR: 11.578909, 09:49:10\n",
      "Step: 194, MSE: 0.069516, PSNR: 11.579134, 09:49:10\n",
      "Step: 195, MSE: 0.069634, PSNR: 11.571774, 09:49:11\n",
      "Step: 196, MSE: 0.069683, PSNR: 11.568713, 09:49:11\n",
      "Step: 197, MSE: 0.069455, PSNR: 11.582952, 09:49:11\n",
      "Step: 198, MSE: 0.069573, PSNR: 11.575593, 09:49:12\n",
      "Step: 199, MSE: 0.069696, PSNR: 11.567925, 09:49:12\n",
      "Step: 200, MSE: 0.069616, PSNR: 11.572914, 09:49:12\n",
      "Step: 201, MSE: 0.069682, PSNR: 11.568764, 09:49:13\n",
      "Step: 202, MSE: 0.069422, PSNR: 11.585058, 09:49:13\n",
      "Step: 203, MSE: 0.069619, PSNR: 11.572712, 09:49:13\n",
      "Step: 204, MSE: 0.069665, PSNR: 11.569868, 09:49:14\n",
      "Step: 205, MSE: 0.069582, PSNR: 11.575037, 09:49:14\n",
      "Step: 206, MSE: 0.069648, PSNR: 11.570936, 09:49:14\n",
      "Step: 207, MSE: 0.069628, PSNR: 11.572142, 09:49:15\n",
      "Step: 208, MSE: 0.069582, PSNR: 11.575049, 09:49:15\n",
      "Step: 209, MSE: 0.069577, PSNR: 11.575336, 09:49:15\n",
      "Step: 210, MSE: 0.069582, PSNR: 11.575061, 09:49:16\n",
      "Step: 211, MSE: 0.069791, PSNR: 11.561996, 09:49:16\n",
      "Step: 212, MSE: 0.069657, PSNR: 11.570375, 09:49:17\n",
      "Step: 213, MSE: 0.069522, PSNR: 11.578765, 09:49:17\n",
      "Step: 214, MSE: 0.069541, PSNR: 11.577562, 09:49:17\n",
      "Step: 215, MSE: 0.069821, PSNR: 11.560159, 09:49:18\n",
      "Step: 216, MSE: 0.069562, PSNR: 11.576307, 09:49:18\n",
      "Step: 217, MSE: 0.069612, PSNR: 11.573182, 09:49:18\n",
      "Step: 218, MSE: 0.069674, PSNR: 11.569314, 09:49:19\n",
      "Step: 219, MSE: 0.069562, PSNR: 11.576280, 09:49:19\n",
      "Step: 220, MSE: 0.069439, PSNR: 11.583977, 09:49:19\n",
      "Step: 221, MSE: 0.069524, PSNR: 11.578627, 09:49:20\n",
      "Step: 222, MSE: 0.069593, PSNR: 11.574316, 09:49:20\n",
      "Step: 223, MSE: 0.069394, PSNR: 11.586761, 09:49:20\n",
      "Step: 224, MSE: 0.069780, PSNR: 11.562715, 09:49:21\n",
      "Step: 225, MSE: 0.069653, PSNR: 11.570599, 09:49:21\n",
      "Step: 226, MSE: 0.069660, PSNR: 11.570171, 09:49:22\n",
      "Step: 227, MSE: 0.069568, PSNR: 11.575901, 09:49:22\n",
      "Step: 228, MSE: 0.069555, PSNR: 11.576742, 09:49:22\n",
      "Step: 229, MSE: 0.069688, PSNR: 11.568445, 09:49:23\n",
      "Step: 230, MSE: 0.069509, PSNR: 11.579594, 09:49:23\n",
      "Step: 231, MSE: 0.069425, PSNR: 11.584826, 09:49:23\n",
      "Step: 232, MSE: 0.069584, PSNR: 11.574887, 09:49:24\n",
      "Step: 233, MSE: 0.069653, PSNR: 11.570629, 09:49:24\n",
      "Step: 234, MSE: 0.069553, PSNR: 11.576813, 09:49:24\n",
      "Step: 235, MSE: 0.069661, PSNR: 11.570073, 09:49:25\n",
      "Step: 236, MSE: 0.069785, PSNR: 11.562349, 09:49:25\n",
      "Step: 237, MSE: 0.069605, PSNR: 11.573581, 09:49:25\n",
      "Step: 238, MSE: 0.069572, PSNR: 11.575643, 09:49:26\n",
      "Step: 239, MSE: 0.069716, PSNR: 11.566690, 09:49:26\n",
      "Step: 240, MSE: 0.069636, PSNR: 11.571679, 09:49:27\n",
      "Step: 241, MSE: 0.069396, PSNR: 11.586633, 09:49:27\n",
      "Step: 242, MSE: 0.069660, PSNR: 11.570163, 09:49:27\n",
      "Step: 243, MSE: 0.069532, PSNR: 11.578175, 09:49:28\n",
      "Step: 244, MSE: 0.069629, PSNR: 11.572122, 09:49:28\n",
      "Step: 245, MSE: 0.069519, PSNR: 11.578945, 09:49:28\n",
      "Step: 246, MSE: 0.069711, PSNR: 11.566993, 09:49:29\n",
      "Step: 247, MSE: 0.069617, PSNR: 11.572863, 09:49:29\n",
      "Step: 248, MSE: 0.069696, PSNR: 11.567945, 09:49:29\n",
      "Step: 249, MSE: 0.069568, PSNR: 11.575891, 09:49:30\n",
      "Step: 250, MSE: 0.069628, PSNR: 11.572131, 09:49:30\n",
      "Step: 251, MSE: 0.069677, PSNR: 11.569085, 09:49:30\n",
      "Step: 252, MSE: 0.069624, PSNR: 11.572436, 09:49:31\n",
      "Step: 253, MSE: 0.069582, PSNR: 11.575021, 09:49:31\n",
      "Step: 254, MSE: 0.069508, PSNR: 11.579646, 09:49:31\n",
      "Step: 255, MSE: 0.069588, PSNR: 11.574636, 09:49:32\n",
      "Step: 256, MSE: 0.069599, PSNR: 11.573953, 09:49:32\n",
      "Step: 257, MSE: 0.069541, PSNR: 11.577567, 09:49:33\n",
      "Step: 258, MSE: 0.069649, PSNR: 11.570850, 09:49:33\n",
      "Step: 259, MSE: 0.069802, PSNR: 11.561291, 09:49:33\n",
      "Step: 260, MSE: 0.069657, PSNR: 11.570338, 09:49:34\n",
      "Step: 261, MSE: 0.069569, PSNR: 11.575871, 09:49:34\n",
      "Step: 262, MSE: 0.069638, PSNR: 11.571539, 09:49:34\n",
      "Step: 263, MSE: 0.069455, PSNR: 11.582974, 09:49:35\n",
      "Step: 264, MSE: 0.069487, PSNR: 11.580965, 09:49:35\n",
      "Step: 265, MSE: 0.069482, PSNR: 11.581253, 09:49:35\n",
      "Step: 266, MSE: 0.069667, PSNR: 11.569750, 09:49:36\n",
      "Step: 267, MSE: 0.069515, PSNR: 11.579229, 09:49:36\n",
      "Step: 268, MSE: 0.069623, PSNR: 11.572454, 09:49:36\n",
      "Step: 269, MSE: 0.069602, PSNR: 11.573765, 09:49:37\n",
      "Step: 270, MSE: 0.069660, PSNR: 11.570150, 09:49:37\n",
      "Step: 271, MSE: 0.069512, PSNR: 11.579375, 09:49:37\n",
      "Step: 272, MSE: 0.069553, PSNR: 11.576825, 09:49:38\n",
      "Step: 273, MSE: 0.069632, PSNR: 11.571882, 09:49:38\n",
      "Step: 274, MSE: 0.069717, PSNR: 11.566602, 09:49:38\n",
      "Step: 275, MSE: 0.069466, PSNR: 11.582264, 09:49:39\n",
      "Step: 276, MSE: 0.069720, PSNR: 11.566450, 09:49:39\n",
      "Step: 277, MSE: 0.069643, PSNR: 11.571237, 09:49:39\n",
      "Step: 278, MSE: 0.069618, PSNR: 11.572811, 09:49:40\n",
      "Step: 279, MSE: 0.069593, PSNR: 11.574333, 09:49:40\n",
      "Step: 280, MSE: 0.069684, PSNR: 11.568651, 09:49:41\n",
      "Step: 281, MSE: 0.069487, PSNR: 11.580959, 09:49:41\n",
      "Step: 282, MSE: 0.069669, PSNR: 11.569597, 09:49:41\n",
      "Step: 283, MSE: 0.069628, PSNR: 11.572166, 09:49:42\n",
      "Step: 284, MSE: 0.069680, PSNR: 11.568946, 09:49:42\n",
      "Step: 285, MSE: 0.069613, PSNR: 11.573071, 09:49:42\n",
      "Step: 286, MSE: 0.069634, PSNR: 11.571815, 09:49:43\n",
      "Step: 287, MSE: 0.069782, PSNR: 11.562565, 09:49:43\n",
      "Step: 288, MSE: 0.069606, PSNR: 11.573558, 09:49:43\n",
      "Step: 289, MSE: 0.069623, PSNR: 11.572472, 09:49:44\n",
      "Step: 290, MSE: 0.069443, PSNR: 11.583717, 09:49:44\n",
      "Step: 291, MSE: 0.069439, PSNR: 11.583985, 09:49:44\n",
      "Step: 292, MSE: 0.069643, PSNR: 11.571240, 09:49:45\n",
      "Step: 293, MSE: 0.069593, PSNR: 11.574356, 09:49:45\n",
      "Step: 294, MSE: 0.069522, PSNR: 11.578803, 09:49:45\n",
      "Step: 295, MSE: 0.069664, PSNR: 11.569919, 09:49:46\n",
      "Step: 296, MSE: 0.069710, PSNR: 11.567019, 09:49:46\n",
      "Step: 297, MSE: 0.069423, PSNR: 11.584969, 09:49:46\n",
      "Step: 298, MSE: 0.069568, PSNR: 11.575924, 09:49:47\n",
      "Step: 299, MSE: 0.069537, PSNR: 11.577815, 09:49:47\n",
      "Step: 300, MSE: 0.069717, PSNR: 11.566610, 09:49:47\n",
      "Step: 301, MSE: 0.069514, PSNR: 11.579308, 09:49:48\n",
      "Step: 302, MSE: 0.069729, PSNR: 11.565855, 09:49:48\n",
      "Step: 303, MSE: 0.069503, PSNR: 11.579991, 09:49:48\n",
      "Step: 304, MSE: 0.069666, PSNR: 11.569801, 09:49:49\n",
      "Step: 305, MSE: 0.069665, PSNR: 11.569861, 09:49:49\n",
      "Step: 306, MSE: 0.069468, PSNR: 11.582170, 09:49:50\n",
      "Step: 307, MSE: 0.069613, PSNR: 11.573071, 09:49:50\n",
      "Step: 308, MSE: 0.069541, PSNR: 11.577592, 09:49:50\n",
      "Step: 309, MSE: 0.069626, PSNR: 11.572293, 09:49:51\n",
      "Step: 310, MSE: 0.069696, PSNR: 11.567912, 09:49:51\n",
      "Step: 311, MSE: 0.069445, PSNR: 11.583570, 09:49:51\n",
      "Step: 312, MSE: 0.069650, PSNR: 11.570815, 09:49:52\n",
      "Step: 313, MSE: 0.069581, PSNR: 11.575074, 09:49:52\n",
      "Step: 314, MSE: 0.069513, PSNR: 11.579352, 09:49:52\n",
      "Step: 315, MSE: 0.069429, PSNR: 11.584614, 09:49:53\n",
      "Step: 316, MSE: 0.069553, PSNR: 11.576817, 09:49:53\n",
      "Step: 317, MSE: 0.069487, PSNR: 11.580978, 09:49:53\n",
      "Step: 318, MSE: 0.069448, PSNR: 11.583389, 09:49:54\n",
      "Step: 319, MSE: 0.069595, PSNR: 11.574228, 09:49:54\n",
      "Step: 320, MSE: 0.069542, PSNR: 11.577510, 09:49:55\n",
      "Step: 321, MSE: 0.069598, PSNR: 11.574057, 09:49:55\n",
      "Step: 322, MSE: 0.069643, PSNR: 11.571226, 09:49:55\n",
      "Step: 323, MSE: 0.069729, PSNR: 11.565870, 09:49:56\n",
      "Step: 324, MSE: 0.069478, PSNR: 11.581520, 09:49:56\n",
      "Step: 325, MSE: 0.069551, PSNR: 11.576960, 09:49:56\n",
      "Step: 326, MSE: 0.069717, PSNR: 11.566610, 09:49:57\n",
      "Step: 327, MSE: 0.069614, PSNR: 11.573059, 09:49:57\n",
      "Step: 328, MSE: 0.069630, PSNR: 11.572057, 09:49:57\n",
      "Step: 329, MSE: 0.069672, PSNR: 11.569416, 09:49:58\n",
      "Step: 330, MSE: 0.069615, PSNR: 11.572982, 09:49:58\n",
      "Step: 331, MSE: 0.069634, PSNR: 11.571803, 09:49:58\n",
      "Step: 332, MSE: 0.069651, PSNR: 11.570739, 09:49:59\n",
      "Step: 333, MSE: 0.069409, PSNR: 11.585860, 09:49:59\n",
      "Step: 334, MSE: 0.069592, PSNR: 11.574394, 09:50:00\n",
      "Step: 335, MSE: 0.069475, PSNR: 11.581715, 09:50:00\n",
      "Step: 336, MSE: 0.069852, PSNR: 11.558237, 09:50:00\n",
      "Step: 337, MSE: 0.069559, PSNR: 11.576495, 09:50:01\n",
      "Step: 338, MSE: 0.069630, PSNR: 11.572027, 09:50:01\n",
      "Step: 339, MSE: 0.069518, PSNR: 11.579041, 09:50:01\n",
      "Step: 340, MSE: 0.069541, PSNR: 11.577589, 09:50:02\n",
      "Step: 341, MSE: 0.069436, PSNR: 11.584129, 09:50:02\n",
      "Step: 342, MSE: 0.069691, PSNR: 11.568233, 09:50:02\n",
      "Step: 343, MSE: 0.069692, PSNR: 11.568147, 09:50:03\n",
      "Step: 344, MSE: 0.069684, PSNR: 11.568660, 09:50:03\n",
      "Step: 345, MSE: 0.069598, PSNR: 11.574043, 09:50:03\n",
      "Step: 346, MSE: 0.069748, PSNR: 11.564713, 09:50:04\n",
      "Step: 347, MSE: 0.069559, PSNR: 11.576467, 09:50:04\n",
      "Step: 348, MSE: 0.069559, PSNR: 11.576479, 09:50:04\n",
      "Step: 349, MSE: 0.069617, PSNR: 11.572875, 09:50:05\n",
      "Step: 350, MSE: 0.069718, PSNR: 11.566581, 09:50:05\n",
      "Step: 351, MSE: 0.069704, PSNR: 11.567432, 09:50:06\n",
      "Step: 352, MSE: 0.069600, PSNR: 11.573919, 09:50:06\n",
      "Step: 353, MSE: 0.069732, PSNR: 11.565680, 09:50:06\n",
      "Step: 354, MSE: 0.069595, PSNR: 11.574203, 09:50:07\n",
      "Step: 355, MSE: 0.069567, PSNR: 11.575970, 09:50:07\n",
      "Step: 356, MSE: 0.069477, PSNR: 11.581573, 09:50:07\n",
      "Step: 357, MSE: 0.069734, PSNR: 11.565556, 09:50:08\n",
      "Step: 358, MSE: 0.069679, PSNR: 11.569000, 09:50:08\n",
      "Step: 359, MSE: 0.069512, PSNR: 11.579375, 09:50:08\n",
      "Step: 360, MSE: 0.069556, PSNR: 11.576666, 09:50:09\n",
      "Step: 361, MSE: 0.069593, PSNR: 11.574329, 09:50:09\n",
      "Step: 362, MSE: 0.069449, PSNR: 11.583351, 09:50:09\n",
      "Step: 363, MSE: 0.069465, PSNR: 11.582322, 09:50:10\n",
      "Step: 364, MSE: 0.069610, PSNR: 11.573314, 09:50:10\n",
      "Step: 365, MSE: 0.069597, PSNR: 11.574088, 09:50:10\n",
      "Step: 366, MSE: 0.069628, PSNR: 11.572178, 09:50:11\n",
      "Step: 367, MSE: 0.069476, PSNR: 11.581683, 09:50:11\n",
      "Step: 368, MSE: 0.069647, PSNR: 11.570951, 09:50:11\n",
      "Step: 369, MSE: 0.069745, PSNR: 11.564877, 09:50:12\n",
      "Step: 370, MSE: 0.069595, PSNR: 11.574215, 09:50:12\n",
      "Step: 371, MSE: 0.069590, PSNR: 11.574511, 09:50:12\n",
      "Step: 372, MSE: 0.069614, PSNR: 11.573043, 09:50:13\n",
      "Step: 373, MSE: 0.069651, PSNR: 11.570696, 09:50:13\n",
      "Step: 374, MSE: 0.069362, PSNR: 11.588773, 09:50:13\n",
      "Step: 375, MSE: 0.069528, PSNR: 11.578403, 09:50:14\n",
      "Step: 376, MSE: 0.069664, PSNR: 11.569943, 09:50:14\n",
      "Step: 377, MSE: 0.069652, PSNR: 11.570694, 09:50:14\n",
      "Step: 378, MSE: 0.069576, PSNR: 11.575376, 09:50:15\n",
      "Step: 379, MSE: 0.069650, PSNR: 11.570807, 09:50:15\n",
      "Step: 380, MSE: 0.069402, PSNR: 11.586265, 09:50:16\n",
      "Step: 381, MSE: 0.069652, PSNR: 11.570673, 09:50:16\n",
      "Step: 382, MSE: 0.069658, PSNR: 11.570295, 09:50:16\n",
      "Step: 383, MSE: 0.069473, PSNR: 11.581850, 09:50:17\n",
      "Step: 384, MSE: 0.069743, PSNR: 11.565009, 09:50:17\n",
      "Step: 385, MSE: 0.069623, PSNR: 11.572466, 09:50:17\n",
      "Step: 386, MSE: 0.069686, PSNR: 11.568560, 09:50:18\n",
      "Step: 387, MSE: 0.069703, PSNR: 11.567516, 09:50:18\n",
      "Step: 388, MSE: 0.069603, PSNR: 11.573716, 09:50:18\n",
      "Step: 389, MSE: 0.069559, PSNR: 11.576437, 09:50:19\n",
      "Step: 390, MSE: 0.069765, PSNR: 11.563631, 09:50:19\n",
      "Step: 391, MSE: 0.069665, PSNR: 11.569857, 09:50:19\n",
      "Step: 392, MSE: 0.069663, PSNR: 11.570006, 09:50:20\n",
      "Step: 393, MSE: 0.069668, PSNR: 11.569692, 09:50:20\n",
      "Step: 394, MSE: 0.069628, PSNR: 11.572179, 09:50:20\n",
      "Step: 395, MSE: 0.069686, PSNR: 11.568563, 09:50:21\n",
      "Step: 396, MSE: 0.069601, PSNR: 11.573870, 09:50:21\n",
      "Step: 397, MSE: 0.069604, PSNR: 11.573647, 09:50:21\n",
      "Step: 398, MSE: 0.069594, PSNR: 11.574259, 09:50:22\n",
      "Step: 399, MSE: 0.069585, PSNR: 11.574845, 09:50:22\n",
      "Step: 400, MSE: 0.069423, PSNR: 11.584954, 09:50:23\n",
      "Step: 401, MSE: 0.069580, PSNR: 11.575157, 09:50:23\n",
      "Step: 402, MSE: 0.069715, PSNR: 11.566754, 09:50:23\n",
      "Step: 403, MSE: 0.069578, PSNR: 11.575252, 09:50:24\n",
      "Step: 404, MSE: 0.069635, PSNR: 11.571709, 09:50:24\n",
      "Step: 405, MSE: 0.069556, PSNR: 11.576643, 09:50:24\n",
      "Step: 406, MSE: 0.069587, PSNR: 11.574736, 09:50:25\n",
      "Step: 407, MSE: 0.069389, PSNR: 11.587095, 09:50:25\n",
      "Step: 408, MSE: 0.069534, PSNR: 11.578012, 09:50:25\n",
      "Step: 409, MSE: 0.069671, PSNR: 11.569487, 09:50:26\n",
      "Step: 410, MSE: 0.069430, PSNR: 11.584534, 09:50:26\n",
      "Step: 411, MSE: 0.069573, PSNR: 11.575580, 09:50:26\n",
      "Step: 412, MSE: 0.069577, PSNR: 11.575322, 09:50:27\n",
      "Step: 413, MSE: 0.069450, PSNR: 11.583292, 09:50:27\n",
      "Step: 414, MSE: 0.069547, PSNR: 11.577235, 09:50:27\n",
      "Step: 415, MSE: 0.069589, PSNR: 11.574569, 09:50:28\n",
      "Step: 416, MSE: 0.069582, PSNR: 11.575060, 09:50:28\n",
      "Step: 417, MSE: 0.069542, PSNR: 11.577539, 09:50:28\n",
      "Step: 418, MSE: 0.069539, PSNR: 11.577684, 09:50:29\n",
      "Step: 419, MSE: 0.069506, PSNR: 11.579746, 09:50:29\n",
      "Step: 420, MSE: 0.069687, PSNR: 11.568459, 09:50:29\n",
      "Step: 421, MSE: 0.069503, PSNR: 11.579994, 09:50:30\n",
      "Step: 422, MSE: 0.069509, PSNR: 11.579604, 09:50:30\n",
      "Step: 423, MSE: 0.069536, PSNR: 11.577894, 09:50:31\n",
      "Step: 424, MSE: 0.069559, PSNR: 11.576476, 09:50:31\n",
      "Step: 425, MSE: 0.069719, PSNR: 11.566515, 09:50:31\n",
      "Step: 426, MSE: 0.069564, PSNR: 11.576145, 09:50:32\n",
      "Step: 427, MSE: 0.069560, PSNR: 11.576413, 09:50:32\n",
      "Step: 428, MSE: 0.069713, PSNR: 11.566868, 09:50:32\n",
      "Step: 429, MSE: 0.069453, PSNR: 11.583082, 09:50:33\n",
      "Step: 430, MSE: 0.069673, PSNR: 11.569345, 09:50:33\n",
      "Step: 431, MSE: 0.069655, PSNR: 11.570461, 09:50:33\n",
      "Step: 432, MSE: 0.069546, PSNR: 11.577299, 09:50:34\n",
      "Step: 433, MSE: 0.069591, PSNR: 11.574468, 09:50:34\n",
      "Step: 434, MSE: 0.069404, PSNR: 11.586182, 09:50:34\n",
      "Step: 435, MSE: 0.069715, PSNR: 11.566730, 09:50:35\n",
      "Step: 436, MSE: 0.069734, PSNR: 11.565527, 09:50:35\n",
      "Step: 437, MSE: 0.069564, PSNR: 11.576176, 09:50:35\n",
      "Step: 438, MSE: 0.069789, PSNR: 11.562126, 09:50:36\n",
      "Step: 439, MSE: 0.069590, PSNR: 11.574549, 09:50:36\n",
      "Step: 440, MSE: 0.069711, PSNR: 11.566958, 09:50:36\n",
      "Step: 441, MSE: 0.069599, PSNR: 11.573987, 09:50:37\n",
      "Step: 442, MSE: 0.069716, PSNR: 11.566700, 09:50:37\n",
      "Step: 443, MSE: 0.069724, PSNR: 11.566154, 09:50:37\n",
      "Step: 444, MSE: 0.069678, PSNR: 11.569036, 09:50:38\n",
      "Step: 445, MSE: 0.069655, PSNR: 11.570500, 09:50:38\n",
      "Step: 446, MSE: 0.069668, PSNR: 11.569666, 09:50:39\n",
      "Step: 447, MSE: 0.069454, PSNR: 11.583031, 09:50:39\n",
      "Step: 448, MSE: 0.069665, PSNR: 11.569853, 09:50:39\n",
      "Step: 449, MSE: 0.069591, PSNR: 11.574472, 09:50:40\n",
      "Step: 450, MSE: 0.069573, PSNR: 11.575615, 09:50:40\n",
      "Step: 451, MSE: 0.069459, PSNR: 11.582727, 09:50:40\n",
      "Step: 452, MSE: 0.069538, PSNR: 11.577783, 09:50:41\n",
      "Step: 453, MSE: 0.069503, PSNR: 11.579943, 09:50:41\n",
      "Step: 454, MSE: 0.069622, PSNR: 11.572528, 09:50:41\n",
      "Step: 455, MSE: 0.069638, PSNR: 11.571541, 09:50:42\n",
      "Step: 456, MSE: 0.069466, PSNR: 11.582298, 09:50:42\n",
      "Step: 457, MSE: 0.069641, PSNR: 11.571375, 09:50:42\n",
      "Step: 458, MSE: 0.069506, PSNR: 11.579776, 09:50:43\n",
      "Step: 459, MSE: 0.069587, PSNR: 11.574734, 09:50:43\n",
      "Step: 460, MSE: 0.069533, PSNR: 11.578116, 09:50:43\n",
      "Step: 461, MSE: 0.069586, PSNR: 11.574753, 09:50:44\n",
      "Step: 462, MSE: 0.069531, PSNR: 11.578194, 09:50:44\n",
      "Step: 463, MSE: 0.069649, PSNR: 11.570882, 09:50:45\n",
      "Step: 464, MSE: 0.069610, PSNR: 11.573253, 09:50:45\n",
      "Step: 465, MSE: 0.069576, PSNR: 11.575421, 09:50:45\n",
      "Step: 466, MSE: 0.069509, PSNR: 11.579596, 09:50:46\n",
      "Step: 467, MSE: 0.069712, PSNR: 11.566936, 09:50:46\n",
      "Step: 468, MSE: 0.069588, PSNR: 11.574625, 09:50:46\n",
      "Step: 469, MSE: 0.069427, PSNR: 11.584713, 09:50:47\n",
      "Step: 470, MSE: 0.069622, PSNR: 11.572533, 09:50:47\n",
      "Step: 471, MSE: 0.069585, PSNR: 11.574848, 09:50:47\n",
      "Step: 472, MSE: 0.069416, PSNR: 11.585430, 09:50:48\n",
      "Step: 473, MSE: 0.069578, PSNR: 11.575254, 09:50:48\n",
      "Step: 474, MSE: 0.069426, PSNR: 11.584780, 09:50:48\n",
      "Step: 475, MSE: 0.069512, PSNR: 11.579375, 09:50:49\n",
      "Step: 476, MSE: 0.069724, PSNR: 11.566201, 09:50:49\n",
      "Step: 477, MSE: 0.069698, PSNR: 11.567821, 09:50:50\n",
      "Step: 478, MSE: 0.069544, PSNR: 11.577414, 09:50:50\n",
      "Step: 479, MSE: 0.069630, PSNR: 11.572066, 09:50:50\n",
      "Step: 480, MSE: 0.069569, PSNR: 11.575843, 09:50:51\n",
      "Step: 481, MSE: 0.069705, PSNR: 11.567363, 09:50:51\n",
      "Step: 482, MSE: 0.069660, PSNR: 11.570158, 09:50:51\n",
      "Step: 483, MSE: 0.069594, PSNR: 11.574259, 09:50:52\n",
      "Step: 484, MSE: 0.069568, PSNR: 11.575922, 09:50:52\n",
      "Step: 485, MSE: 0.069609, PSNR: 11.573322, 09:50:52\n",
      "Step: 486, MSE: 0.069492, PSNR: 11.580679, 09:50:53\n",
      "Step: 487, MSE: 0.069440, PSNR: 11.583887, 09:50:53\n",
      "Step: 488, MSE: 0.069625, PSNR: 11.572327, 09:50:53\n",
      "Step: 489, MSE: 0.069737, PSNR: 11.565356, 09:50:54\n",
      "Step: 490, MSE: 0.069589, PSNR: 11.574568, 09:50:54\n",
      "Step: 491, MSE: 0.069713, PSNR: 11.566839, 09:50:54\n",
      "Step: 492, MSE: 0.069675, PSNR: 11.569215, 09:50:55\n",
      "Step: 493, MSE: 0.069747, PSNR: 11.564755, 09:50:55\n",
      "Step: 494, MSE: 0.069634, PSNR: 11.571809, 09:50:55\n",
      "Step: 495, MSE: 0.069569, PSNR: 11.575872, 09:50:56\n",
      "Step: 496, MSE: 0.069601, PSNR: 11.573835, 09:50:56\n",
      "Step: 497, MSE: 0.069677, PSNR: 11.569132, 09:50:57\n",
      "Step: 498, MSE: 0.069576, PSNR: 11.575402, 09:50:57\n",
      "Step: 499, MSE: 0.069631, PSNR: 11.571962, 09:50:57\n",
      "Step: 500, MSE: 0.069544, PSNR: 11.577397, 09:50:58\n",
      "Step: 501, MSE: 0.069684, PSNR: 11.568693, 09:50:58\n",
      "Step: 502, MSE: 0.069623, PSNR: 11.572453, 09:50:58\n",
      "Step: 503, MSE: 0.069632, PSNR: 11.571898, 09:50:59\n",
      "Step: 504, MSE: 0.069538, PSNR: 11.577750, 09:50:59\n",
      "Step: 505, MSE: 0.069615, PSNR: 11.572991, 09:50:59\n",
      "Step: 506, MSE: 0.069680, PSNR: 11.568924, 09:51:00\n",
      "Step: 507, MSE: 0.069506, PSNR: 11.579779, 09:51:00\n",
      "Step: 508, MSE: 0.069533, PSNR: 11.578098, 09:51:00\n",
      "Step: 509, MSE: 0.069648, PSNR: 11.570916, 09:51:01\n",
      "Step: 510, MSE: 0.069510, PSNR: 11.579529, 09:51:01\n",
      "Step: 511, MSE: 0.069569, PSNR: 11.575855, 09:51:02\n",
      "Step: 512, MSE: 0.069674, PSNR: 11.569263, 09:51:02\n",
      "Step: 513, MSE: 0.069540, PSNR: 11.577642, 09:51:02\n",
      "Step: 514, MSE: 0.069642, PSNR: 11.571298, 09:51:03\n",
      "Step: 515, MSE: 0.069743, PSNR: 11.565022, 09:51:03\n",
      "Step: 516, MSE: 0.069645, PSNR: 11.571104, 09:51:03\n",
      "Step: 517, MSE: 0.069342, PSNR: 11.590014, 09:51:04\n",
      "Step: 518, MSE: 0.069851, PSNR: 11.558295, 09:51:04\n",
      "Step: 519, MSE: 0.069656, PSNR: 11.570405, 09:51:04\n",
      "Step: 520, MSE: 0.069344, PSNR: 11.589907, 09:51:05\n",
      "Step: 521, MSE: 0.069765, PSNR: 11.563654, 09:51:05\n",
      "Step: 522, MSE: 0.069858, PSNR: 11.557829, 09:51:05\n",
      "Step: 523, MSE: 0.069527, PSNR: 11.578461, 09:51:06\n",
      "Step: 524, MSE: 0.069617, PSNR: 11.572824, 09:51:06\n",
      "Step: 525, MSE: 0.069605, PSNR: 11.573573, 09:51:06\n",
      "Step: 526, MSE: 0.069570, PSNR: 11.575768, 09:51:07\n",
      "Step: 527, MSE: 0.069495, PSNR: 11.580479, 09:51:07\n",
      "Step: 528, MSE: 0.069580, PSNR: 11.575150, 09:51:07\n",
      "Step: 529, MSE: 0.069580, PSNR: 11.575126, 09:51:08\n",
      "Step: 530, MSE: 0.069662, PSNR: 11.570067, 09:51:08\n",
      "Step: 531, MSE: 0.069595, PSNR: 11.574234, 09:51:08\n",
      "Step: 532, MSE: 0.069577, PSNR: 11.575369, 09:51:09\n",
      "Step: 533, MSE: 0.069635, PSNR: 11.571703, 09:51:09\n",
      "Step: 534, MSE: 0.069699, PSNR: 11.567737, 09:51:09\n",
      "Step: 535, MSE: 0.069715, PSNR: 11.566768, 09:51:10\n",
      "Step: 536, MSE: 0.069638, PSNR: 11.571545, 09:51:10\n",
      "Step: 537, MSE: 0.069708, PSNR: 11.567172, 09:51:10\n",
      "Step: 538, MSE: 0.069521, PSNR: 11.578812, 09:51:11\n",
      "Step: 539, MSE: 0.069657, PSNR: 11.570368, 09:51:11\n",
      "Step: 540, MSE: 0.069429, PSNR: 11.584589, 09:51:11\n",
      "Step: 541, MSE: 0.069545, PSNR: 11.577319, 09:51:12\n",
      "Step: 542, MSE: 0.069595, PSNR: 11.574229, 09:51:12\n",
      "Step: 543, MSE: 0.069585, PSNR: 11.574845, 09:51:13\n",
      "Step: 544, MSE: 0.069631, PSNR: 11.571949, 09:51:13\n",
      "Step: 545, MSE: 0.069633, PSNR: 11.571847, 09:51:13\n",
      "Step: 546, MSE: 0.069667, PSNR: 11.569758, 09:51:14\n",
      "Step: 547, MSE: 0.069574, PSNR: 11.575548, 09:51:14\n",
      "Step: 548, MSE: 0.069438, PSNR: 11.584051, 09:51:14\n",
      "Step: 549, MSE: 0.069595, PSNR: 11.574191, 09:51:15\n",
      "Step: 550, MSE: 0.069613, PSNR: 11.573087, 09:51:15\n",
      "Step: 551, MSE: 0.069648, PSNR: 11.570897, 09:51:15\n",
      "Step: 552, MSE: 0.069606, PSNR: 11.573525, 09:51:16\n",
      "Step: 553, MSE: 0.069661, PSNR: 11.570124, 09:51:16\n",
      "Step: 554, MSE: 0.069553, PSNR: 11.576842, 09:51:16\n",
      "Step: 555, MSE: 0.069514, PSNR: 11.579277, 09:51:17\n",
      "Step: 556, MSE: 0.069696, PSNR: 11.567904, 09:51:17\n",
      "Step: 557, MSE: 0.069718, PSNR: 11.566580, 09:51:17\n",
      "Step: 558, MSE: 0.069292, PSNR: 11.593149, 09:51:18\n",
      "Step: 559, MSE: 0.069574, PSNR: 11.575541, 09:51:18\n",
      "Step: 560, MSE: 0.069622, PSNR: 11.572551, 09:51:18\n",
      "Step: 561, MSE: 0.069547, PSNR: 11.577243, 09:51:19\n",
      "Step: 562, MSE: 0.069693, PSNR: 11.568078, 09:51:19\n",
      "Step: 563, MSE: 0.069513, PSNR: 11.579360, 09:51:20\n",
      "Step: 564, MSE: 0.069569, PSNR: 11.575872, 09:51:20\n",
      "Step: 565, MSE: 0.069558, PSNR: 11.576559, 09:51:20\n",
      "Step: 566, MSE: 0.069579, PSNR: 11.575208, 09:51:21\n",
      "Step: 567, MSE: 0.069522, PSNR: 11.578766, 09:51:21\n",
      "Step: 568, MSE: 0.069555, PSNR: 11.576712, 09:51:21\n",
      "Step: 569, MSE: 0.069529, PSNR: 11.578341, 09:51:22\n",
      "Step: 570, MSE: 0.069704, PSNR: 11.567432, 09:51:22\n",
      "Step: 571, MSE: 0.069765, PSNR: 11.563621, 09:51:22\n",
      "Step: 572, MSE: 0.069525, PSNR: 11.578573, 09:51:23\n",
      "Step: 573, MSE: 0.069576, PSNR: 11.575428, 09:51:23\n",
      "Step: 574, MSE: 0.069584, PSNR: 11.574909, 09:51:23\n",
      "Step: 575, MSE: 0.069422, PSNR: 11.584998, 09:51:24\n",
      "Step: 576, MSE: 0.069506, PSNR: 11.579758, 09:51:24\n",
      "Step: 577, MSE: 0.069609, PSNR: 11.573333, 09:51:24\n",
      "Step: 578, MSE: 0.069685, PSNR: 11.568612, 09:51:25\n",
      "Step: 579, MSE: 0.069548, PSNR: 11.577168, 09:51:25\n",
      "Step: 580, MSE: 0.069515, PSNR: 11.579242, 09:51:25\n",
      "Step: 581, MSE: 0.069462, PSNR: 11.582550, 09:51:26\n",
      "Step: 582, MSE: 0.069585, PSNR: 11.574860, 09:51:26\n",
      "Step: 583, MSE: 0.069582, PSNR: 11.575055, 09:51:27\n",
      "Step: 584, MSE: 0.069638, PSNR: 11.571537, 09:51:27\n",
      "Step: 585, MSE: 0.069659, PSNR: 11.570258, 09:51:27\n",
      "Step: 586, MSE: 0.069583, PSNR: 11.574965, 09:51:28\n",
      "Step: 587, MSE: 0.069582, PSNR: 11.575002, 09:51:28\n",
      "Step: 588, MSE: 0.069483, PSNR: 11.581228, 09:51:28\n",
      "Step: 589, MSE: 0.069676, PSNR: 11.569141, 09:51:29\n",
      "Step: 590, MSE: 0.069433, PSNR: 11.584322, 09:51:29\n",
      "Step: 591, MSE: 0.069508, PSNR: 11.579643, 09:51:29\n",
      "Step: 592, MSE: 0.069338, PSNR: 11.590279, 09:51:30\n",
      "Step: 593, MSE: 0.069398, PSNR: 11.586517, 09:51:30\n",
      "Step: 594, MSE: 0.069542, PSNR: 11.577539, 09:51:30\n",
      "Step: 595, MSE: 0.069604, PSNR: 11.573634, 09:51:31\n",
      "Step: 596, MSE: 0.069489, PSNR: 11.580820, 09:51:31\n",
      "Step: 597, MSE: 0.069641, PSNR: 11.571359, 09:51:31\n",
      "Step: 598, MSE: 0.069523, PSNR: 11.578714, 09:51:32\n",
      "Step: 599, MSE: 0.069592, PSNR: 11.574391, 09:51:32\n",
      "Step: 600, MSE: 0.069581, PSNR: 11.575071, 09:51:32\n",
      "Step: 601, MSE: 0.069563, PSNR: 11.576187, 09:51:33\n",
      "Step: 602, MSE: 0.069687, PSNR: 11.568457, 09:51:33\n",
      "Step: 603, MSE: 0.069629, PSNR: 11.572100, 09:51:33\n",
      "Step: 604, MSE: 0.069678, PSNR: 11.569066, 09:51:34\n",
      "Step: 605, MSE: 0.069395, PSNR: 11.586729, 09:51:34\n",
      "Step: 606, MSE: 0.069731, PSNR: 11.565723, 09:51:34\n",
      "Step: 607, MSE: 0.069559, PSNR: 11.576468, 09:51:35\n",
      "Step: 608, MSE: 0.069466, PSNR: 11.582297, 09:51:35\n",
      "Step: 609, MSE: 0.069729, PSNR: 11.565849, 09:51:35\n",
      "Step: 610, MSE: 0.069499, PSNR: 11.580240, 09:51:36\n",
      "Step: 611, MSE: 0.069668, PSNR: 11.569681, 09:51:36\n",
      "Step: 612, MSE: 0.069694, PSNR: 11.568032, 09:51:37\n",
      "Step: 613, MSE: 0.069524, PSNR: 11.578629, 09:51:37\n",
      "Step: 614, MSE: 0.069503, PSNR: 11.579956, 09:51:37\n",
      "Step: 615, MSE: 0.069827, PSNR: 11.559738, 09:51:38\n",
      "Step: 616, MSE: 0.069623, PSNR: 11.572495, 09:51:38\n",
      "Step: 617, MSE: 0.069555, PSNR: 11.576729, 09:51:38\n",
      "Step: 618, MSE: 0.069410, PSNR: 11.585800, 09:51:39\n",
      "Step: 619, MSE: 0.069504, PSNR: 11.579912, 09:51:39\n",
      "Step: 620, MSE: 0.069450, PSNR: 11.583289, 09:51:39\n",
      "Step: 621, MSE: 0.069742, PSNR: 11.565038, 09:51:40\n",
      "Step: 622, MSE: 0.069728, PSNR: 11.565907, 09:51:40\n",
      "Step: 623, MSE: 0.069494, PSNR: 11.580498, 09:51:41\n",
      "Step: 624, MSE: 0.069707, PSNR: 11.567209, 09:51:41\n",
      "Step: 625, MSE: 0.069675, PSNR: 11.569235, 09:51:41\n",
      "Step: 626, MSE: 0.069720, PSNR: 11.566444, 09:51:42\n",
      "Step: 627, MSE: 0.069567, PSNR: 11.575965, 09:51:42\n",
      "Step: 628, MSE: 0.069670, PSNR: 11.569528, 09:51:42\n",
      "Step: 629, MSE: 0.069646, PSNR: 11.571065, 09:51:42\n",
      "Step: 630, MSE: 0.069442, PSNR: 11.583796, 09:51:43\n",
      "Step: 631, MSE: 0.069676, PSNR: 11.569178, 09:51:43\n",
      "Step: 632, MSE: 0.069503, PSNR: 11.579996, 09:51:44\n",
      "Step: 633, MSE: 0.069799, PSNR: 11.561479, 09:51:44\n",
      "Step: 634, MSE: 0.069474, PSNR: 11.581795, 09:51:44\n",
      "Step: 635, MSE: 0.069553, PSNR: 11.576830, 09:51:45\n",
      "Step: 636, MSE: 0.069520, PSNR: 11.578913, 09:51:45\n",
      "Step: 637, MSE: 0.069470, PSNR: 11.582033, 09:51:45\n",
      "Step: 638, MSE: 0.069666, PSNR: 11.569782, 09:51:46\n",
      "Step: 639, MSE: 0.069585, PSNR: 11.574827, 09:51:46\n",
      "Step: 640, MSE: 0.069524, PSNR: 11.578654, 09:51:46\n",
      "Step: 641, MSE: 0.069488, PSNR: 11.580888, 09:51:47\n",
      "Step: 642, MSE: 0.069873, PSNR: 11.556917, 09:51:47\n",
      "Step: 643, MSE: 0.069655, PSNR: 11.570452, 09:51:47\n",
      "Step: 644, MSE: 0.069770, PSNR: 11.563344, 09:51:48\n",
      "Step: 645, MSE: 0.069661, PSNR: 11.570093, 09:51:48\n",
      "Step: 646, MSE: 0.069550, PSNR: 11.577003, 09:51:48\n",
      "Step: 647, MSE: 0.069565, PSNR: 11.576067, 09:51:49\n",
      "Step: 648, MSE: 0.069628, PSNR: 11.572188, 09:51:49\n",
      "Step: 649, MSE: 0.069546, PSNR: 11.577296, 09:51:49\n",
      "Step: 650, MSE: 0.069538, PSNR: 11.577807, 09:51:50\n",
      "Step: 651, MSE: 0.069581, PSNR: 11.575069, 09:51:50\n",
      "Step: 652, MSE: 0.069676, PSNR: 11.569152, 09:51:50\n",
      "Step: 653, MSE: 0.069517, PSNR: 11.579107, 09:51:51\n",
      "Step: 654, MSE: 0.069464, PSNR: 11.582417, 09:51:51\n",
      "Step: 655, MSE: 0.069544, PSNR: 11.577381, 09:51:51\n",
      "Step: 656, MSE: 0.069426, PSNR: 11.584810, 09:51:52\n",
      "Step: 657, MSE: 0.069748, PSNR: 11.564682, 09:51:52\n",
      "Step: 658, MSE: 0.069610, PSNR: 11.573310, 09:51:52\n",
      "Step: 659, MSE: 0.069783, PSNR: 11.562487, 09:51:53\n",
      "Step: 660, MSE: 0.069649, PSNR: 11.570879, 09:51:53\n",
      "Step: 661, MSE: 0.069495, PSNR: 11.580482, 09:51:54\n",
      "Step: 662, MSE: 0.069571, PSNR: 11.575727, 09:51:54\n",
      "Step: 663, MSE: 0.069626, PSNR: 11.572296, 09:51:54\n",
      "Step: 664, MSE: 0.069444, PSNR: 11.583677, 09:51:55\n",
      "Step: 665, MSE: 0.069555, PSNR: 11.576704, 09:51:55\n",
      "Step: 666, MSE: 0.069564, PSNR: 11.576136, 09:51:55\n",
      "Step: 667, MSE: 0.069533, PSNR: 11.578091, 09:51:56\n",
      "Step: 668, MSE: 0.069622, PSNR: 11.572533, 09:51:56\n",
      "Step: 669, MSE: 0.069574, PSNR: 11.575548, 09:51:56\n",
      "Step: 670, MSE: 0.069429, PSNR: 11.584620, 09:51:57\n",
      "Step: 671, MSE: 0.069468, PSNR: 11.582134, 09:51:57\n",
      "Step: 672, MSE: 0.069766, PSNR: 11.563538, 09:51:57\n",
      "Step: 673, MSE: 0.069513, PSNR: 11.579366, 09:51:58\n",
      "Step: 674, MSE: 0.069571, PSNR: 11.575699, 09:51:58\n",
      "Step: 675, MSE: 0.069455, PSNR: 11.582949, 09:51:58\n",
      "Step: 676, MSE: 0.069617, PSNR: 11.572837, 09:51:59\n",
      "Step: 677, MSE: 0.069673, PSNR: 11.569384, 09:51:59\n",
      "Step: 678, MSE: 0.069570, PSNR: 11.575751, 09:52:00\n",
      "Step: 679, MSE: 0.069464, PSNR: 11.582390, 09:52:00\n",
      "Step: 680, MSE: 0.069544, PSNR: 11.577379, 09:52:00\n",
      "Step: 681, MSE: 0.069619, PSNR: 11.572746, 09:52:01\n",
      "Step: 682, MSE: 0.069606, PSNR: 11.573536, 09:52:01\n",
      "Step: 683, MSE: 0.069574, PSNR: 11.575513, 09:52:01\n",
      "Step: 684, MSE: 0.069505, PSNR: 11.579833, 09:52:02\n",
      "Step: 685, MSE: 0.069750, PSNR: 11.564547, 09:52:02\n",
      "Step: 686, MSE: 0.069653, PSNR: 11.570582, 09:52:02\n",
      "Step: 687, MSE: 0.069463, PSNR: 11.582469, 09:52:03\n",
      "Step: 688, MSE: 0.069447, PSNR: 11.583486, 09:52:03\n",
      "Step: 689, MSE: 0.069552, PSNR: 11.576882, 09:52:03\n",
      "Step: 690, MSE: 0.069648, PSNR: 11.570942, 09:52:04\n",
      "Step: 691, MSE: 0.069648, PSNR: 11.570884, 09:52:04\n",
      "Step: 692, MSE: 0.069509, PSNR: 11.579610, 09:52:05\n",
      "Step: 693, MSE: 0.069491, PSNR: 11.580698, 09:52:05\n",
      "Step: 694, MSE: 0.069684, PSNR: 11.568688, 09:52:05\n",
      "Step: 695, MSE: 0.069617, PSNR: 11.572859, 09:52:06\n",
      "Step: 696, MSE: 0.069536, PSNR: 11.577909, 09:52:06\n",
      "Step: 697, MSE: 0.069581, PSNR: 11.575121, 09:52:06\n",
      "Step: 698, MSE: 0.069578, PSNR: 11.575309, 09:52:07\n",
      "Step: 699, MSE: 0.069712, PSNR: 11.566936, 09:52:07\n",
      "Step: 700, MSE: 0.069548, PSNR: 11.577185, 09:52:07\n",
      "Step: 701, MSE: 0.069607, PSNR: 11.573495, 09:52:08\n",
      "Step: 702, MSE: 0.069445, PSNR: 11.583599, 09:52:08\n",
      "Step: 703, MSE: 0.069565, PSNR: 11.576096, 09:52:08\n",
      "Step: 704, MSE: 0.069789, PSNR: 11.562148, 09:52:09\n",
      "Step: 705, MSE: 0.069507, PSNR: 11.579731, 09:52:09\n",
      "Step: 706, MSE: 0.069502, PSNR: 11.580038, 09:52:10\n",
      "Step: 707, MSE: 0.069703, PSNR: 11.567454, 09:52:10\n",
      "Step: 708, MSE: 0.069774, PSNR: 11.563036, 09:52:10\n",
      "Step: 709, MSE: 0.069588, PSNR: 11.574687, 09:52:11\n",
      "Step: 710, MSE: 0.069569, PSNR: 11.575855, 09:52:11\n",
      "Step: 711, MSE: 0.069436, PSNR: 11.584179, 09:52:11\n",
      "Step: 712, MSE: 0.069802, PSNR: 11.561316, 09:52:12\n",
      "Step: 713, MSE: 0.069544, PSNR: 11.577395, 09:52:12\n",
      "Step: 714, MSE: 0.069661, PSNR: 11.570074, 09:52:12\n",
      "Step: 715, MSE: 0.069585, PSNR: 11.574854, 09:52:13\n",
      "Step: 716, MSE: 0.069600, PSNR: 11.573923, 09:52:13\n",
      "Step: 717, MSE: 0.069567, PSNR: 11.575977, 09:52:13\n",
      "Step: 718, MSE: 0.069735, PSNR: 11.565468, 09:52:14\n",
      "Step: 719, MSE: 0.069450, PSNR: 11.583272, 09:52:14\n",
      "Step: 720, MSE: 0.069666, PSNR: 11.569768, 09:52:14\n",
      "Step: 721, MSE: 0.069837, PSNR: 11.559134, 09:52:15\n",
      "Step: 722, MSE: 0.069710, PSNR: 11.567041, 09:52:15\n",
      "Step: 723, MSE: 0.069581, PSNR: 11.575112, 09:52:15\n",
      "Step: 724, MSE: 0.069516, PSNR: 11.579131, 09:52:16\n",
      "Step: 725, MSE: 0.069589, PSNR: 11.574600, 09:52:16\n",
      "Step: 726, MSE: 0.069520, PSNR: 11.578874, 09:52:17\n",
      "Step: 727, MSE: 0.069645, PSNR: 11.571089, 09:52:17\n",
      "Step: 728, MSE: 0.069748, PSNR: 11.564699, 09:52:17\n",
      "Step: 729, MSE: 0.069623, PSNR: 11.572456, 09:52:18\n",
      "Step: 730, MSE: 0.069579, PSNR: 11.575212, 09:52:18\n",
      "Step: 731, MSE: 0.069873, PSNR: 11.556900, 09:52:18\n",
      "Step: 732, MSE: 0.069454, PSNR: 11.583000, 09:52:19\n",
      "Step: 733, MSE: 0.069570, PSNR: 11.575783, 09:52:19\n",
      "Step: 734, MSE: 0.069567, PSNR: 11.575965, 09:52:19\n",
      "Step: 735, MSE: 0.069629, PSNR: 11.572076, 09:52:20\n",
      "Step: 736, MSE: 0.069675, PSNR: 11.569254, 09:52:20\n",
      "Step: 737, MSE: 0.069586, PSNR: 11.574781, 09:52:20\n",
      "Step: 738, MSE: 0.069485, PSNR: 11.581111, 09:52:21\n",
      "Step: 739, MSE: 0.069694, PSNR: 11.568029, 09:52:21\n",
      "Step: 740, MSE: 0.069750, PSNR: 11.564569, 09:52:21\n",
      "Step: 741, MSE: 0.069512, PSNR: 11.579426, 09:52:22\n",
      "Step: 742, MSE: 0.069575, PSNR: 11.575465, 09:52:22\n",
      "Step: 743, MSE: 0.069408, PSNR: 11.585912, 09:52:23\n",
      "Step: 744, MSE: 0.069524, PSNR: 11.578670, 09:52:23\n",
      "Step: 745, MSE: 0.069640, PSNR: 11.571406, 09:52:23\n",
      "Step: 746, MSE: 0.069583, PSNR: 11.574988, 09:52:24\n",
      "Step: 747, MSE: 0.069521, PSNR: 11.578812, 09:52:24\n",
      "Step: 748, MSE: 0.069701, PSNR: 11.567632, 09:52:24\n",
      "Step: 749, MSE: 0.069601, PSNR: 11.573826, 09:52:25\n",
      "Step: 750, MSE: 0.069536, PSNR: 11.577930, 09:52:25\n",
      "Step: 751, MSE: 0.069729, PSNR: 11.565836, 09:52:25\n",
      "Step: 752, MSE: 0.069797, PSNR: 11.561624, 09:52:26\n",
      "Step: 753, MSE: 0.069506, PSNR: 11.579747, 09:52:26\n",
      "Step: 754, MSE: 0.069710, PSNR: 11.567044, 09:52:26\n",
      "Step: 755, MSE: 0.069606, PSNR: 11.573558, 09:52:27\n",
      "Step: 756, MSE: 0.069515, PSNR: 11.579246, 09:52:27\n",
      "Step: 757, MSE: 0.069546, PSNR: 11.577310, 09:52:27\n",
      "Step: 758, MSE: 0.069594, PSNR: 11.574303, 09:52:28\n",
      "Step: 759, MSE: 0.069681, PSNR: 11.568828, 09:52:28\n",
      "Step: 760, MSE: 0.069441, PSNR: 11.583815, 09:52:29\n",
      "Step: 761, MSE: 0.069737, PSNR: 11.565384, 09:52:29\n",
      "Step: 762, MSE: 0.069481, PSNR: 11.581362, 09:52:29\n",
      "Step: 763, MSE: 0.069653, PSNR: 11.570581, 09:52:30\n",
      "Step: 764, MSE: 0.069549, PSNR: 11.577068, 09:52:30\n",
      "Step: 765, MSE: 0.069658, PSNR: 11.570288, 09:52:30\n",
      "Step: 766, MSE: 0.069597, PSNR: 11.574105, 09:52:31\n",
      "Step: 767, MSE: 0.069712, PSNR: 11.566925, 09:52:31\n",
      "Step: 768, MSE: 0.069735, PSNR: 11.565467, 09:52:31\n",
      "Step: 769, MSE: 0.069574, PSNR: 11.575554, 09:52:32\n",
      "Step: 770, MSE: 0.069643, PSNR: 11.571251, 09:52:32\n",
      "Step: 771, MSE: 0.069592, PSNR: 11.574411, 09:52:32\n",
      "Step: 772, MSE: 0.069677, PSNR: 11.569134, 09:52:33\n",
      "Step: 773, MSE: 0.069694, PSNR: 11.568030, 09:52:33\n",
      "Step: 774, MSE: 0.069738, PSNR: 11.565327, 09:52:33\n",
      "Step: 775, MSE: 0.069548, PSNR: 11.577130, 09:52:34\n",
      "Step: 776, MSE: 0.069409, PSNR: 11.585832, 09:52:34\n",
      "Step: 777, MSE: 0.069655, PSNR: 11.570456, 09:52:34\n",
      "Step: 778, MSE: 0.069565, PSNR: 11.576114, 09:52:35\n",
      "Step: 779, MSE: 0.069577, PSNR: 11.575373, 09:52:35\n",
      "Step: 780, MSE: 0.069536, PSNR: 11.577893, 09:52:35\n",
      "Step: 781, MSE: 0.069533, PSNR: 11.578106, 09:52:36\n",
      "Step: 782, MSE: 0.069482, PSNR: 11.581255, 09:52:36\n",
      "Step: 783, MSE: 0.069679, PSNR: 11.568953, 09:52:37\n",
      "Step: 784, MSE: 0.069702, PSNR: 11.567531, 09:52:37\n",
      "Step: 785, MSE: 0.069669, PSNR: 11.569585, 09:52:37\n",
      "Step: 786, MSE: 0.069551, PSNR: 11.576977, 09:52:38\n",
      "Step: 787, MSE: 0.069618, PSNR: 11.572804, 09:52:38\n",
      "Step: 788, MSE: 0.069426, PSNR: 11.584788, 09:52:38\n",
      "Step: 789, MSE: 0.069599, PSNR: 11.573968, 09:52:39\n",
      "Step: 790, MSE: 0.069667, PSNR: 11.569733, 09:52:39\n",
      "Step: 791, MSE: 0.069629, PSNR: 11.572118, 09:52:39\n",
      "Step: 792, MSE: 0.069680, PSNR: 11.568901, 09:52:40\n",
      "Step: 793, MSE: 0.069478, PSNR: 11.581497, 09:52:40\n",
      "Step: 794, MSE: 0.069505, PSNR: 11.579855, 09:52:40\n",
      "Step: 795, MSE: 0.069532, PSNR: 11.578146, 09:52:41\n",
      "Step: 796, MSE: 0.069514, PSNR: 11.579260, 09:52:41\n",
      "Step: 797, MSE: 0.069733, PSNR: 11.565639, 09:52:42\n",
      "Step: 798, MSE: 0.069531, PSNR: 11.578240, 09:52:42\n",
      "Step: 799, MSE: 0.069590, PSNR: 11.574522, 09:52:42\n",
      "Step: 800, MSE: 0.069572, PSNR: 11.575636, 09:52:43\n",
      "Step: 801, MSE: 0.069642, PSNR: 11.571314, 09:52:43\n",
      "Step: 802, MSE: 0.069680, PSNR: 11.568932, 09:52:43\n",
      "Step: 803, MSE: 0.069458, PSNR: 11.582761, 09:52:44\n",
      "Step: 804, MSE: 0.069602, PSNR: 11.573752, 09:52:44\n",
      "Step: 805, MSE: 0.069679, PSNR: 11.568956, 09:52:44\n",
      "Step: 806, MSE: 0.069685, PSNR: 11.568608, 09:52:45\n",
      "Step: 807, MSE: 0.069596, PSNR: 11.574187, 09:52:45\n",
      "Step: 808, MSE: 0.069643, PSNR: 11.571197, 09:52:45\n",
      "Step: 809, MSE: 0.069667, PSNR: 11.569744, 09:52:46\n",
      "Step: 810, MSE: 0.069608, PSNR: 11.573430, 09:52:46\n",
      "Step: 811, MSE: 0.069581, PSNR: 11.575112, 09:52:46\n",
      "Step: 812, MSE: 0.069535, PSNR: 11.577977, 09:52:47\n",
      "Step: 813, MSE: 0.069629, PSNR: 11.572098, 09:52:47\n",
      "Step: 814, MSE: 0.069617, PSNR: 11.572844, 09:52:47\n",
      "Step: 815, MSE: 0.069523, PSNR: 11.578717, 09:52:48\n",
      "Step: 816, MSE: 0.069555, PSNR: 11.576696, 09:52:48\n",
      "Step: 817, MSE: 0.069627, PSNR: 11.572252, 09:52:48\n",
      "Step: 818, MSE: 0.069691, PSNR: 11.568251, 09:52:49\n",
      "Step: 819, MSE: 0.069685, PSNR: 11.568583, 09:52:49\n",
      "Step: 820, MSE: 0.069640, PSNR: 11.571432, 09:52:49\n",
      "Step: 821, MSE: 0.069806, PSNR: 11.561087, 09:52:50\n",
      "Step: 822, MSE: 0.069630, PSNR: 11.572022, 09:52:50\n",
      "Step: 823, MSE: 0.069582, PSNR: 11.575018, 09:52:50\n",
      "Step: 824, MSE: 0.069573, PSNR: 11.575619, 09:52:51\n",
      "Step: 825, MSE: 0.069724, PSNR: 11.566187, 09:52:51\n",
      "Step: 826, MSE: 0.069740, PSNR: 11.565171, 09:52:51\n",
      "Step: 827, MSE: 0.069613, PSNR: 11.573125, 09:52:52\n",
      "Step: 828, MSE: 0.069624, PSNR: 11.572381, 09:52:52\n",
      "Step: 829, MSE: 0.069494, PSNR: 11.580551, 09:52:52\n",
      "Step: 830, MSE: 0.069513, PSNR: 11.579350, 09:52:53\n",
      "Step: 831, MSE: 0.069407, PSNR: 11.585989, 09:52:53\n",
      "Step: 832, MSE: 0.069482, PSNR: 11.581264, 09:52:54\n",
      "Step: 833, MSE: 0.069412, PSNR: 11.585675, 09:52:54\n",
      "Step: 834, MSE: 0.069542, PSNR: 11.577554, 09:52:54\n",
      "Step: 835, MSE: 0.069533, PSNR: 11.578119, 09:52:55\n",
      "Step: 836, MSE: 0.069527, PSNR: 11.578442, 09:52:55\n",
      "Step: 837, MSE: 0.069656, PSNR: 11.570445, 09:52:55\n",
      "Step: 838, MSE: 0.069640, PSNR: 11.571416, 09:52:56\n",
      "Step: 839, MSE: 0.069500, PSNR: 11.580122, 09:52:56\n",
      "Step: 840, MSE: 0.069686, PSNR: 11.568542, 09:52:56\n",
      "Step: 841, MSE: 0.069753, PSNR: 11.564389, 09:52:57\n",
      "Step: 842, MSE: 0.069622, PSNR: 11.572515, 09:52:57\n",
      "Step: 843, MSE: 0.069635, PSNR: 11.571724, 09:52:57\n",
      "Step: 844, MSE: 0.069729, PSNR: 11.565875, 09:52:58\n",
      "Step: 845, MSE: 0.069695, PSNR: 11.567985, 09:52:58\n",
      "Step: 846, MSE: 0.069661, PSNR: 11.570130, 09:52:58\n",
      "Step: 847, MSE: 0.069616, PSNR: 11.572881, 09:52:59\n",
      "Step: 848, MSE: 0.069652, PSNR: 11.570639, 09:52:59\n",
      "Step: 849, MSE: 0.069328, PSNR: 11.590902, 09:53:00\n",
      "Step: 850, MSE: 0.069708, PSNR: 11.567145, 09:53:00\n",
      "Step: 851, MSE: 0.069620, PSNR: 11.572634, 09:53:00\n",
      "Step: 852, MSE: 0.069602, PSNR: 11.573788, 09:53:01\n",
      "Step: 853, MSE: 0.069712, PSNR: 11.566955, 09:53:01\n",
      "Step: 854, MSE: 0.069682, PSNR: 11.568798, 09:53:01\n",
      "Step: 855, MSE: 0.069688, PSNR: 11.568409, 09:53:02\n",
      "Step: 856, MSE: 0.069846, PSNR: 11.558579, 09:53:02\n",
      "Step: 857, MSE: 0.069637, PSNR: 11.571630, 09:53:02\n",
      "Step: 858, MSE: 0.069549, PSNR: 11.577102, 09:53:03\n",
      "Step: 859, MSE: 0.069784, PSNR: 11.562472, 09:53:03\n",
      "Step: 860, MSE: 0.069648, PSNR: 11.570917, 09:53:03\n",
      "Step: 861, MSE: 0.069598, PSNR: 11.574040, 09:53:04\n",
      "Step: 862, MSE: 0.069580, PSNR: 11.575178, 09:53:04\n",
      "Step: 863, MSE: 0.069697, PSNR: 11.567858, 09:53:04\n",
      "Step: 864, MSE: 0.069600, PSNR: 11.573914, 09:53:05\n",
      "Step: 865, MSE: 0.069780, PSNR: 11.562693, 09:53:05\n",
      "Step: 866, MSE: 0.069562, PSNR: 11.576253, 09:53:05\n",
      "Step: 867, MSE: 0.069675, PSNR: 11.569216, 09:53:06\n",
      "Step: 868, MSE: 0.069707, PSNR: 11.567245, 09:53:06\n",
      "Step: 869, MSE: 0.069411, PSNR: 11.585714, 09:53:06\n",
      "Step: 870, MSE: 0.069636, PSNR: 11.571641, 09:53:07\n",
      "Step: 871, MSE: 0.069447, PSNR: 11.583492, 09:53:07\n",
      "Step: 872, MSE: 0.069526, PSNR: 11.578559, 09:53:07\n",
      "Step: 873, MSE: 0.069575, PSNR: 11.575470, 09:53:08\n",
      "Step: 874, MSE: 0.069632, PSNR: 11.571914, 09:53:08\n",
      "Step: 875, MSE: 0.069615, PSNR: 11.572977, 09:53:09\n",
      "Step: 876, MSE: 0.069518, PSNR: 11.579000, 09:53:09\n",
      "Step: 877, MSE: 0.069413, PSNR: 11.585598, 09:53:09\n",
      "Step: 878, MSE: 0.069440, PSNR: 11.583919, 09:53:10\n",
      "Step: 879, MSE: 0.069705, PSNR: 11.567371, 09:53:10\n",
      "Step: 880, MSE: 0.069632, PSNR: 11.571920, 09:53:10\n",
      "Step: 881, MSE: 0.069532, PSNR: 11.578130, 09:53:11\n",
      "Step: 882, MSE: 0.069598, PSNR: 11.574059, 09:53:11\n",
      "Step: 883, MSE: 0.069770, PSNR: 11.563288, 09:53:11\n",
      "Step: 884, MSE: 0.069569, PSNR: 11.575833, 09:53:12\n",
      "Step: 885, MSE: 0.069603, PSNR: 11.573750, 09:53:12\n",
      "Step: 886, MSE: 0.069518, PSNR: 11.579031, 09:53:12\n",
      "Step: 887, MSE: 0.069579, PSNR: 11.575239, 09:53:13\n",
      "Step: 888, MSE: 0.069567, PSNR: 11.575937, 09:53:13\n",
      "Step: 889, MSE: 0.069528, PSNR: 11.578413, 09:53:14\n",
      "Step: 890, MSE: 0.069499, PSNR: 11.580210, 09:53:14\n",
      "Step: 891, MSE: 0.069662, PSNR: 11.570064, 09:53:14\n",
      "Step: 892, MSE: 0.069583, PSNR: 11.574994, 09:53:15\n",
      "Step: 893, MSE: 0.069681, PSNR: 11.568852, 09:53:15\n",
      "Step: 894, MSE: 0.069587, PSNR: 11.574698, 09:53:15\n",
      "Step: 895, MSE: 0.069668, PSNR: 11.569695, 09:53:16\n",
      "Step: 896, MSE: 0.069537, PSNR: 11.577815, 09:53:16\n",
      "Step: 897, MSE: 0.069515, PSNR: 11.579217, 09:53:16\n",
      "Step: 898, MSE: 0.069724, PSNR: 11.566185, 09:53:17\n",
      "Step: 899, MSE: 0.069559, PSNR: 11.576443, 09:53:17\n",
      "Step: 900, MSE: 0.069676, PSNR: 11.569157, 09:53:17\n",
      "Step: 901, MSE: 0.069491, PSNR: 11.580688, 09:53:18\n",
      "Step: 902, MSE: 0.069487, PSNR: 11.580945, 09:53:18\n",
      "Step: 903, MSE: 0.069691, PSNR: 11.568226, 09:53:18\n",
      "Step: 904, MSE: 0.069659, PSNR: 11.570211, 09:53:19\n",
      "Step: 905, MSE: 0.069601, PSNR: 11.573851, 09:53:19\n",
      "Step: 906, MSE: 0.069583, PSNR: 11.574938, 09:53:19\n",
      "Step: 907, MSE: 0.069486, PSNR: 11.581022, 09:53:20\n",
      "Step: 908, MSE: 0.069598, PSNR: 11.574024, 09:53:20\n",
      "Step: 909, MSE: 0.069631, PSNR: 11.571978, 09:53:20\n",
      "Step: 910, MSE: 0.069494, PSNR: 11.580498, 09:53:21\n",
      "Step: 911, MSE: 0.069555, PSNR: 11.576721, 09:53:21\n",
      "Step: 912, MSE: 0.069723, PSNR: 11.566265, 09:53:21\n",
      "Step: 913, MSE: 0.069609, PSNR: 11.573377, 09:53:22\n",
      "Step: 914, MSE: 0.069571, PSNR: 11.575738, 09:53:22\n",
      "Step: 915, MSE: 0.069503, PSNR: 11.579958, 09:53:23\n",
      "Step: 916, MSE: 0.069402, PSNR: 11.586305, 09:53:23\n",
      "Step: 917, MSE: 0.069708, PSNR: 11.567165, 09:53:23\n",
      "Step: 918, MSE: 0.069446, PSNR: 11.583501, 09:53:24\n",
      "Step: 919, MSE: 0.069511, PSNR: 11.579460, 09:53:24\n",
      "Step: 920, MSE: 0.069556, PSNR: 11.576644, 09:53:24\n",
      "Step: 921, MSE: 0.069460, PSNR: 11.582668, 09:53:25\n",
      "Step: 922, MSE: 0.069643, PSNR: 11.571215, 09:53:25\n",
      "Step: 923, MSE: 0.069540, PSNR: 11.577629, 09:53:25\n",
      "Step: 924, MSE: 0.069608, PSNR: 11.573391, 09:53:26\n",
      "Step: 925, MSE: 0.069665, PSNR: 11.569834, 09:53:26\n",
      "Step: 926, MSE: 0.069578, PSNR: 11.575260, 09:53:26\n",
      "Step: 927, MSE: 0.069505, PSNR: 11.579824, 09:53:27\n",
      "Step: 928, MSE: 0.069574, PSNR: 11.575562, 09:53:27\n",
      "Step: 929, MSE: 0.069726, PSNR: 11.566075, 09:53:27\n",
      "Step: 930, MSE: 0.069530, PSNR: 11.578266, 09:53:28\n",
      "Step: 931, MSE: 0.069419, PSNR: 11.585218, 09:53:28\n",
      "Step: 932, MSE: 0.069503, PSNR: 11.579963, 09:53:29\n",
      "Step: 933, MSE: 0.069672, PSNR: 11.569433, 09:53:29\n",
      "Step: 934, MSE: 0.069540, PSNR: 11.577664, 09:53:29\n",
      "Step: 935, MSE: 0.069443, PSNR: 11.583700, 09:53:30\n",
      "Step: 936, MSE: 0.069591, PSNR: 11.574458, 09:53:30\n",
      "Step: 937, MSE: 0.069733, PSNR: 11.565629, 09:53:30\n",
      "Step: 938, MSE: 0.069726, PSNR: 11.566050, 09:53:31\n",
      "Step: 939, MSE: 0.069603, PSNR: 11.573704, 09:53:31\n",
      "Step: 940, MSE: 0.069667, PSNR: 11.569747, 09:53:31\n",
      "Step: 941, MSE: 0.069577, PSNR: 11.575340, 09:53:32\n",
      "Step: 942, MSE: 0.069360, PSNR: 11.588926, 09:53:32\n",
      "Step: 943, MSE: 0.069570, PSNR: 11.575768, 09:53:32\n",
      "Step: 944, MSE: 0.069363, PSNR: 11.588750, 09:53:33\n",
      "Step: 945, MSE: 0.069495, PSNR: 11.580450, 09:53:33\n",
      "Step: 946, MSE: 0.069502, PSNR: 11.580008, 09:53:33\n",
      "Step: 947, MSE: 0.069632, PSNR: 11.571942, 09:53:34\n",
      "Step: 948, MSE: 0.069510, PSNR: 11.579537, 09:53:34\n",
      "Step: 949, MSE: 0.069629, PSNR: 11.572107, 09:53:34\n",
      "Step: 950, MSE: 0.069501, PSNR: 11.580095, 09:53:35\n",
      "Step: 951, MSE: 0.069517, PSNR: 11.579084, 09:53:35\n",
      "Step: 952, MSE: 0.069602, PSNR: 11.573773, 09:53:35\n",
      "Step: 953, MSE: 0.069726, PSNR: 11.566082, 09:53:36\n",
      "Step: 954, MSE: 0.069580, PSNR: 11.575160, 09:53:36\n",
      "Step: 955, MSE: 0.069740, PSNR: 11.565188, 09:53:36\n",
      "Step: 956, MSE: 0.069498, PSNR: 11.580263, 09:53:37\n",
      "Step: 957, MSE: 0.069503, PSNR: 11.579976, 09:53:37\n",
      "Step: 958, MSE: 0.069737, PSNR: 11.565398, 09:53:38\n",
      "Step: 959, MSE: 0.069425, PSNR: 11.584826, 09:53:38\n",
      "Step: 960, MSE: 0.069462, PSNR: 11.582527, 09:53:38\n",
      "Step: 961, MSE: 0.069469, PSNR: 11.582103, 09:53:39\n",
      "Step: 962, MSE: 0.069586, PSNR: 11.574789, 09:53:39\n",
      "Step: 963, MSE: 0.069666, PSNR: 11.569822, 09:53:39\n",
      "Step: 964, MSE: 0.069703, PSNR: 11.567507, 09:53:40\n",
      "Step: 965, MSE: 0.069454, PSNR: 11.583040, 09:53:40\n",
      "Step: 966, MSE: 0.069598, PSNR: 11.574020, 09:53:40\n",
      "Step: 967, MSE: 0.069537, PSNR: 11.577826, 09:53:41\n",
      "Step: 968, MSE: 0.069735, PSNR: 11.565517, 09:53:41\n",
      "Step: 969, MSE: 0.069502, PSNR: 11.580025, 09:53:41\n",
      "Step: 970, MSE: 0.069774, PSNR: 11.563070, 09:53:42\n",
      "Step: 971, MSE: 0.069593, PSNR: 11.574325, 09:53:42\n",
      "Step: 972, MSE: 0.069545, PSNR: 11.577354, 09:53:42\n",
      "Step: 973, MSE: 0.069727, PSNR: 11.565973, 09:53:43\n",
      "Step: 974, MSE: 0.069566, PSNR: 11.576008, 09:53:43\n",
      "Step: 975, MSE: 0.069510, PSNR: 11.579525, 09:53:43\n",
      "Step: 976, MSE: 0.069528, PSNR: 11.578417, 09:53:44\n",
      "Step: 977, MSE: 0.069498, PSNR: 11.580285, 09:53:44\n",
      "Step: 978, MSE: 0.069573, PSNR: 11.575607, 09:53:44\n",
      "Step: 979, MSE: 0.069713, PSNR: 11.566887, 09:53:45\n",
      "Step: 980, MSE: 0.069430, PSNR: 11.584539, 09:53:45\n",
      "Step: 981, MSE: 0.069448, PSNR: 11.583433, 09:53:45\n",
      "Step: 982, MSE: 0.069594, PSNR: 11.574254, 09:53:46\n",
      "Step: 983, MSE: 0.069710, PSNR: 11.567068, 09:53:46\n",
      "Step: 984, MSE: 0.069714, PSNR: 11.566784, 09:53:47\n",
      "Step: 985, MSE: 0.069578, PSNR: 11.575310, 09:53:47\n",
      "Step: 986, MSE: 0.069582, PSNR: 11.575051, 09:53:47\n",
      "Step: 987, MSE: 0.069579, PSNR: 11.575247, 09:53:48\n",
      "Step: 988, MSE: 0.069405, PSNR: 11.586074, 09:53:48\n",
      "Step: 989, MSE: 0.069518, PSNR: 11.579021, 09:53:48\n",
      "Step: 990, MSE: 0.069585, PSNR: 11.574855, 09:53:49\n",
      "Step: 991, MSE: 0.069553, PSNR: 11.576835, 09:53:49\n",
      "Step: 992, MSE: 0.069520, PSNR: 11.578903, 09:53:49\n",
      "Step: 993, MSE: 0.069603, PSNR: 11.573701, 09:53:50\n",
      "Step: 994, MSE: 0.069695, PSNR: 11.568008, 09:53:50\n",
      "Step: 995, MSE: 0.069569, PSNR: 11.575853, 09:53:50\n",
      "Step: 996, MSE: 0.069529, PSNR: 11.578332, 09:53:51\n",
      "Step: 997, MSE: 0.069575, PSNR: 11.575468, 09:53:51\n",
      "Step: 998, MSE: 0.069559, PSNR: 11.576480, 09:53:51\n",
      "Step: 999, MSE: 0.069617, PSNR: 11.572857, 09:53:52\n",
      "Step: 1000, MSE: 0.069641, PSNR: 11.571340, 09:53:52\n",
      "Step: 1001, MSE: 0.069513, PSNR: 11.579361, 09:53:52\n",
      "Step: 1002, MSE: 0.069548, PSNR: 11.577159, 09:53:53\n",
      "Step: 1003, MSE: 0.069642, PSNR: 11.571268, 09:53:53\n",
      "Step: 1004, MSE: 0.069629, PSNR: 11.572119, 09:53:53\n",
      "Step: 1005, MSE: 0.069591, PSNR: 11.574498, 09:53:54\n",
      "Step: 1006, MSE: 0.069611, PSNR: 11.573206, 09:53:54\n",
      "Step: 1007, MSE: 0.069451, PSNR: 11.583202, 09:53:54\n",
      "Step: 1008, MSE: 0.069566, PSNR: 11.576048, 09:53:55\n",
      "Step: 1009, MSE: 0.069563, PSNR: 11.576241, 09:53:55\n",
      "Step: 1010, MSE: 0.069494, PSNR: 11.580553, 09:53:55\n",
      "Step: 1011, MSE: 0.069702, PSNR: 11.567528, 09:53:56\n",
      "Step: 1012, MSE: 0.069652, PSNR: 11.570692, 09:53:56\n",
      "Step: 1013, MSE: 0.069537, PSNR: 11.577859, 09:53:57\n",
      "Step: 1014, MSE: 0.069472, PSNR: 11.581881, 09:53:57\n",
      "Step: 1015, MSE: 0.069450, PSNR: 11.583298, 09:53:57\n",
      "Step: 1016, MSE: 0.069776, PSNR: 11.562943, 09:53:58\n",
      "Step: 1017, MSE: 0.069641, PSNR: 11.571377, 09:53:58\n",
      "Step: 1018, MSE: 0.069610, PSNR: 11.573314, 09:53:58\n",
      "Step: 1019, MSE: 0.069514, PSNR: 11.579299, 09:53:59\n",
      "Step: 1020, MSE: 0.069666, PSNR: 11.569776, 09:53:59\n",
      "Step: 1021, MSE: 0.069729, PSNR: 11.565866, 09:53:59\n",
      "Step: 1022, MSE: 0.069585, PSNR: 11.574859, 09:54:00\n",
      "Step: 1023, MSE: 0.069653, PSNR: 11.570601, 09:54:00\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 359  |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 427\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# PPO 학습\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    418\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m )\n\u001b[0;32m--> 427\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    430\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:213\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 213\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:738\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    736\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[1;32m    737\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 738\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    740\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/distributions.py:175\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.log_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Get the log probabilities of actions according to the distribution.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Note that you must first call the ``proba_distribution()`` method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sum_independent_dims(log_prob)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/normal.py:84\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[1;32m     82\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     83\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 84\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[1;32m     90\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 현재 날짜와 시간을 가져와 포맷 지정\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # *으로 list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 1024, 1024).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(1024)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((1024, 1024))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 1024 or target.shape[-2] < 1024:\n",
    "            target = torchvision.transforms.Resize(1024)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "#BinaryHologramEnv 클래스\n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=100000, T_PSNR=30, T_steps=1000):\n",
    "        \"\"\"\n",
    "        target_function: 타겟 이미지와의 손실(MSE 또는 PSNR) 계산 함수.\n",
    "        trainloader: 학습 데이터셋 로더.\n",
    "        max_steps: 최대 타임스텝 제한.\n",
    "        T_PSNR: 목표 PSNR 값.\n",
    "        T_steps: PSNR 목표를 유지해야 하는 최소 타임스텝.\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        # 관찰 공간 (1, 8, 1024, 1024)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간: (1, 8, 1024, 1024) 형태의 이진 데이터\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.int8)\n",
    "\n",
    "        # 모델 및 데이터 로더 설정\n",
    "        self.target_function = target_function  # BinaryNet 모델\n",
    "        self.trainloader = trainloader          # 학습 데이터 로더\n",
    "\n",
    "        # 에피소드 설정\n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        # 학습 상태\n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 학습 데이터셋에서 첫 배치 추출\n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            self.target_image = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "        초기 상태를 생성하고, 시뮬레이션 및 관련 값을 계산합니다.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  # 이진화 상태\n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        # 관찰값 업데이트\n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        # 첫 스텝에서 초기 상태와 동일한 행동 적용\n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1  # 스텝 증가\n",
    "            # reset과 동일한 로직을 호출해 초기 상태 생성\n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        # 현재 상태에 행동을 적용하여 새로운 상태 생성\n",
    "        new_state = np.logical_xor(self.state, action).astype(np.int8)\n",
    "\n",
    "        # 이진화된 새로운 상태를 torch 텐서로 변환\n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta 정보 포함\n",
    "\n",
    "        # 시뮬레이션 수행\n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE 및 PSNR 계산\n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "        reward = -mse\n",
    "\n",
    "        # 출력 추가\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        # 상태 업데이트\n",
    "        self.state = new_state\n",
    "        self.observation = self.state  # 관찰값은 항상 상태와 동일\n",
    "\n",
    "        # 종료 조건\n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        # 행동 마스크 생성\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "        관찰값에 따라 행동 마스크 생성.\n",
    "        관찰값이 0~0.2인 경우 -> 행동 0으로 고정.\n",
    "        관찰값이 0.8~1인 경우 -> 행동 1로 고정.\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  # 기본적으로 모든 행동 가능\n",
    "        mask[observation <= 0.2] = 0  # 관찰값이 0~0.2면 행동 0으로 고정\n",
    "        mask[observation >= 0.8] = 1  # 관찰값이 0.8~1이면 행동 1로 고정\n",
    "        return mask\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  # 메타 정보\n",
    "padding = 0\n",
    "\n",
    "# Dataset512 클래스 사용\n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet 모델 로드\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 마스크 함수 정의\n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "# 환경 생성에 새로운 데이터 로더 적용\n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  # 업데이트된 train_loader 사용\n",
    "    max_steps=100000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=1000\n",
    ")\n",
    "\n",
    "# ActionMasker 래퍼 적용\n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized 환경 생성\n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO 학습\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "# 학습된 모델 저장\n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "# 평가용 환경 생성\n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback 추가\n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  # 평가 빈도 (타임스텝 기준)\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "# 학습 시작 (콜백 추가)\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
