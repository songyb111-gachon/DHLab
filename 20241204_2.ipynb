{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8166d0a3-99d7-463e-ab64-48c27db6dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./logs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Recent Reward: [1.0658484]\n",
      "Step: 2000, Recent Reward: [2.4157393]\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1456 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Step: 3000, Recent Reward: [4.355744]\n",
      "Step: 4000, Recent Reward: [3.3777492]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1082        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010824179 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.92       |\n",
      "|    explained_variance   | -0.000832   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 440         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 1.18e+03    |\n",
      "-----------------------------------------\n",
      "Step: 5000, Recent Reward: [1.5564599]\n",
      "Step: 6000, Recent Reward: [5.013902]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 979         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011759669 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.89       |\n",
      "|    explained_variance   | 0.000154    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 621         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 1.53e+03    |\n",
      "-----------------------------------------\n",
      "Step: 7000, Recent Reward: [3.3462315]\n",
      "Step: 8000, Recent Reward: [-0.38032955]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 943         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009680084 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.85       |\n",
      "|    explained_variance   | 6.79e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 693         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 1.58e+03    |\n",
      "-----------------------------------------\n",
      "Step: 9000, Recent Reward: [5.368772]\n",
      "Step: 10000, Recent Reward: [4.3921776]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 912         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011462132 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.8        |\n",
      "|    explained_variance   | -0.000578   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 731         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 1.67e+03    |\n",
      "-----------------------------------------\n",
      "Step: 11000, Recent Reward: [4.4717574]\n",
      "Step: 12000, Recent Reward: [0.9684403]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 898         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008697342 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.75       |\n",
      "|    explained_variance   | -0.000169   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 729         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 1.65e+03    |\n",
      "-----------------------------------------\n",
      "Step: 13000, Recent Reward: [1.3322359]\n",
      "Step: 14000, Recent Reward: [1.6823121]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 888         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008851099 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.68       |\n",
      "|    explained_variance   | -2.28e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 756         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 1.67e+03    |\n",
      "-----------------------------------------\n",
      "Step: 15000, Recent Reward: [1.0857873]\n",
      "Step: 16000, Recent Reward: [5.3800063]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925471 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.59       |\n",
      "|    explained_variance   | 9.65e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 676         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 1.62e+03    |\n",
      "-----------------------------------------\n",
      "Step: 17000, Recent Reward: [1.8501941]\n",
      "Step: 18000, Recent Reward: [2.60202]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010421155 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | -2.96e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 668         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 1.62e+03    |\n",
      "-----------------------------------------\n",
      "Step: 19000, Recent Reward: [3.019529]\n",
      "Step: 20000, Recent Reward: [3.6062627]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011596038 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | -0.000473   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 741         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 1.68e+03    |\n",
      "-----------------------------------------\n",
      "Step: 21000, Recent Reward: [4.089368]\n",
      "Step: 22000, Recent Reward: [2.421308]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007500417 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 2.12e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 817         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    value_loss           | 1.84e+03    |\n",
      "-----------------------------------------\n",
      "Step: 23000, Recent Reward: [1.9793848]\n",
      "Step: 24000, Recent Reward: [3.964672]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010459257 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | 7.03e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 768         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 1.77e+03    |\n",
      "-----------------------------------------\n",
      "Step: 25000, Recent Reward: [1.2402357]\n",
      "Step: 26000, Recent Reward: [2.789159]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009989921 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | -8.58e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 757         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 1.8e+03     |\n",
      "-----------------------------------------\n",
      "Step: 27000, Recent Reward: [4.3008504]\n",
      "Step: 28000, Recent Reward: [4.3506465]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492383 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.85       |\n",
      "|    explained_variance   | -4.65e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 776         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 1.84e+03    |\n",
      "-----------------------------------------\n",
      "Step: 29000, Recent Reward: [6.381261]\n",
      "Step: 30000, Recent Reward: [4.7912893]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 853          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074655944 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.7         |\n",
      "|    explained_variance   | -7.87e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 838          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00909     |\n",
      "|    value_loss           | 1.85e+03     |\n",
      "------------------------------------------\n",
      "Step: 31000, Recent Reward: [2.6170526]\n",
      "Step: 32000, Recent Reward: [4.7299876]\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 852        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00944411 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.55      |\n",
      "|    explained_variance   | 2.92e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 785        |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 1.89e+03   |\n",
      "----------------------------------------\n",
      "Step: 33000, Recent Reward: [6.9211993]\n",
      "Step: 34000, Recent Reward: [6.8092875]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009962581 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 885         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 2.05e+03    |\n",
      "-----------------------------------------\n",
      "Step: 35000, Recent Reward: [2.2776587]\n",
      "Step: 36000, Recent Reward: [4.8482533]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009718736 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.21       |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 815         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "Step: 37000, Recent Reward: [3.8430915]\n",
      "Step: 38000, Recent Reward: [0.7960824]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007724568 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 2.03e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 776         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 1.95e+03    |\n",
      "-----------------------------------------\n",
      "Step: 39000, Recent Reward: [4.619435]\n",
      "Step: 40000, Recent Reward: [2.7982047]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 845         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010892469 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | -7.15e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 765         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 1.94e+03    |\n",
      "-----------------------------------------\n",
      "Step: 41000, Recent Reward: [4.359248]\n",
      "Step: 42000, Recent Reward: [3.9848719]\n",
      "Step: 43000, Recent Reward: [5.345459]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 845         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011125572 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | -5.96e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 940         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    value_loss           | 2.09e+03    |\n",
      "-----------------------------------------\n",
      "Step: 44000, Recent Reward: [8.671379]\n",
      "Step: 45000, Recent Reward: [7.5445385]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110645145 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.35        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 858          |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 1.98e+03     |\n",
      "------------------------------------------\n",
      "Step: 46000, Recent Reward: [2.4436097]\n",
      "Step: 47000, Recent Reward: [1.4010711]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012887397 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 828         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.89e+03    |\n",
      "-----------------------------------------\n",
      "Step: 48000, Recent Reward: [3.7778206]\n",
      "Step: 49000, Recent Reward: [5.7686253]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011673148 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 811         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    value_loss           | 1.94e+03    |\n",
      "-----------------------------------------\n",
      "Step: 50000, Recent Reward: [4.1975956]\n",
      "Step: 51000, Recent Reward: [5.3313766]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012220062 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.63       |\n",
      "|    explained_variance   | 2.38e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 826         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00638    |\n",
      "|    value_loss           | 1.96e+03    |\n",
      "-----------------------------------------\n",
      "Step: 52000, Recent Reward: [5.499859]\n",
      "Step: 53000, Recent Reward: [3.6677115]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015272101 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.32       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 828         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 1.95e+03    |\n",
      "-----------------------------------------\n",
      "Step: 54000, Recent Reward: [3.2870705]\n",
      "Step: 55000, Recent Reward: [7.9958334]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 838          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075033773 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.13        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 833          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 1.9e+03      |\n",
      "------------------------------------------\n",
      "Step: 56000, Recent Reward: [2.4543805]\n",
      "Step: 57000, Recent Reward: [3.1724927]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016087994 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 961         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 2.07e+03    |\n",
      "-----------------------------------------\n",
      "Step: 58000, Recent Reward: [3.63759]\n",
      "Step: 59000, Recent Reward: [4.3819027]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015291082 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 893         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 1.97e+03    |\n",
      "-----------------------------------------\n",
      "Step: 60000, Recent Reward: [5.0356803]\n",
      "Step: 61000, Recent Reward: [6.212727]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012829395 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 890         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    value_loss           | 2.01e+03    |\n",
      "-----------------------------------------\n",
      "Step: 62000, Recent Reward: [3.5830767]\n",
      "Step: 63000, Recent Reward: [6.5526004]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008430196 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 860         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 1.95e+03    |\n",
      "-----------------------------------------\n",
      "Step: 64000, Recent Reward: [7.192385]\n",
      "Step: 65000, Recent Reward: [5.350961]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010853639 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 801         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "Step: 66000, Recent Reward: [5.357244]\n",
      "Step: 67000, Recent Reward: [10.53058]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006542716 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 812         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 1.86e+03    |\n",
      "-----------------------------------------\n",
      "Step: 68000, Recent Reward: [5.9787884]\n",
      "Step: 69000, Recent Reward: [6.1358433]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008352603 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 812         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00492    |\n",
      "|    value_loss           | 1.87e+03    |\n",
      "-----------------------------------------\n",
      "Step: 70000, Recent Reward: [1.2175695]\n",
      "Step: 71000, Recent Reward: [5.5775785]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008347407 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 709         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    value_loss           | 1.67e+03    |\n",
      "-----------------------------------------\n",
      "Step: 72000, Recent Reward: [4.9220552]\n",
      "Step: 73000, Recent Reward: [5.011149]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053958935 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 724          |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 1.71e+03     |\n",
      "------------------------------------------\n",
      "Step: 74000, Recent Reward: [3.2909114]\n",
      "Step: 75000, Recent Reward: [6.274946]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046161376 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 722          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 1.63e+03     |\n",
      "------------------------------------------\n",
      "Step: 76000, Recent Reward: [4.756837]\n",
      "Step: 77000, Recent Reward: [4.9778943]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036668829 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 623          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 1.6e+03      |\n",
      "------------------------------------------\n",
      "Step: 78000, Recent Reward: [1.3027612]\n",
      "Step: 79000, Recent Reward: [4.74076]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004839528 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 679         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    value_loss           | 1.58e+03    |\n",
      "-----------------------------------------\n",
      "Step: 80000, Recent Reward: [3.144378]\n",
      "Step: 81000, Recent Reward: [3.054087]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026859012 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 646          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    value_loss           | 1.5e+03      |\n",
      "------------------------------------------\n",
      "Step: 82000, Recent Reward: [4.0821886]\n",
      "Step: 83000, Recent Reward: [7.5597134]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035011838 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 591          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    value_loss           | 1.39e+03     |\n",
      "------------------------------------------\n",
      "Step: 84000, Recent Reward: [9.734397]\n",
      "Step: 85000, Recent Reward: [3.9196436]\n",
      "Step: 86000, Recent Reward: [4.89009]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031094332 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.951       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 500          |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 1.31e+03     |\n",
      "------------------------------------------\n",
      "Step: 87000, Recent Reward: [6.5259213]\n",
      "Step: 88000, Recent Reward: [10.25348]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023931046 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.898       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 523          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00314     |\n",
      "|    value_loss           | 1.28e+03     |\n",
      "------------------------------------------\n",
      "Step: 89000, Recent Reward: [8.432547]\n",
      "Step: 90000, Recent Reward: [2.7784379]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003455297 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 459         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "Step: 91000, Recent Reward: [4.980402]\n",
      "Step: 92000, Recent Reward: [2.6524048]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024685834 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.762       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 465          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 1.11e+03     |\n",
      "------------------------------------------\n",
      "Step: 93000, Recent Reward: [4.190229]\n",
      "Step: 94000, Recent Reward: [5.2855034]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023703882 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.721       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 385          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 1.02e+03     |\n",
      "------------------------------------------\n",
      "Step: 95000, Recent Reward: [8.47868]\n",
      "Step: 96000, Recent Reward: [3.3010972]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002281149 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 430         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    value_loss           | 1.07e+03    |\n",
      "-----------------------------------------\n",
      "Step: 97000, Recent Reward: [5.7359147]\n",
      "Step: 98000, Recent Reward: [4.3101883]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019090073 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 339          |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 909          |\n",
      "------------------------------------------\n",
      "Step: 99000, Recent Reward: [7.9529295]\n",
      "Step: 100000, Recent Reward: [6.647564]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019506523 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 361          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 886          |\n",
      "------------------------------------------\n",
      "Step: 101000, Recent Reward: [7.4604383]\n",
      "Step: 102000, Recent Reward: [3.988973]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019891826 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 344          |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 880          |\n",
      "------------------------------------------\n",
      "Step: 103000, Recent Reward: [4.7813334]\n",
      "Step: 104000, Recent Reward: [5.769521]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018480141 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 309          |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 735          |\n",
      "------------------------------------------\n",
      "Step: 105000, Recent Reward: [9.055593]\n",
      "Step: 106000, Recent Reward: [3.2821805]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010668417 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 279          |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 709          |\n",
      "------------------------------------------\n",
      "Step: 107000, Recent Reward: [3.5794973]\n",
      "Step: 108000, Recent Reward: [2.1199229]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 129          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013438239 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 263          |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 713          |\n",
      "------------------------------------------\n",
      "Step: 109000, Recent Reward: [4.4231863]\n",
      "Step: 110000, Recent Reward: [3.2607908]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018317531 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.518       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 241          |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 581          |\n",
      "------------------------------------------\n",
      "Step: 111000, Recent Reward: [6.22933]\n",
      "Step: 112000, Recent Reward: [4.7747507]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013949209 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 241          |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 606          |\n",
      "------------------------------------------\n",
      "Step: 113000, Recent Reward: [6.7490244]\n",
      "Step: 114000, Recent Reward: [5.5961356]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010951118 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 183          |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 547          |\n",
      "------------------------------------------\n",
      "Step: 115000, Recent Reward: [2.1190023]\n",
      "Step: 116000, Recent Reward: [1.8023006]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019127545 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 160          |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 486          |\n",
      "------------------------------------------\n",
      "Step: 117000, Recent Reward: [5.928056]\n",
      "Step: 118000, Recent Reward: [2.8596044]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 142          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018265226 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 191          |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    value_loss           | 460          |\n",
      "------------------------------------------\n",
      "Step: 119000, Recent Reward: [4.3419905]\n",
      "Step: 120000, Recent Reward: [4.3812275]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 120832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013307368 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 162          |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 437          |\n",
      "------------------------------------------\n",
      "Step: 121000, Recent Reward: [1.4386775]\n",
      "Step: 122000, Recent Reward: [7.330083]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014721819 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 158          |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 387          |\n",
      "------------------------------------------\n",
      "Step: 123000, Recent Reward: [2.7571423]\n",
      "Step: 124000, Recent Reward: [5.827704]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 834           |\n",
      "|    iterations           | 61            |\n",
      "|    time_elapsed         | 149           |\n",
      "|    total_timesteps      | 124928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090252253 |\n",
      "|    clip_fraction        | 0.00972       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.41         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 110           |\n",
      "|    n_updates            | 600           |\n",
      "|    policy_gradient_loss | -0.000478     |\n",
      "|    value_loss           | 303           |\n",
      "-------------------------------------------\n",
      "Step: 125000, Recent Reward: [7.032212]\n",
      "Step: 126000, Recent Reward: [4.856996]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 152          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011910972 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 321          |\n",
      "------------------------------------------\n",
      "Step: 127000, Recent Reward: [6.1029243]\n",
      "Step: 128000, Recent Reward: [7.738609]\n",
      "Step: 129000, Recent Reward: [6.40983]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 154          |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011790141 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.358       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70.7         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 252          |\n",
      "------------------------------------------\n",
      "Step: 130000, Recent Reward: [2.60456]\n",
      "Step: 131000, Recent Reward: [6.784507]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007650063 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 99.2         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00079     |\n",
      "|    value_loss           | 261          |\n",
      "------------------------------------------\n",
      "Step: 132000, Recent Reward: [1.687714]\n",
      "Step: 133000, Recent Reward: [8.538101]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 833           |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 133120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00091924856 |\n",
      "|    clip_fraction        | 0.00845       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.355        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 46.4          |\n",
      "|    n_updates            | 640           |\n",
      "|    policy_gradient_loss | -0.00055      |\n",
      "|    value_loss           | 169           |\n",
      "-------------------------------------------\n",
      "Step: 134000, Recent Reward: [6.908995]\n",
      "Step: 135000, Recent Reward: [2.879478]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011956678 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.7         |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 224          |\n",
      "------------------------------------------\n",
      "Step: 136000, Recent Reward: [3.8004196]\n",
      "Step: 137000, Recent Reward: [7.851672]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007230437 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.9         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "Step: 138000, Recent Reward: [6.4927077]\n",
      "Step: 139000, Recent Reward: [7.168716]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 833           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 167           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087259524 |\n",
      "|    clip_fraction        | 0.0109        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.301        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 54.2          |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.000895     |\n",
      "|    value_loss           | 161           |\n",
      "-------------------------------------------\n",
      "Step: 140000, Recent Reward: [2.8261025]\n",
      "Step: 141000, Recent Reward: [4.48344]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010449828 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.6         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "Step: 142000, Recent Reward: [9.467954]\n",
      "Step: 143000, Recent Reward: [5.4401565]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 833           |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 171           |\n",
      "|    total_timesteps      | 143360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056278164 |\n",
      "|    clip_fraction        | 0.0131        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.3          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 29.8          |\n",
      "|    n_updates            | 690           |\n",
      "|    policy_gradient_loss | -0.00127      |\n",
      "|    value_loss           | 100           |\n",
      "-------------------------------------------\n",
      "Step: 144000, Recent Reward: [8.519257]\n",
      "Step: 145000, Recent Reward: [4.2388544]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 174          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009201189 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.3         |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 75.3         |\n",
      "------------------------------------------\n",
      "Step: 146000, Recent Reward: [1.985111]\n",
      "Step: 147000, Recent Reward: [5.969822]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 176          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009727733 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.1         |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 83.5         |\n",
      "------------------------------------------\n",
      "Step: 148000, Recent Reward: [5.3738537]\n",
      "Step: 149000, Recent Reward: [7.0694313]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010284928 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 69.6         |\n",
      "------------------------------------------\n",
      "Step: 150000, Recent Reward: [3.0399208]\n",
      "Step: 151000, Recent Reward: [2.2760785]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023001602 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.5         |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "Step: 152000, Recent Reward: [5.4186025]\n",
      "Step: 153000, Recent Reward: [5.7417636]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 184          |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012567767 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.1         |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000577    |\n",
      "|    value_loss           | 52.1         |\n",
      "------------------------------------------\n",
      "Step: 154000, Recent Reward: [10.957445]\n",
      "Step: 155000, Recent Reward: [2.4654658]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 186          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008659299 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 43.8         |\n",
      "------------------------------------------\n",
      "Step: 156000, Recent Reward: [5.400263]\n",
      "Step: 157000, Recent Reward: [8.780528]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005573509 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -3.47e-05    |\n",
      "|    value_loss           | 38.8         |\n",
      "------------------------------------------\n",
      "Step: 158000, Recent Reward: [6.7772274]\n",
      "Step: 159000, Recent Reward: [3.337061]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009748743 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000608    |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "Step: 160000, Recent Reward: [6.461137]\n",
      "Step: 161000, Recent Reward: [3.1906226]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010443504 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000653    |\n",
      "|    value_loss           | 36.5         |\n",
      "------------------------------------------\n",
      "Step: 162000, Recent Reward: [0.9504302]\n",
      "Step: 163000, Recent Reward: [2.7872074]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009929741 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.000904    |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "Step: 164000, Recent Reward: [2.9539459]\n",
      "Step: 165000, Recent Reward: [7.5718465]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 199          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007258273 |\n",
      "|    clip_fraction        | 0.00811      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.000848    |\n",
      "|    value_loss           | 38.2         |\n",
      "------------------------------------------\n",
      "Step: 166000, Recent Reward: [6.7728963]\n",
      "Step: 167000, Recent Reward: [5.116908]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 201          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007463245 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 35.7         |\n",
      "------------------------------------------\n",
      "Step: 168000, Recent Reward: [10.12421]\n",
      "Step: 169000, Recent Reward: [2.8359373]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004890442 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 37.4         |\n",
      "------------------------------------------\n",
      "Step: 170000, Recent Reward: [4.261037]\n",
      "Step: 171000, Recent Reward: [7.006761]\n",
      "Step: 172000, Recent Reward: [3.5855339]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 206          |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007559682 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n",
      "Step: 173000, Recent Reward: [7.693454]\n",
      "Step: 174000, Recent Reward: [3.5541003]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 832           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 209           |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085222616 |\n",
      "|    clip_fraction        | 0.00991       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.158        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 18            |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    value_loss           | 36.9          |\n",
      "-------------------------------------------\n",
      "Step: 175000, Recent Reward: [6.3601356]\n",
      "Step: 176000, Recent Reward: [5.2601995]\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000949955 |\n",
      "|    clip_fraction        | 0.0101      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "Step: 177000, Recent Reward: [7.943367]\n",
      "Step: 178000, Recent Reward: [4.3472886]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005639399 |\n",
      "|    clip_fraction        | 0.00962      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000897    |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "Step: 179000, Recent Reward: [4.300034]\n",
      "Step: 180000, Recent Reward: [4.9755654]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 832          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003038515 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000344    |\n",
      "|    value_loss           | 34.4         |\n",
      "------------------------------------------\n",
      "Step: 181000, Recent Reward: [7.708696]\n",
      "Step: 182000, Recent Reward: [8.283019]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 832           |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 218           |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043195864 |\n",
      "|    clip_fraction        | 0.00786       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.133        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 16.5          |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | -0.000781     |\n",
      "|    value_loss           | 36.2          |\n",
      "-------------------------------------------\n",
      "Step: 183000, Recent Reward: [7.5749993]\n",
      "Step: 184000, Recent Reward: [6.3451676]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 221           |\n",
      "|    total_timesteps      | 184320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050855154 |\n",
      "|    clip_fraction        | 0.0085        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.126        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 19.1          |\n",
      "|    n_updates            | 890           |\n",
      "|    policy_gradient_loss | -0.00139      |\n",
      "|    value_loss           | 42.4          |\n",
      "-------------------------------------------\n",
      "Step: 185000, Recent Reward: [1.8468895]\n",
      "Step: 186000, Recent Reward: [6.9588337]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 224           |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053708674 |\n",
      "|    clip_fraction        | 0.00898       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.121        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 22.5          |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | -0.00094      |\n",
      "|    value_loss           | 36.7          |\n",
      "-------------------------------------------\n",
      "Step: 187000, Recent Reward: [1.9621015]\n",
      "Step: 188000, Recent Reward: [5.655392]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 226          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004387885 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.9         |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000865    |\n",
      "|    value_loss           | 37.8         |\n",
      "------------------------------------------\n",
      "Step: 189000, Recent Reward: [9.037485]\n",
      "Step: 190000, Recent Reward: [5.6739254]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 830           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 229           |\n",
      "|    total_timesteps      | 190464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049122644 |\n",
      "|    clip_fraction        | 0.0063        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.113        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 20.5          |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    value_loss           | 34.8          |\n",
      "-------------------------------------------\n",
      "Step: 191000, Recent Reward: [6.028418]\n",
      "Step: 192000, Recent Reward: [5.1346555]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 94            |\n",
      "|    time_elapsed         | 231           |\n",
      "|    total_timesteps      | 192512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047218803 |\n",
      "|    clip_fraction        | 0.00693       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.124        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 18.1          |\n",
      "|    n_updates            | 930           |\n",
      "|    policy_gradient_loss | -0.000856     |\n",
      "|    value_loss           | 31.2          |\n",
      "-------------------------------------------\n",
      "Step: 193000, Recent Reward: [3.8510518]\n",
      "Step: 194000, Recent Reward: [5.1489563]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 234           |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037703157 |\n",
      "|    clip_fraction        | 0.0102        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.123        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 12.4          |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -0.000761     |\n",
      "|    value_loss           | 31.8          |\n",
      "-------------------------------------------\n",
      "Step: 195000, Recent Reward: [4.784728]\n",
      "Step: 196000, Recent Reward: [5.240709]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 236           |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073629105 |\n",
      "|    clip_fraction        | 0.00825       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.131        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 17.8          |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -0.000701     |\n",
      "|    value_loss           | 35.9          |\n",
      "-------------------------------------------\n",
      "Step: 197000, Recent Reward: [4.181222]\n",
      "Step: 198000, Recent Reward: [5.768184]\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 831           |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 239           |\n",
      "|    total_timesteps      | 198656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038141216 |\n",
      "|    clip_fraction        | 0.00547       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.137        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 15.8          |\n",
      "|    n_updates            | 960           |\n",
      "|    policy_gradient_loss | -0.000406     |\n",
      "|    value_loss           | 40.7          |\n",
      "-------------------------------------------\n",
      "Step: 199000, Recent Reward: [7.9565754]\n",
      "Step: 200000, Recent Reward: [3.0563822]\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 241          |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004646292 |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.1         |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    value_loss           | 40.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f0f78f2cb80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiBinary\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "\n",
    "# 스마트 팩토리 환경 생성\n",
    "class SmartFactoryEnv(gym.Env):\n",
    "    def __init__(self, n_machines=10, energy_limit=18):\n",
    "        super(SmartFactoryEnv, self).__init__()\n",
    "        self.n_machines = n_machines\n",
    "        self.energy_limit = energy_limit\n",
    "\n",
    "        # Multi-Binary 행동 공간\n",
    "        self.action_space = MultiBinary(n_machines)\n",
    "\n",
    "        # 관찰 공간\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(n_machines + 1,), dtype=float\n",
    "        )\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.zeros(self.n_machines + 1, dtype=float)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # 에너지 사용량과 생산량 계산\n",
    "        energy_usage = np.dot(np.random.uniform(1, 2, self.n_machines), action)\n",
    "        production = np.dot(np.random.uniform(1, 3, self.n_machines), action)\n",
    "        penalty = max(0, energy_usage - self.energy_limit)\n",
    "        \n",
    "        # 보상 계산\n",
    "        reward = production - energy_usage - penalty\n",
    "\n",
    "        # 상태 업데이트\n",
    "        self.state = np.concatenate([action, [energy_usage]])\n",
    "\n",
    "        # 종료 조건\n",
    "        done = energy_usage > self.energy_limit * 2\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Machines: {self.state[:-1]}, Energy Usage: {self.state[-1]}\")\n",
    "\n",
    "# 보상을 출력하는 콜백 클래스\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # 주기적으로 보상 출력 (1000 스텝마다)\n",
    "        if self.n_calls % 1000 == 0:\n",
    "            print(f\"Step: {self.n_calls}, Recent Reward: {self.locals['rewards']}\")\n",
    "        return True\n",
    "\n",
    "# 로그 저장 디렉토리 설정\n",
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 학습 환경 생성 및 Monitor로 래핑\n",
    "train_env = Monitor(SmartFactoryEnv(n_machines=10, energy_limit=18), filename=log_dir)\n",
    "env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "# PPO 모델 생성\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 로그 설정\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 학습 - RewardLoggerCallback 사용\n",
    "model.learn(total_timesteps=200000, callback=RewardLoggerCallback())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3f62f3-61cc-4b0a-8237-7202c1ba0533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIjCAYAAAD1OgEdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACcqElEQVR4nOzdd3RU1d7G8e/MpPcQUiGEEHrvRaUoSEeaBSyAot6rWLBduwJXxXJV7L42sKGAIiKCFAVBadJ7LwmQQktvk8x5/wgZiQmQhCST8nzWypKcs+fM7+wEzJOzi8kwDAMREREREREpU2ZHFyAiIiIiIlIdKWyJiIiIiIiUA4UtERERERGRcqCwJSIiIiIiUg4UtkRERERERMqBwpaIiIiIiEg5UNgSEREREREpBwpbIiIiIiIi5UBhS0REREREpBwobImIVDHjxo2jfv36pXrtpEmTMJlMZVuQVBm9evWiV69eji5DRKTGUNgSESkjJpOpWB8rVqxwdKkOMW7cOLy8vBxdRrEYhsGXX35Jjx498PPzw8PDg1atWjFlyhTS0tIcXZ7dkSNHiv19d+TIEUeXe0kvvfQS8+bNc3QZIiJlxmQYhuHoIkREqoOvvvqqwOdffPEFS5cu5csvvyxw/NprryU4OLjU72O1WrHZbLi6upb4tTk5OeTk5ODm5lbq9y+tcePG8d1335Gamlrh710Subm53HzzzcyePZvu3bszYsQIPDw8WLVqFTNnzqR58+YsW7bssr6GZSUtLY0ffvihwLHXX3+dY8eO8eabbxY4Pnz4cJydnQFwcXGpsBpLwsvLi+uvv54ZM2Y4uhQRkTLh5OgCRESqi1tvvbXA52vXrmXp0qWFjv9Teno6Hh4exX6f/B+YS8PJyQknJ/3TfzGvvvoqs2fP5tFHH+W1116zH7/77ru58cYbGTZsGOPGjWPRokUVWldR3yeenp6Fvr++/fZbzp49e8nvOxERKX8aRigiUoF69epFy5Yt2bhxIz169MDDw4OnnnoKgB9//JFBgwYRFhaGq6srUVFR/Pe//yU3N7fANf45Zyt/KNn//vc/PvroI6KionB1daVTp0789ddfBV5b1Jwtk8nEfffdx7x582jZsiWurq60aNGCX375pVD9K1asoGPHjri5uREVFcX//d//lfk8sDlz5tChQwfc3d2pXbs2t956K8ePHy/QJi4ujttvv526devi6upKaGgoQ4cOLTBUbsOGDfTr14/atWvj7u5OZGQkd9xxx0XfOyMjg9dee43GjRszderUQueHDBnC2LFj+eWXX1i7di0AgwcPpkGDBkVer1u3bnTs2LHAsa+++sp+f7Vq1WLUqFHExMQUaHOx75PL8c85WytWrMBkMjF79mwmT55MnTp18Pb25vrrrycpKYmsrCwmTpxIUFAQXl5e3H777WRlZRW6bnHuaf/+/YwcOZKQkBDc3NyoW7cuo0aNIikpCcj7PkxLS+Pzzz+3D30cN26c/fXHjx/njjvuIDg42P49+tlnnxV4j/z7mTVrFk899RQhISF4enpy3XXXlbgeEZGyoF9viohUsNOnTzNgwABGjRrFrbfeah+ONmPGDLy8vHj44Yfx8vLit99+47nnniM5ObnAE5YLmTlzJikpKfzrX//CZDLx6quvMmLECA4dOnTJp2F//PEHc+fO5d5778Xb25u3336bkSNHEh0dTUBAAACbN2+mf//+hIaGMnnyZHJzc5kyZQqBgYGX3ynnzJgxg9tvv51OnToxdepU4uPjeeutt/jzzz/ZvHkzfn5+AIwcOZKdO3dy//33U79+fRISEli6dCnR0dH2z/v27UtgYCBPPPEEfn5+HDlyhLlz516yH86ePcuDDz54wSeAY8aMYfr06SxYsICuXbty0003MWbMGP766y86depkb3f06FHWrl1b4Gv34osv8uyzz3LjjTdy5513cvLkSd555x169OhR4P7gwt8n5WHq1Km4u7vzxBNPcODAAd555x2cnZ0xm82cPXuWSZMmsXbtWmbMmEFkZCTPPfdcie4pOzubfv36kZWVxf33309ISAjHjx9nwYIFJCYm4uvry5dffsmdd95J586dufvuuwGIiooCID4+nq5du9p/MRAYGMiiRYsYP348ycnJTJw4scD9vPjii5hMJh5//HESEhKYNm0affr0YcuWLbi7uxerHhGRMmGIiEi5mDBhgvHPf2Z79uxpAMaHH35YqH16enqhY//6178MDw8PIzMz035s7NixRkREhP3zw4cPG4AREBBgnDlzxn78xx9/NADjp59+sh97/vnnC9UEGC4uLsaBAwfsx7Zu3WoAxjvvvGM/NmTIEMPDw8M4fvy4/dj+/fsNJyenQtcsytixYw1PT88Lns/OzjaCgoKMli1bGhkZGfbjCxYsMADjueeeMwzDMM6ePWsAxmuvvXbBa/3www8GYPz111+XrOt806ZNMwDjhx9+uGCbM2fOGIAxYsQIwzAMIykpyXB1dTUeeeSRAu1effVVw2QyGUePHjUMwzCOHDliWCwW48UXXyzQbvv27YaTk1OB4xf7PrmUQYMGFfj+OF/Pnj2Nnj172j9fvny5ARgtW7Y0srOz7cdHjx5tmEwmY8CAAQVe361btwLXLu49bd682QCMOXPmXLR2T09PY+zYsYWOjx8/3ggNDTVOnTpV4PioUaMMX19f+9+d/PupU6eOkZycbG83e/ZsAzDeeuutEtUjInK5NIxQRKSCubq6cvvttxc67u7ubv9zSkoKp06donv37qSnp7Nnz55LXvemm27C39/f/nn37t0BOHTo0CVf26dPH/tTBIDWrVvj4+Njf21ubi7Lli1j2LBhhIWF2ds1bNiQAQMGXPL6xbFhwwYSEhK49957CyzgMWjQIJo2bcrPP/8M5PWTi4sLK1as4OzZs0VeK/8J0YIFC7BarcWuISUlBQBvb+8Ltsk/l5ycDICPjw8DBgxg9uzZGOetOTVr1iy6du1KvXr1AJg7dy42m40bb7yRU6dO2T9CQkJo1KgRy5cvL/A+F/o+KQ9jxowp8PSzS5cuGIZRaNhlly5diImJIScnByj+PeU/KVq8eDHp6eklqs0wDL7//nuGDBmCYRgF3qdfv34kJSWxadOmQvdz/tfw+uuvJzQ0lIULF152PSIiJaGwJSJSwerUqVPkanA7d+5k+PDh+Pr64uPjQ2BgoH2Rg+LMI8n/oT5ffvC6UCC52GvzX5//2oSEBDIyMmjYsGGhdkUdK42jR48C0KRJk0LnmjZtaj/v6urKK6+8wqJFiwgODqZHjx68+uqrxMXF2dv37NmTkSNHMnnyZGrXrs3QoUOZPn16kfONzpf/A3p+6CpKUYHspptuIiYmhjVr1gBw8OBBNm7cyE033WRvs3//fgzDoFGjRgQGBhb42L17NwkJCQXe50LfJ+Xhn1///DASHh5e6LjNZrN/Pxb3niIjI3n44Yf55JNPqF27Nv369eO9994r1vf1yZMnSUxM5KOPPir0Hvlh9J9916hRowKfm0wmGjZsaJ/Tdzn1iIiUhOZsiYhUsPOfYOVLTEykZ8+e+Pj4MGXKFKKionBzc2PTpk08/vjj2Gy2S17XYrEUedwoxg4fl/NaR5g4cSJDhgxh3rx5LF68mGeffZapU6fy22+/0a5dO0wmE9999x1r167lp59+YvHixdxxxx28/vrrrF279oL7fTVr1gyAbdu2MWzYsCLbbNu2DYDmzZvbjw0ZMgQPDw9mz57NFVdcwezZszGbzdxwww32NjabDZPJxKJFi4rs73/WVNT3SXm50Nf/Ut8XJbmn119/nXHjxvHjjz+yZMkSHnjgAaZOncratWupW7fuBWvL/96/9dZbGTt2bJFtWrdufcHXX0hp6xERKQmFLRGRSmDFihWcPn2auXPn0qNHD/vxw4cPO7CqvwUFBeHm5saBAwcKnSvqWGlEREQAsHfvXq655poC5/bu3Ws/ny8qKopHHnmERx55hP3799O2bVtef/31Avudde3ala5du/Liiy8yc+ZMbrnlFr799lvuvPPOImu46qqr8PPzY+bMmTz99NNFBogvvvgCyFuFMJ+npyeDBw9mzpw5vPHGG8yaNYvu3bsXGHIZFRWFYRhERkbSuHHjEvZO5VTSe2rVqhWtWrXimWeeYfXq1Vx55ZV8+OGHvPDCCwBFrmoZGBiIt7c3ubm59OnTp1h17d+/v8DnhmFw4MCBQqHsUvWIiFwuDSMUEakE8n+oP/9JUnZ2Nu+//76jSirAYrHQp08f5s2bx4kTJ+zHDxw4UGb7TXXs2JGgoCA+/PDDAsP9Fi1axO7duxk0aBCQt99UZmZmgddGRUXh7e1tf93Zs2cLPZVr27YtwEWHEnp4ePDoo4+yd+9enn766ULnf/75Z2bMmEG/fv3o2rVrgXM33XQTJ06c4JNPPmHr1q0FhhACjBgxAovFwuTJkwvVZhgGp0+fvmBdlVVx7yk5Odk+zytfq1atMJvNBb4enp6eJCYmFmhnsVgYOXIk33//PTt27ChUw8mTJwsd++KLLwoMBf3uu++IjY21zy8sbj0iIpdLT7ZERCqBK664An9/f8aOHcsDDzyAyWTiyy+/rFTD+CZNmsSSJUu48sorueeee8jNzeXdd9+lZcuWbNmypVjXsFqtRT41qFWrFvfeey+vvPIKt99+Oz179mT06NH2pd/r16/PQw89BMC+ffvo3bs3N954I82bN8fJyYkffviB+Ph4Ro0aBcDnn3/O+++/z/Dhw4mKiiIlJYWPP/4YHx8fBg4ceNEan3jiCTZv3swrr7zCmjVrGDlyJO7u7vzxxx989dVXNGvWjM8//7zQ6wYOHIi3tzePPvqoPSCcLyoqihdeeIEnn3ySI0eOMGzYMLy9vTl8+DA//PADd999N48++mix+rGyKO49/fbbb9x3333ccMMNNG7cmJycHL788stC/dShQweWLVvGG2+8QVhYGJGRkXTp0oWXX36Z5cuX06VLF+666y6aN2/OmTNn2LRpE8uWLePMmTMF6qpVqxZXXXUVt99+O/Hx8UybNo2GDRty1113ARS7HhGRy6WwJSJSCQQEBLBgwQIeeeQRnnnmGfz9/bn11lvp3bs3/fr1c3R5QN4PwosWLeLRRx/l2WefJTw8nClTprB79+5irZYIeU/rnn322ULHo6KiuPfeexk3bhweHh68/PLLPP7443h6ejJ8+HBeeeUV+wqD4eHhjB49ml9//ZUvv/wSJycnmjZtyuzZs+0/KPfs2ZP169fz7bffEh8fj6+vL507d+brr78mMjLyojVaLBZmz57NF198wSeffMKzzz5LdnY2UVFRPP/88zzyyCN4enoWep2bmxvXXXcdX3/9NX369CEoKKhQmyeeeILGjRvz5ptvMnnyZPv99O3bl+uuu65YfVjZFOee2rRpQ79+/fjpp584fvw4Hh4etGnThkWLFhV4QvjGG29w991388wzz5CRkcHYsWPp0qULwcHBrF+/nilTpjB37lzef/99AgICaNGiBa+88kqhmp566im2bdvG1KlTSUlJoXfv3rz//vt4eHiUqB4RkctlMirTr01FRKTKGTZsGDt37iw0T0akoq1YsYKrr76aOXPmcP311zu6HBERzdkSEZHiy8jIKPD5/v37WbhwIb169XJMQSIiIpWYhhGKiEixNWjQgHHjxtGgQQOOHj3KBx98gIuLC//5z38cXZqIiEilo7AlIiLF1r9/f7755hvi4uJwdXWlW7duvPTSS4U2kRURERHN2RIRERERESkXmrMlIiIiIiJSDhS2REREREREyoHmbBWDzWbjxIkTeHt7YzKZHF2OiIiIiIg4iGEYpKSkEBYWhtl88WdXClvFcOLECcLDwx1dhoiIiIiIVBIxMTHUrVv3om0UtorB29sbyOtQHx+fMrmm1WplyZIl9O3bF2dn5zK5phSmfi5/6uOKoX4uf+rjiqF+Ln/q44qhfq4YlbGfk5OTCQ8Pt2eEi1HYKob8oYM+Pj5lGrY8PDzw8fGpNN841ZH6ufypjyuG+rn8qY8rhvq5/KmPK4b6uWJU5n4uzvQiLZAhIiIiIiJSDhS2REREREREyoHCloiIiIiISDnQnC0RERERKRbDMMjJySE3N9fRpTic1WrFycmJzMxM9Uc5clQ/Ozs7Y7FYLvs6ClsiIiIicknZ2dnExsaSnp7u6FIqBcMwCAkJISYmRvuwliNH9bPJZKJu3bp4eXld1nUUtkRERETkomw2G4cPH8ZisRAWFoaLi0uNDxg2m43U1FS8vLwuubGtlJ4j+tkwDE6ePMmxY8do1KjRZT3hUtgSERERkYvKzs7GZrMRHh6Oh4eHo8upFGw2G9nZ2bi5uSlslSNH9XNgYCBHjhzBarVeVtjSd4aIiIiIFItChdQUZfXkVn9jREREREREyoHCloiIiIiISDlQ2BIRERERuYBevXoxceJER5chVZTCloiIiIhUO0OGDKF///5Fnlu1ahUmk4lt27ZVcFVFO3LkCCaTiS1btji6FCljClsiIiIiUu2MHz+epUuXcuzYsULnpk+fTseOHWndurUDKpOaRGFLRERERErEMAzSs3Mc8mEYRrFqHDx4MIGBgcyYMaPA8dTUVObMmcP48eM5ffo0o0ePpk6dOnh4eNCqVSu++eabi17XZDIxb968Asf8/PwKvE9MTAw33ngjfn5+1KpVi6FDh3LkyJFi1V2UrKwsHnjgAYKCgnBzc+Oqq67ir7/+sp8/e/Yst9xyC4GBgbi7u9OoUSOmT58O5C3bf9999xEaGoqbmxsRERFMnTq11LVIyWifLREREREpkQxrLs2fW+yQ9941pR8eLpf+EdbJyYkxY8YwY8YMnn76aftS3nPmzCE3N5fRo0eTmppKhw4dePzxx/Hx8eHnn3/mtttuIyoqis6dO5eqPqvVSr9+/ejWrRurVq3CycmJF154gf79+7Nt2zZcXFxKfM3//Oc/fP/993z++edERETw6quv0q9fPw4cOECtWrV49tln2bVrF4sWLaJ27docOHCAjIwMAN5++23mz5/P7NmzqVevHjExMcTExJTq3qTkFLZEREREpFq64447eO211/j999/p1asXkDeEcOTIkfj6+uLr68ujjz5qb3///fezePFiZs+eXeqwNWvWLGw2G5988ok94E2fPh0/Pz9WrFhB3759S3S9tLQ0PvjgA2bMmMGAAQMA+Pjjj1m6dCmffvopjz32GNHR0bRr146OHTsCUL9+ffvro6OjadSoEVdddRUmk4mIiIhS3ZeUjsKWSDUWl5RJVk4uEQGeji5FRESqEXdnC7um9HPYexdX06ZNueKKK/jss8/o1asXBw4cYNWqVUyZMgWA3NxcXnrpJWbPns3x48fJzs4mKysLDw+PUte3detWDhw4gLe3d4HjmZmZHDx4sMTXO3jwIFarlSuvvNJ+zNnZmc6dO7N7924A7rnnHkaOHMmmTZvo27cvw4YN44orrgBg3LhxXHvttTRp0oT+/fszePDgEgc+KT2FLZFqKtdmMPKD1SSmZ7P80V4E+bg5uiQREakmTCZTsYbyVQbjx4/n/vvv57333mP69OlERUXRs2dPAF577TXeeustpk2bRqtWrfD09GTixIlkZ2df8Homk6nQvDGr1Wr/c/7QxK+//rrQawMDA8vorgoaMGAAR48eZeHChSxdupTevXszYcIE/ve//9G+fXsOHz7MokWLWLZsGTfeeCN9+vThu+++K5dapCAtkCFSTe2OTeZ4YgZp2bks2RXv6HJEREQc4sYbb8RsNjNz5ky++OIL7rjjDvvwvj///JOhQ4dy66230qZNGxo0aMC+ffsuer3AwEBiY2Ptn+/fv5/09HT75+3bt2f//v0EBQXRsGHDAh++vr4lrj8qKgoXFxf+/PNP+zGr1cpff/1F8+bNC9Q1duxYvvrqK6ZNm8ZHH31kP+fj48NNN93Exx9/zKxZs/j+++85c+ZMiWuRkqsav5IQkRJbf/jvf0SX7Irn1q4aoy0iIjWPl5cXN910E08++STJycmMGzfOfq5Ro0Z89913rF69Gn9/f9544w3i4+MLhJh/uuaaa3j33Xfp0qULycnJvPDCCzg7O9vP33LLLbz22msMHTqUKVOmULduXY4ePcrcuXP5z3/+Q926dS947b179xY61qJFC+655x4ee+wxatWqRb169Xj11VdJT09n/PjxADz33HN06NCBFi1akJWVxYIFC2jWrBkAb7zxBqGhobRr1w6z2cycOXMICQnBz8+vhD0ppaGwJVJNrTt82v7nNQdPkZxpxcfN+SKvEBERqZ7Gjx/Pp59+ysCBAwkLC7Mff+aZZzh06BD9+vXDw8ODu+++m2HDhpGUlHTBa73++uvcfvvt9OzZk5CQEN566y02btxoP+/h4cHKlSt5/PHHGTFiBCkpKdSpU4fevXvj4+Nz0TpHjRpV6FhMTAwvv/wyNpuN2267jZSUFDp27MjixYvx9/cHwMXFhSeffJIjR47g7u5O9+7d+fbbbwHw9vbm1VdfZf/+/VgsFjp16sTChQsxmzXArSIobIlUQ4Zh2J9suTmbybTaWL4ngaFt6zi4MhERkYrXrVu3IvfnqlWrVqE9s/5pxYoVBT4PCwtj8eLF2Gw2kpOT8fHxITExsUCbkJAQPv/882LXV79+/UvuH/b222/z9ttvF3numWee4Zlnniny3F133cVdd91V7FqkbCnSilRDBxJSOZtuxc3ZzG3nhg9W9XlbG4+eZcxn6zmQkOroUkRERESKRWFLpBpad+6pVvt6/gxqnTdcYsWeBLJych1Z1mX5YMVBVu47yWuL9zi6FBEREZFiUdgSqYbyhxB2jqxF6zq+BPu4kpady+qDpy/xysrJMAw2R58FYNnuBBKSMx1ckYiIiMilKWyJVIC0rBwenbOVFXsTyv29zp+v1TmyFmaziWubBwOwZGfVHEoYfSad02l5e57k2gxmb4hxcEUiIiIil6awJVIB5m05zncbj/HKL4WXdC1r0WfSiUvOxNliol143ipFfZuHALB0Vzw228Un4FZGm8491XK25O2L8s36GHKr4H2IiIhIzaKwJVIBNh7NCwsHElLIzrGV63vlz9dqU9cPdxcLAF0bBODt5sSp1Cw2x5wt1/cvD5uOJgJwY8dwfN2dOZ6Ywcr9Jx1blIiIiMglKGyJVIBN58KWNdfg4MnyXU3v/CGE+VyczFzTNAiomkMJ859sdYsKYET7vOXrv1kX7ciSRERERC5JYUuknJ1KzeLI6XT757tjk8v1/YoKW/D3UMLFO+MuuZdHZZKencOeuBQgb3XFW7rUA+DXPQnEa6EMERERqcQUtkTK2eboxAKfl2fYik3KIPpMOmYTdIjwL3CuZ5NAXCxmjpxOr1J7VW2NSSLXZhDi40aYnzsNg7zpXL8WuTaDWX9poQwRERGpvBS2RMpZ/nwtL1cnAHbHppTbe+U/1WoR5ou3m3OBc16uTlzZMACoWhsc5w8hbB/hZz9287mnW9+uj9ZCGSIiUm5mzJiBn5+f/fNJkybRtm1bh9VTXlasWIHJZCIxMREofN9SegpbIuUsf77WsHZ5mwvvjk0ut2F8+YtjdPnHEMJ8fVv8PZSwqsjfX6t9vb+f1PVvGYKfhzMnkjL5fV/5L6cvIiJV07hx4zCZTJhMJlxcXGjYsCFTpkwhJyenVNd79NFH+fXXX8u4yoIMw+Djjz+mW7du+Pj44OXlRYsWLXjwwQc5cOBAub53vptuuol9+/aV6TX/Gegut11V4dCwNXXqVDp16oS3tzdBQUEMGzaMvXsLLo2dmZnJhAkTCAgIwMvLi5EjRxIfX/C38tHR0QwaNAgPDw+CgoJ47LHHCv0lWrFiBe3bt8fV1ZWGDRsyY8aM8r49Eay5NrYeSwRgVKd6mE1wOi2bkylZ5fJ+F5qvla9Ps2BMJth2LIkTiRnlUkNZMgyDTeeGYbY/b1ikm7OFke3rAjBz3cWHEhqGweerjzB10W6sueW7EqSIiFQ+/fv3JzY2lv379/PII48wadIkXnvttVJdy8vLi4CAgDKu8G+GYXDzzTfzwAMPMHDgQJYsWcKuXbv49NNPcXNz44UXXrjga7Ozs8usDnd3d4KCgsrsejWZQ8PW77//zoQJE1i7di1Lly7FarXSt29f0tLS7G0eeughfvrpJ+bMmcPvv//OiRMnGDFihP18bm4ugwYNIjs7m9WrV/P5558zY8YMnnvuOXubw4cPM2jQIK6++mq2bNnCxIkTufPOO1m8eHGF3q/UPLtOJJOVY8PX3ZnmoT40CPTKO14O87ZOpWbZ52J1ql902Ar0dqXDuSdEy3ZX/qGER0+ncyYtGxeLmRZhPgXOje6cN5Twtz3xxCYVHRwNw2DKgl08P38n//f7IT7743C51ywiUiMYBmSnOeajhKNDXF1dCQkJISIignvuuYc+ffowf/58AM6ePcuYMWPw9/fHw8ODAQMGsH///gteq6hhhJ999hktWrTA1dWV0NBQ7rvvPgDuuOMOBg8eXKCt1WolKCiITz/9tMjrz5o1i2+//ZZZs2bx7LPP0rVrV+rVq0fXrl155ZVXmD59ur3tuHHjGDZsGC+++CJhYWE0adIEgC+//JKOHTvi7e1NSEgIN998MwkJBUeBLFy4kMaNG+Pu7s7VV1/NkSNHCpwvahjhjz/+SPv27XFzc6NBgwZMnjy5wMMNk8nEJ598wvDhw/Hw8KBRo0b2fj5y5AhXX301AP7+/phMJsaNG3fBfr6YS33Njh49ypAhQ/D398fT05MWLVqwcOFC+2tvueUWAgMDcXd3p1GjRgX6tDw4levVL+GXX34p8PmMGTMICgpi48aN9OjRg6SkJD799FNmzpzJNddcA8D06dNp1qwZa9eupWvXrvbEv2zZMoKDg2nbti3//e9/efzxx5k0aRIuLi58+OGHREZG8vrrrwPQrFkz/vjjD95880369etXqK6srCyysv5+8pCcnPeDsdVqxWq1lsm951+nrK4nRXN0P/91+BQAbcN9yc3NoUmwFwcSUtlxLJErG/hf4tUls/ZA3r5TjYO88HIxXfCeezcNZMPRs/yyI5bRHetc9vuWZx/n91+LMG/Mhg2r9e8nUxH+rnSu78/6I2f5Zu1R7r8mqsBrbTaDKT/v4ev1fz/5euvX/QxoEUSor1uZ11reHP29XBOojyuG+rn8lUcfW61WDMPAZrNhs9kgOw3zy3XL7PolYXviGLh4FqutYRj2uvO5ublx+vRpbDYbY8eO5cCBA8ybNw8fHx+eeOIJBg4cyI4dO3B2dra/Lv+/+dMAbDYbhmHw6aef8swzzzB16lT69+9PUlISq1evxmazcccdd9CrVy+OHz9OaGgoAPPnzyc9PZ0bbrihQE35Zs6cSZMmTRg8eHCR58+vwTAMfv31V7y9ve0PEGw2G1lZWUyePJkmTZqQkJDAo48+ytixY/n5558BiImJYcSIEdx7773cddddbNiwgccee8z+evvX+Lz7XrVqFWPGjGHatGl0796dgwcP8u9//xvDMAo84Jg8eTIvv/wyr7zyCu+++y633HILhw8fpk6dOsyZM4cbbriB3bt34+Pjg7u7e5H3eP575/dz/v0W52t27733kp2dzYoVK/D09GTXrl14eHhgs9l45pln2LVrFz///DO1a9fmwIEDZGRkXLAOwzCwWq1YLJYC50ryd8uhYeufkpKSAKhVK++38hs3bsRqtdKnTx97m6ZNm1KvXj3WrFlD165dWbNmDa1atSI4ONjepl+/ftxzzz3s3LmTdu3asWbNmgLXyG8zceLEIuuYOnUqkydPLnR8yZIleHh4XO5tFrB06dIyvZ4UzVH9/PM+M2DGKyOehQsXYkoyARZ+27SX8NTdZfpecw/nvVeQKdn+G5yiOGUAOLH20Gm+m78QjzL6V6A8+njeobx78s05W+Q9NXU2sR4LX/x5gPoZe7GY8o7bDJh9yMyaBDMmDG5qYGNtgpkjqbk8MH0FtzeuusMJ9W9G+VMfVwz1c/kryz52cnIiJCSE1NTUvOFq1nT8yuzqJZOckgLOucVqa7VaycnJITk5b77077//zpIlS7jrrrvYvHkzP/30E7/88gtt2rQB4IMPPqBly5Z88803DBs2jMzMTAzDsP/iPSsri9zcXPvnr7/+OhMmTLA/pQkJCaFJkyYkJyfTsmVLGjVqxCeffMKDDz4IwCeffMLQoUOx2Wz2a5xv7969REVFFTj35JNP8uWXXwLg6+vLzp077ffm4eHB66+/jouLS17fJCdz/fXX219bu3ZtXnzxRa655hpOnDiBl5cXb731FpGRkfaQNGTIEDZu3Mhbb71FSkoKZrO50H0///zzPPjggwwfPtx+3SeeeIJJkyYV+Hl61KhRDBo0CIDHH3+cd955hxUrVtCnTx/c3PJ+0enu7m7/ebqoPkhPz9suJ7+WfCkpKRw8ePCSX7MjR45w3XXXERERAUCPHj3s73Xo0CFatGhB48aNAejcufMF68jOziYjI4OVK1cWmp6UX2NxVJqwZbPZmDhxIldeeSUtW7YEIC4uDhcXl0KPMYODg4mLi7O3OT9o5Z/PP3exNsnJyWRkZODu7l7g3JNPPsnDDz9s/zw5OZnw8HD69u2Lj0/BoUylZbVaWbp0Kddeey3Ozs6XfoGUiqP7+eVdK4FMburTmW4NAvDcd5IFX24m2ezNwIFXlul7ffjeGiCFG3q2ZWCrkIu2nX3iT/YnpOFcry0D24Zd1vuWZx/n39OIHm0Z0LLwPfXOsfHTa79zNt2KZ8NOXNMkkFybwdM/7mRNwgnMJnh5eCuGtwtjV2wywz9Yy5bTZrwbd6R7w9plWmt5c/T3ck2gPq4Y6ufyVx59nJmZSUxMDF5eXnk/NBveeU+YHMDH2QNMpmK1dXZ2ZvHixdStWxer1YrNZmP06NG89NJL/Prrrzg5OXHNNdfYn1z4+PjQpEkTjh49io+PD25ubphMJvvPf66urlgsFnx8fIiPjyc2Npb+/ftf8OfDu+66i48//phnn32W+Ph4li1bxrJlyy7Y3mw24+TkVOD8pEmTeOihh/jhhx+YOnWq/ZyzszOtWrWidu2C/z/buHEjkydPZtu2bZw9e9b+1CYxMZGwsDAOHTpE165dC7xHz549eeutt/D29i7yvnfu3Mm6det444037K/Jzc0lMzMTJycne3jq2LGj/TU+Pj74+PiQmpqKj4+PvU3+e1zIP9sZhkFKSgre3t7ExMRc8mv24IMPMmHCBFauXEnv3r0ZMWIErVu3BuC+++7jhhtuYMeOHVx77bUMHTqUK664osg6MjMzcXd3p0ePHvagmK+ocHYhlSZsTZgwgR07dvDHH384uhRcXV1xdXUtdNzZ2bnM/8dQHteUwhzRz7FJGcQmZebteVW/Ns7OTrQKz3tqe/h0OrmYcXO2XOIqxZOUYWVPfN6S8t0aBl7yXvu3DGX/bwf4de8pru8UUSY1lHUfp2XlsPfcPXVuUPQ9OTvD9R3q8vGqw8zecJzezUJ4cu42fticF7TevKktQ9vmDZVsUy+AMd3qM2P1Ef77815+mRiEq1PZ9H9F0r8Z5U99XDHUz+WvLPs4NzcXk8mE2Wz++2mDxbtMrl2eTCYTV199NR988AEuLi6EhYXh5JT342/+fRS4p/Ned/7x/P+azoU8s9lsDwX5bYsyduxYnnzySdatW8fq1auJjIykZ8+eF6y3UaNG7Nu3r8D1goOD7R//rMXLy6tA27S0NAYMGEC/fv34+uuvCQwMJDo6mn79+pGTk4PZbLavznj+6/7ZF/+879TUVCZPnlxg3YR8Hh4e9naurq4Frnt+f12sv8/3z3b5YfH8mi/2Nbv77rsZMGAAP//8M0uWLOHll1/m9ddf5/7772fQoEEcPXqUhQsX2n8hMWHCBP73v/8VWYfJZCry71FJ/l5ViqXf77vvPhYsWMDy5cupW/fv8b8hISFkZ2cXWvoxPj6ekJAQe5t/rk6Y//ml2uSPFxUpD5uOJgLQLNQHz3N7bAV5u1LL04Vcm8G++LLbb2vj0TMYBkTW9iTI59Lzkfo2z/u78fu+k2RaizcUo6JtPZaIzYAwXzdCLjLHKn+hjOV7E/jXlxv5YfNxLGYT74xubw9a+R7u25jaXq4cPpXGJ6u0WIaISE3g6elJw4YNqVevnj1oQd4c/pycHNatW2c/dvr0afbu3Uvz5s0veV1vb2/q1avHb7/9dsE2AQEBDBs2jOnTpzNjxgxuv/32i15z9OjR7N27lx9//LEYd1bYnj17OH36NC+//DLdu3enadOmhRbHaNasGevXry9wbO3atRe9bvv27dm7dy8NGzYs9HGx4HS+/KGOubml/7mjuF+z8PBw/v3vfzN37lweeeQRPv74Y/u5wMBAxo4dy1dffcW0adP46KOPSl1PcTg0bBmGwX333ccPP/zAb7/9RmRkZIHzHTp0wNnZucB+Bnv37iU6Oppu3boB0K1bN7Zv317gG2np0qX4+PjYO71bt26F9kRYunSp/Roi5SF/M+Pz94cymUw0C837TeDuMlyRMH9/rc4XWIXwn1rW8SHU14307Fz+2H+qzOooS/n7k7WLuPhCIg0CvejWIACbAb/uScDZYuK9m9szqHVoobY+bs48PagpAO/8tp+YM8Ufcy0iItVLo0aNGDp0KHfddRd//PEHW7du5dZbb6VOnToMHTq0WNd44okneOONN3j77bfZv38/mzZt4p133inQ5s477+Tzzz9n9+7djB079qLXGzVqFNdffz2jRo1iypQprFu3jiNHjvD7778za9asQgs1/FO9evVwcXHhnXfe4dChQ8yfP5///ve/Bdr8+9//Zv/+/Tz22GPs3buXmTNnXnJLpOeee44vvviCyZMns3PnTnbv3s23337LM888c9HXnS8iIgKTycSCBQs4efIkqampF22/fft2tmzZwpYtW9i+fTtbt24t1tds4sSJLF68mMOHD7Np0yaWL19Os2bN7Pfx448/cuDAAXbu3MmCBQvs58qLQ8PWhAkT+Oqrr5g5cybe3t7ExcURFxdHRkbeMs6+vr6MHz+ehx9+mOXLl7Nx40Zuv/12unXrRteuXQHo27cvzZs357bbbmPr1q0sXryYZ555hgkTJtiHAv773//m0KFD/Oc//2HPnj28//77zJ49m4ceeshh9y7V36Zzm/F2+EdYaBaSN055d2zZPdm61P5a/2Qymeh/bg7UrA0X36fKUez7a9W79KqNN3fJe7rlYjHzwS0d7PdWlGFt69AlshaZVhtTFuwqk1pFRKRqmj59Oh06dGDw4MF069YNwzBYuHBhsYeJjR49mjfeeIP333+fFi1aMHjw4EJLx/fp04fQ0FD69etHWNjF50mbTCZmzZrFtGnTWLhwIb1796ZJkybccccdhIeHX3K6TWBgIDNmzGDOnDk0b96cl19+udAQuXr16vH9998zb9482rRpw4cffshLL7100ev269ePBQsWsGTJEjp16kTXrl1588037YtQFEedOnWYPHkyTzzxBMHBwfYl8i+kR48etGvXjg4dOtCjRw86deoEXPprlpuby4QJE2jWrBn9+/encePGvP/++0De07Unn3yS1q1b06NHDywWC99++22x76FUDAcCivyYPn26vU1GRoZx7733Gv7+/oaHh4cxfPhwIzY2tsB1jhw5YgwYMMBwd3c3ateubTzyyCOG1Wot0Gb58uVG27ZtDRcXF6NBgwYF3uNSkpKSDMBISkq6nNstIDs725g3b56RnZ1dZteUwhzVzxnZOUbDp342Ih5fYESfTitw7rsNMUbE4wuMGz5cXSbvlZZlNaKezHuvmDNpl37BOfviko2IxxcYkU8sMI6dTS/1+/+zj/fHJxvv/rbfOJiQUupr2mw2o+3kxUbE4wuMTUfPXLJ9bq7N+Hz1YWNjMdoahmHsjUu299mvu+NKXWdF0r8Z5U99XDHUz+WvPPo4IyPD2LVrl5GRkVFm16zqcnNzjbNnzxq5ubkXbZeSkmL4+PgY33//fQVVVr0Ut5/L2sW+50uSDRy6QIZRjE3p3NzceO+993jvvfcu2CYiIuKiS10D9OrVi82bN5e4RpHS2HE8CWuuQW0vV+r6F5wX2Cw0/8lW3jK0pmKuqHQhm44mkmMzqOPnTl3/4m9N0CjYm24NAlhz6DRfrz3Kf/o3vaw6rLk2/m/Vft7+9QDZuTZeX7KXwa3DmHB1Q5qElGwS9eFTaZxNt+LiZKZFmO8l25vNJsZ0q1/s6zcO9uaOqyL5aOUhJs3fxRVRtctssRIRERHIW2n71KlTvP766/j5+XHdddc5uiRxgEqxQIZIdZM/X6tDhF+hMNUwyAtni4mUzByOJ2Zc9nutP3waKP4QwvON6Zb3+H/WXzFk5ZR+wuqxNBj54Tr+t2Qf2bk2GgR6YjNg/tYT9Ju2kn99uYEdx5OKfb38IYSt6vji4lQ+/0w92LsRIT5uRJ9J54MVB8vlPUREpOaKjo4mODiYmTNn8tlnnxVYnENqDoUtkXLwd9gqPN/IxclMVKAXUDbzttaVcL7W+a5tHkyorxun07JZuD22xK/Pysll2q8HeH27hd1xKfh5ODPtprb8+nBPfn7gKga2CsFkgsU74xn8zh/cPn29vW8uJn++W/t6fiWuqbg8XZ14dnDeIjof/H6Qo6fTyu29RESk5qlfvz6GYRATE0Pv3r0dXY44iCK2SBkzDOOSizs0D/VhT1wKu2OTubZ5cJFt8q05eJr3VxzAy9WJ2l6uBHi5UNvLldpeecvIb47Je6/ShC0ni5mbO9fj9aX7+GLNUYa3q3vpF52zJSaR/3y3lX3xqYCJfs2DeGF4awK98xamaRHmy/u3dGB/fArvLT/A/K0nWL73JMv3nmR053BeGt7qgkMoNxWxkmN5GNgqhKsa1uaPA6f4eNUhXhjWqlzfT0RERGoWhS2RMhZzJoNTqVk4W0y0rFP0fKNmoT6w+fgll383DIMpC3Zdsl1tL1ca1PYsVb2jOtfj7d/2szk6kR3Hky5Y8/ne/nU/05btw2ZALU9nhtbJ5KnRbYtcvalRsDfTRrXjwT6N+WDFAb7beIxv1scQWduTu3tEFWqfmpVj34Os/SWWfb9cJpOJf/VswB8HTrFgWyzPDW5RbsMWRUSqg+LMtxepDsrqe10/VYiUsY3RecP6WtbxveCiC+cvknExO44nszs2GRcnM88Pac4D1zRkdOd6XNs8mPb1/KhXywNfd2duv7J+qRfaCPR2ZUDLvD2pvlhz5JLtf9kRxxtL84LWsLZhLLr/StoGXPofpMjanrx6fRueH9ICgJcX7eH3fScLtdsak7eZcR0/d4KLsUHz5boiqjaB3q4kpluLrEdERLD/Mi09XfsTSs2QnZ0NcMm9zS5FT7ZEytimo4nAxYfA5W9sfPRMOmlZOXi6Fv1XcdaGaAD6twjh9isji2xTFsZ0i2D+1hP8uOUETw1shp+HS5Ht4pMzeWLuNgD+1aMBTw5shtVqLfF77TyRxOwNx7h/5ibm33cV9c97KmcfQljOT7XyWcwmhrYJ45M/DvPD5mOXHNYpIlITWSwW/Pz8SEhIAMDDw+OyV9Ot6mw2G9nZ2WRmZmI26/lFeXFEP9tsNk6ePImHh8dlL2yisCVSxi62OEa+AC9XgrxdSUjJYk9cSpFtM625/LjlBAA3dQovn2LP6RDhT7NQH3bHJjNnwzHu6tGgUBubzeDROVtJTLfSIsyHR/o2KdV7mUwm/jusJfsTUtkcnchdX2zghwlX4nUucFbE4hj/NLx9HT754zDLdieQlGHF1714m1mKiNQkISF5G8bnB66azjAMMjIycHd3r/HBszw5qp/NZjP16tW77PdU2BIpQ2lZOeyJyxsaeLGwBXlDCRNSTrI7NrnItot2xJKSmUN4LXe6NQgol3rzmUwmxnSL4Mm52/lq3VHGXxWJ2VzwH5fpq4+wav8pXJ3MvDWq7WXNbXJ1svDhrR0Y8s4f7E9I5eFZW/jw1g6YTNgX/CjvxTHO1zzUh8bBXuyLT+WXHbHc1Klehb23iEhVYTKZCA0NJSgoqMSjGqojq9XKypUr6dGjR5FzlqVsOKqfXVxcyuRJmsKWSBkqyXyjZqE+/L7v5AXnbc36KwaAGzqEFwo+5WFo2zBeWribo6fT+X3/Sa5uEmQ/tycumVd+2QPAM4Ob0zCoZJsUFyXYx43/u60DN/3fWpbsiuft3/YzpE0YielWXJ3M9nltFcFkMjGsXR1e/WUvP2w+rrAlInIRFovlsuexVAcWi4WcnBzc3NwUtspRVe9nDTAVKUMbSzDfKH/eVlFh6+jpNNYeOoPJBNd3KP5y7JfDw8WJGzrkDVf8cs1R+/FMay4PfrOF7BwbvZsGcWuXsgsi7er588LwlgBMW7afN5bsA6B13fLbzPhChrWtA8DaQ2fKZLNpEREREYUtkTKUP9+oQzHmGzU/9+RmT1wKNlvB1fxmb8h7qtWjUSBhfu5lW+RF3NYtAoDlexOIOZO34tSrv+xlb3wKtb1ceOX61mU+XvrGjuGMu6I+AD+f21i5IocQ5gvzc6drg7y9yn7ccrzC319ERESqH4UtkTJis523mXExnmxF1vbExclMenYu0Wf+Xko3J9fGdxuPAeW/MEZRNXVvVBvDgK/WHmXlvpN89udhAF67vg21vVzL5X2fHtSswLy0dg4IWwDD2+U93fph03HtJSMiIiKXTWFLpIwcOpVKUoYVN+fizTdysphpElx4KOHK/SeJT86ilqcLfZpV/DLkY7rVB2DWhhgenbMVgNu6RnB106CLvOryOFvMvHdLe+oHeODj5kSXyFrl9l4XM6BVKC5OZvYnpLLzxMX3QBMRERG5FIUtkTKSv79Wm7p+OFuK91erqHlb+QtjDG9Xp8LnLQFc0zSIOn7uJKZbSUjJIirQk6cGNiv3963l6cKiB3uw6vFr8Pcsep+v8ubj5sy15wLuvM0aSigiIiKXR2FLpAzYbAYL8ucblWAz3vwnYLtiUwA4mZLFr7vz9i+p6CGE+SxmE7d2zZu75Wwx8daodri7VMyqU+4uFofvcTXs3FDCH7eeINemoYQiIiJSelr6XaQMvLZkLyv3ncTFYmZI67Bivy4/bOU/2fph8zFybAZtw/1oHHz5y6uX1m3dIjh0MpUejQNpWcfXYXU4Qs/Ggfh7OHMyJYvVB0/RvVGgo0sSERGRKkpPtkQu0+y/YvhgxUEAXrm+Fc3Dir8/VLOQvLbHEzNIyrDahxA66qlWPi9XJ167oQ1D2hQ/OFYXLk5mBp8LzD9s0lBCERERKT2FLZHLsPrAKZ76YTsAD/RuxPB2JdsTy9fDmTrnlnafuS6agyfTcHe2MLh1aJnXKsWXP5Twl51xpGfnOLgaERERqao0jFCklA4kpPLvrzaSYzO4rk0YD/VpVKrrNAv15nhiBu8tPwDAoNaheLtVvR3Sq5P29fyICPDg6Ol0lu6KZ+i5DY8vJCXTSsyZDI6dTefY2Qxizv03PTuH5wa3oEmI44aEioiIiOMobImUwunULO6Y8RfJmTl0iPDn1cvY7LdZqA/LdieQmpX3BMXRQwgFTCYTw9rW4a1f9zN30/FCYSs9O4elu+KZt/k4m6ITScqwXvBaX649wgvDWpV3ySIiIlIJKWyJlFCmNZd/fbmR6DPphNdy56PbOuDmXPrV+s7fk6tBoCcdS7CaoZSfYe3ywtaq/Sc5mZKFv4czqw+eZt7m4+eGF+YWaF/L04W6/u7U9Xcn3N+Ds+nZzN5wjN3nVpoUERGRmkdhS6QEDMPg8e+3seHoWbzdnJg+rhMBXq6Xdc3zw9aNHcNL/YRMylZkbU/ahvuxJSaR+2Zu4tCpNE6mZNnP16vlwbB2dejbPJj6tT3xci34z+m++BRmbzjGnthkbDYDs1lfVxERkZpGYUukGBLTs9l2LImF22P5ccsJnMwmPry1Aw2DLn8uTkQtD+r6u5OUYWVE+4vPDZKKNbxdHbbEJLLu8BkA/D2cGdw6jGHt6tC+nt9Fg3GD2p64OJlJy84l5mw6EQGeFVW2iIiIVBIKW1IlxSVlcu/XGxnVuR43dizbOU6pWTlsP5bE9uOJbDuWxLZjSUSfSS/Q5oVhLbmyYe0yeT+z2cQP915Jdq6NIG+3MrmmlI0R7evwx4FTuDqZGd6uDt0bBeLiVLxFXJ0sZhoHe7HjeDK7Y5MVtkRERGoghS2pkuZuPsam6ER2x6bQq3EgQT5lE1IWbY/l4dlbybDmFjpXP8CDVnX96N8ihEFlvDR7oPflDUWU8uHt5szHYzqW+vXNQnzYcTyZXbEp9G+p5fxFRERqGoUtqZI2HjkLQIY1l2m/7uel4Ze/2tuPW47z8Oyt5NoMQn3daFPXj9bhvrSu40erOr74emg5dimZ/Pl4u2OTHVyJiIiIOILCllQ5NpvBhqNn7Z/P+iuG8VdFEhXoVeprfrfxGP/5bis2A67vUJdXRrbGogUN5DI1Dc2b06ewJSIiUjMVb/KBSCVy8GQqSRlW3J0t9GoSSK7N4NVf9pT6et+uj+axc0FrdOdwXlXQkjLS/NyTrWNnM0jOvPBeXCIiIlI9KWxJlZP/VKttuB9PDWyG2QSLd8az8eiZEl/ryzVHeGLudgwDxnSL4MVhrbREt5QZPw8XQn3z5hPujdN+WyIiIjWNwpZUOX8dyQtVHev70zjYm+s71AXg5UV7MAyj2Nf59I/DPPvjTgDuvCqSyde1UNCSMqd5WyIiIjWXwpZUORvPPdnqEOEPwEPXNsbVycxfR86ybHdCsa7x4e8H+e+CXQDc2yuKpwc102bCUi6aad6WiIhIjaWwJVVKQkomR0+nYzJB+3NhK9TXnTuuigTglV/2kJNru+Drc3JtTF20m5cX5c3xmtinEY/1a6KgJeUm/8nWrlgNIxQREalpFLakSslf8r1JsDc+bn8vxf7vnlH4eThzICGV7zYeK/K18cmZ3PzJOv7v90MAPNavCRP7NFbQknKVH7b2xiWTayv+MFcRERGp+hS2pErJXxyjY33/Asd93Z257+qGALy5bB8Z2QU3JV598DSD3l7F+sNn8HJ14t2b2zHhXHuR8lQ/wBM3ZzOZVhtHTqc5uhwRERGpQApbUqXkh61O9WsVOndbtwjq+LkTn5zFZ38eBiDXZvBLjIlxn2/kVGo2TUO8mX/flQxuHVahdUvNZTGbaBKiRTJERERqIoUtqTIysnPZeTwJ+HtxjPO5Oll4tF9jAD5ccZADCSmM/2ITi45ZMAwY1SmceROupMFlbH4sUhrNtUiGiIhIjeTk6AJEimtLTCI5NoMQHzfq+LkX2WZomzp8vPIwu2KT6TdtFbk2AxezwQvDWnFj54gKrlgkz9/Lv2uRDBERkZpET7akysjftLhDff8LLmphNpt4YkBTIG8IYVSgJw+3ymV4Ow0bFMfRXlsiIiI1k55sSZXx17mVCDsVMYTwfD0aB/J4/6YkZ1r511UR/P7rkoooT+SCmobkDSOMTcokMT0bPw8XB1ckIiIiFUFhS6oEm81gU3T+SoSFF8f4p3t6RQFgtVrLtS6R4vB2cya8ljsxZzLYFZvMFVG1HV2SiIiIVAANI5QqYV9CCimZOXi4WOxPCUSqkmYhmrclIiJS0yhsSZWw4dwQwvb1/HGy6NtWqh7N2xIREal59FOrVAkbjpxbHOMS87VEKiuFLRERkZpHYUuqhPzNjDvWV9iSqqn5ubC1Pz4Va67NwdWIiIhIRVDYkkovLimTY2czMJugXT2FLama6vq74+XqRHaujUMn0xxdjoiIiFQAhS2p9Dac21+rWagPXq5aQFOqJrPZZF/cRUMJRUREagaFLan08hfH6Kj5WlLFNQ1V2BIREalJFLak0tt4br5Wh2LsryVSmdkXyYjT8u8iIiI1gcKWVGppWTnsOvcUoJMWx5AqrrgrEsacSWfjueGzIiIiUnUpbEmltiUmkVybQR0/d0J93R1djshlaRrijckEJ1OyOJWaVWSbE4kZXPfuH1z/4Rq2xCRWbIEiIiJSphS2pFLLn6+l/bWkOvBwcaJ+gCdQ9NOt7BwbE2Zu4my6FcOAz1cfqeAKRUREpCwpbEmllr8SofbXkuqi2UUWyZi6aDeboxNxd7YA8PO22As+ARMREZHKT2FLKq1cm8Hm6EQAOkZocQypHpqF5M/bKrhIxs/bYpn+5xEA3hndjjbhfmTn2pj1V0xFlygiIiJlRGFLKq09ccmkZuXg7epEk3P7E4lUdUUtknHwZCr/+W4rAPf0iqJP82DGdI0A4Ku1R8nJtVV8oSIiInLZFLak0sqfr9W2nh8Ws8nB1YiUjWZheWHrQEIqWTm5ZGTncu9Xm0jLzqVLZC0eubYxAINah1LL04XYpEyW7U5wZMkiIiJSSgpbUmmt2Jv3A2Yn7a8l1UiYrxs+bk7k2AwOJKTy9Lzt7I1PobaXK++MboeTJe+fZTdnC6M6hQPwxZojDqxYRERESkthSyqllftOsnzvScwm6N8yxNHliJQZk8lkH0r40sLdzN10HLMpb55WkI9bgba3dI3AbILVB09zICHVEeWKiIjIZVDYEgAMw2D1gVOcSMwo0ev2x6fw4Leb6ffmSvbEXXyj1uLKysnl+fk7ARh7RX0aB2u+llQv+WHrzwOnAXi0XxO6RQUUalfHz50+zYIB+Hq9FsoQERGpapwcXYBUDnM2HOM/32/DbII+zYIZ060+VzYMwGQqeq7U3rgU3v5tPwu3x2IYeccemb2VeROuxNlyeRn+k1WHOXwqjUBvVx46N39FpDppfi5sAfRuGsS/e0RdsO2YbvVZsiueHzafoFWbiqhOREREyorClmAYBjPObZ5qM2DJrniW7IqnQaAnt3WNYGSHuvi4OQN5K6i9/et+Fu2Is7++X4tg1h46w84TyXy86hD39mpY6lqOnU3nnd/2A/D0wGb29xWpTjpF1sJiNhHm58brN7bBfJEFYK5sGECDQE8OnUzjr1MmRlRgnSIiInJ5FLaEbceS2BWbjIuTmZl3dmH+1hPM3XScQyfTmPzTLl79ZS/D2tXhdGoWS3bFA2AywcCWodx3TUOahfrw/cZjPDJnK9OW7adfixCiAr1KVcuUn3aRabXRJbIWQ9uGleVtilQakbU9WTyxB4Hervi6X/wXCiaTiTFdI5j00y5WxZkx8h8li4iISKWnOVvCN+ujARjUKpSO9WsxZWhL1j7Vm/8Oa0njYC8yrLl8sz6aJbviMZlgSJswFk/swXu3tLfPPRnRvg49GgeSnWPjie+3YbOV/AfC5XsSWLIrHovZxH+HtbzgEEaR6qBhkNclg1a+kR3q4uliIT7DxNrDZ8q5MhERESkrCls1XEqmlflbTwAwunM9+3EvVydu6xrB4ok9+PburoxsX5fRneux9KEevDO6XaFFK0wmEy8Nb4mni4W/jpzl63VHS1RHpjWXST/lLYpxx5VaFEPkfN5uzgxtGwrAl2u1UIaIiEhVobBVw/245QTp2bk0DPKiU33/QudNJhNdGwTw+o1tmDqiFQ2DLhyC6vp78PiApgC8vGgPx0uwsuFHKw9x9HQ6wT6uPNhHi2KI/NOt534Z8uuehBL93RIRERHHUdiqwQzDYOa6vCGEozvXK5Nhe7d2iaBjhD9p2bk8NXd7seaXxJxJ573lBwB4ZlBzvFw1lVDknxoFe9HQx4bNgJklfHIsIiIijqGwVYOdvzDGyPZ1yuSaZrOJV65vjYuTmd/3neSHzccv+ZrJP+0kK8fGFVEBDG4dWiZ1iFRH3UPyfnnx7foYsnJyHVyNiIiIXIrCVg12/sIYfh4uZXbdqEAvHuzdCIApC3ZxMiXrgm2X7Ypn2e4EnC0mpgzVohgiF9OqlkGIjyun07L5v98PaWVCERGRSk5hq4a60MIYZeXuHg1oHupDYrrVvvBFdo6NnSeSmLMhhknzd3Lj/63h/m82AzD+qgY0DCrdcvEiNYXFBLdfEQHAG0v3cd83m0nJtDq4KhEREbkQTY6poS61MMblcraYefX61gx9709+3hbL/vjfOXwqDWtu4d/Etwjz4f5rSr8RskhNcvsVEVgsFl5etIeft8Wy83gS793SnhZhvo4uTURERP5BYasGKo+FMYrSso4vd/dowAcrDrIvPhUAHzcnmof50CLMlxZhPjQP86FhoBdOFj1kFSkOk8nEnd0b0D7Cn/tnbubI6XSGv7+a54c05+Zy/PssIiIiJaewVQOVx8IYF/LwtY1pUNsTbzdnWoT5UNffXT8MipSB9vX8+fmBq3hk9lZ+3ZPA0z/sYN2hM7w0opVW9BQREakk9DihBiqvhTGK4mwxc0PHcPq3DCG8loeClkgZ8vNw4eMxHXlyQFMsZhPzt57gunf+YOeJJEeXJiIiIihs1TjlvTCGiFQss9nEv3pGMftfXQn1dePQqTSue/dPJv+0k6QMLZ4hIiLiSApbNUx5L4whIo7RIaIWPz/QnYGtQsi1GUz/8wi9X1/BdxuPYbNpiXgRERFHUNiqQSpqYQwRcYxani68f0sHvhzfmQaBnpxKzebROVu5/sPV7DiuoYUiIiIVTWGrBqnIhTFExHG6Nwrklwd78OSApni4WNgUnciQd//gmXnbSUzPdnR5IiIiNYbCVg3yyR+HgYpZGENEHMvFycy/ekbx2yO9uK5NGIYBX62N5to3V5KQnOno8kRERGoEha0aYs6GGH7aegKTCcZeUd/R5YhIBQnxdePt0e349u6uRAR4cDIly/6LFxERESlfCls1wO7YZJ79cQcAD/VpTNtwP8cWJCIVrmuDACYNaQHA12uPkpSulQpFRETKm8JWNZeSaeXerzeRabXRo3Eg913d0NEliYiD9GoSSNMQb9Kyc/lq3VFHlyMiIlLtKWxVY4Zh8Pj32zh8Ko0wXzem3dQWs1krEIrUVCaTiX/3jAJg+p+HybTmOrgiERGR6k1hqxqb/ucRFm6Pw9li4t1b2lPLU4tiiNR0g1uHUsfPnVOp2czZeMzR5YiIiFRrClvV1MajZ3lp4W4AnhrYjPb1tIGxiICTxczdPRoA8PHKQ+Tk2hxckYiISPWlsFUNnUnL5r6Zm8ixGQxqFco4rT4oIue5sWM4tTxdiD6TzsIdcY4uR0REpNpS2Kpmcm0GD367mdikTBrU9uTlka0wmTRPS0T+5u5iYWy3+gB8uOIghmE4tiAREZFqSmGrmnn3twOs2n8KN2cz79/aHm83Z0eXJCKV0JhuEXi4WNgVm8zK/accXY6IiEi1pLBVjaRkWnnnt/0AvDisFU1DfBxckYhUVv6eLozqVA/Ie7olIiIiZU9hqxo5kZhJjs3Az8OZkR3qOrocEank7uweiZPZxJpDp9kSk+jockRERKodha1qJD45E4AQHzcHVyIiVUGYnztD29YB9HRLRESkPDg0bK1cuZIhQ4YQFhaGyWRi3rx5Bc6PGzcOk8lU4KN///4F2pw5c4ZbbrkFHx8f/Pz8GD9+PKmpqQXabNu2je7du+Pm5kZ4eDivvvpqed+aQ8SdC1tBClsiUkz/7pm3DPziXXEcPJl6idYiIiJSEg4NW2lpabRp04b33nvvgm369+9PbGys/eObb74pcP6WW25h586dLF26lAULFrBy5Uruvvtu+/nk5GT69u1LREQEGzdu5LXXXmPSpEl89NFH5XZfjpJwLmwFe7s6uBIRqSoaBXvTp1kwhgEf/X7I0eWIiIhUK06OfPMBAwYwYMCAi7ZxdXUlJCSkyHO7d+/ml19+4a+//qJjx44AvPPOOwwcOJD//e9/hIWF8fXXX5Odnc1nn32Gi4sLLVq0YMuWLbzxxhsFQll1EJ+cBUCIr55siUjx3dOrAct2xzN38zEeurax/g0REREpIw4NW8WxYsUKgoKC8Pf355prruGFF14gICAAgDVr1uDn52cPWgB9+vTBbDazbt06hg8fzpo1a+jRowcuLi72Nv369eOVV17h7Nmz+Pv7F3rPrKwssrKy7J8nJycDYLVasVqtZXJf+dcpq+sBxCamAxDg6Vym163KyqOfpSD1ccUoz35uHeZNxwg/NhxN5PM/D/HwtY3K/D2qAn0vVwz1c/lTH1cM9XPFqIz9XJJaKnXY6t+/PyNGjCAyMpKDBw/y1FNPMWDAANasWYPFYiEuLo6goKACr3FycqJWrVrExcUBEBcXR2RkZIE2wcHB9nNFha2pU6cyefLkQseXLFmCh4dHWd0eAEuXLi2za+2LsQAmju3bwcJT28vsutVBWfazFE19XDHKq59buJjYgIU56w7RJHs/NXkvdH0vVwz1c/lTH1cM9XPFqEz9nJ6eXuy2lTpsjRo1yv7nVq1a0bp1a6KiolixYgW9e/cut/d98sknefjhh+2fJycnEx4eTt++ffHxKZu9q6xWK0uXLuXaa6/F2blsNh5+aefvQBYDr76CVnV8y+SaVV159LMUpD6uGOXdz72yc5j18gpOZdmIaHsVLevUvH369L1cMdTP5U99XDHUzxWjMvZz/qi34qjUYeufGjRoQO3atTlw4AC9e/cmJCSEhISEAm1ycnI4c+aMfZ5XSEgI8fHxBdrkf36huWCurq64uhZeZMLZ2bnMv8hldc1cm8Gp1GwA6tTyqjTfjJVFeXztpCD1ccUor372dXamd9Ngft4eyy+7E2hXP6DM36Oq0PdyxVA/lz/1ccVQP1eMytTPJamjSu2zdezYMU6fPk1oaCgA3bp1IzExkY0bN9rb/Pbbb9hsNrp06WJvs3LlygJjK5cuXUqTJk2KHEJYVZ1OzSLXZmA2QW0vrUYoIiU3qHXev60/b4vFMAwHVyMiIlL1OTRspaamsmXLFrZs2QLA4cOH2bJlC9HR0aSmpvLYY4+xdu1ajhw5wq+//srQoUNp2LAh/fr1A6BZs2b079+fu+66i/Xr1/Pnn39y3333MWrUKMLCwgC4+eabcXFxYfz48ezcuZNZs2bx1ltvFRgmWB3kr0QY6O2KxVyDJ1uISKld3SQId2cLx85msO1YkqPLERERqfIcGrY2bNhAu3btaNeuHQAPP/ww7dq147nnnsNisbBt2zauu+46GjduzPjx4+nQoQOrVq0qMMTv66+/pmnTpvTu3ZuBAwdy1VVXFdhDy9fXlyVLlnD48GE6dOjAI488wnPPPVftln3P39A4WBsai0gpubtY6N0sb9Ghn7fHOrgaERGRqs+hc7Z69ep10aEqixcvvuQ1atWqxcyZMy/apnXr1qxatarE9VUl8QpbIlIGBrcOZcG2WH7eFsuTA5piqsnLEoqIiFymKjVnSy7s77Cl+VoiUnq9mgTh6WLheGIGm2MSHV2OiIhIlaawVU3Yw5a3nmyJSOm5OVvo0zxvL8Kft2kooYiIyOVQ2Kom8hfICPZV2BKRyzOoVd6qhAu3x2KzaVVCERGR0lLYqiY0Z0tEykqPxoF4uToRm5TJ5pizji5HRESkylLYqibyw1aIwpaIXCY3ZwvXnhtKuEBDCUVEREpNYasayMrJ5Wx63qbNWiBDRMqChhKKiIhcPoWtaiDh3HwtFyczvu7ODq5GRKqD7o1r4+3mRHxyFhuOaiihiIhIaShsVQPnDyHUnjgiUhZcnSz0bR4CwM/bTji4GhERkapJYasasK9EqCGEIlKGBrc+N5RwRxy5GkooIiJSYgpb1UDcuSdbQVocQ0TK0JUNa+Pj5sTJlCz+OnLG0eWIiIhUOQpb1UCCViIUkXLg4mSmX4v8oYRalVBERKSkFLaqgTj7HlsaRigiZWvQuaGEi3bEkpNrc3A1IiIiVYvCVjWgDY1FpLxc2bA2fh7OnErNZv1hDSUUEREpCYWtaiDBvkCGwpaIlC1ni5n+54YSLtiuoYQiIiIlobBVxRmGcd4wQoUtESl7+UMJF2w9QWpWjoOrERERqToUtqq41Kwc0rNzAc3ZEpHycUVUbSJre5KcmcO366MdXY6IiEiVobBVxeXvseXt5oSHi5ODqxGR6shiNvGvHg0A+HjVIbJych1ckYiISNWgsFXFaXEMEakIw9vXIdjHlfjkLOZtPu7ockRERKoEha0qLl57bIlIBXB1snDnVXlPt/7v90Pk2gwHVyQiIlL5KWxVcfnDCIM0X0tEytnoLvXwdXfm0Kk0luyMc3Q5IiIilZ7CVhWnJ1siUlG8XJ0Y2y0CgPdXHMQw9HRLRETkYhS2qjjN2RKRijT2ivq4OZvZfjyJPw+cdnQ5IiIilZrCVhX39x5bGkYoIuUvwMuVUZ3qAfD+igMOrkZERKRyU9iq4hLOzdnSky0RqSh3do/EyWxi9cHTbIlJdHQ5IiIilZbCVhVmsxkkpGgYoYhUrLr+HlzXNgyAD1ccdHA1IiIilZfCVhV2Jj0ba66ByQSB3hpGKCIV5989owBYvCuOAwmpDq5GRESkclLYqsLyF8cI8HTF2aIvpYhUnMbB3vRpFoxhwP/9rqdbIiIiRdFP6FXY3/O19FRLRCrevVfnPd2at+U4JxIzHFyNiIhI5aOwVYXFaY8tEXGg9vX86RJZC2uuwSerDju6HBERkUpHYasKyx9GGKSwJSIOck+vvKdb36yP5ujpNAdXIyIiUrkobFVh8RpGKCIO1rNxIG3C/ciw5nLDh2vYH5/i6JJEREQqDYWtKixewwhFxMFMJhMf39aBJsHeJKRkceP/rWH7sSRHlyUiIlIpKGxVYflhS3tsiYgjBfm48e3dXWlT15ez6VZu/ngtfx054+iyREREHE5hqwr7e86WhhGKiGP5e7rw1Z1d6BxZi5SsHG77dB0r9510dFkiIiIOpbBVRVlzbZxKzQY0jFBEKgdvN2c+v70zPRsHkmm1cefnG/hlR5yjyxIREXEYha0q6mRK3uIYzhYT/h4uDq5GRCSPu4uFj8d0ZGCrELJzbUyYuYm5m445uiwRERGHUNiqovL32ArydsNsNjm4GhGRv7k4mXl7VDuu71CXXJvBw7O3Mn/rCUeXJSIiUuEUtqqoBPviGJqvJSKVj5PFzKsjW3Nb1wgAnp23g1OpWQ6uSkREpGIpbFVRf++xpflaIlI5mc0mnh/SnOahPiRlWJny0y5HlyQiIlKhFLaqqDgt+y4iVYCTxczLI1thNsH8rSdYvjfB0SWJiIhUGIWtKkp7bIlIVdG6rh+3XxkJwDM/7CAtK8fBFYmIiFQMha0qKsE+jFBztkSk8nv42sbU8XPneGIGbyzd5+hyREREKoTCVhWVP4xQe2yJSFXg6erEC8NbAjD9z8NsO5bo2IJEREQqgMJWFZU/jDBIYUtEqoirmwRxXZswbAY88f12rLk2R5ckIiJSrhS2qqD07BxSMvPmPGgYoYhUJc8NaY6fhzO7YpP59I/Dji5HRESkXClsVUH5y757uljwdnN2cDUiIsVX28uVpwc2A2Dasn0cPZ3m4IpERETKj8JWFaSVCEWkKru+Q12uiAog02rj6R92YBiGo0sSEREpFwpbVZDClohUZSaTiZeGt8LVycwfB04xd9NxR5ckIiJSLhS2qqC/w5bma4lI1VS/ticP9mkEwCu/7CHXpqdbIiJS/ShsVUHx9j229GRLRKquO69qgK+7MwkpWaw7dNrR5YiIiJQ5ha0qKE7DCEWkGnBxMtO/RQgAP22LdXA1IiIiZU9hqwpKUNgSkWpicJtQAH7ZEat9t0REpNpR2KqC/h5GqDlbIlK1dWsQQICnC2fTraw+qKGEIiJSvShsVTGGYWgYoYhUG04WM/1b5g0lXLD1hIOrERERKVsKW1VMUoaV7Jy8oTZBerIlItXA4NZhACzeGWf/901ERKQ6UNiqYvKfatXydMHVyeLgakRELl/nyFoEeruSnJnDqv0nHV2OiIhImVHYqmLy52sFeeuplohUDxaziUGt8hbKWKBVCUVEpBpR2Kpi4jVfS0SqocGt88LW0l3xZFpzHVyNiIhI2VDYqmJybQa1vVwJ81PYEpHqo309f0J93UjNyuH3fRpKKCIi1YPCVhUzunM9NjzTh5eGt3J0KSIiZcasoYQiIlINKWxVUSaTydEliIiUqcFt8lYl/HV3PBnZGkooIiJVn8KWiIhUCm3q+hJey5307Fx+25Pg6HJEREQum8KWiIhUCiaTiUGt8p5uLdimDY5FRKTqU9gSEZFKI39Vwt/2JJCalePgakRERC6PwpaIiFQaLcJ8iKztSVaOjV93xzu6HBERkcuisCUiIpWGyWSyP936aatWJRQRkapNYUtERCqVwa3z5m2t3HeSpAyrg6sREREpPYUtERGpVJqEeNMoyIvsXBtLd2kooYiIVF0KWyIiUunkP93SqoQiIlKVKWyJiEilM7hN3rytP/af4mxatoOrERERKR2FLRERqXSiAr1oHupDjs3Q0y0REamyFLZERKRSGtmhLgDfbTzm4EpERERKp1RhKyYmhmPH/v6f3/r165k4cSIfffRRmRUmIiI127C2YTiZTWw9lsS++BRHlyMiIlJipQpbN998M8uXLwcgLi6Oa6+9lvXr1/P0008zZcqUMi1QRERqpgAvV65pGgTA93q6JSIiVVCpwtaOHTvo3LkzALNnz6Zly5asXr2ar7/+mhkzZpRlfSIiUoNdf24o4dzNx8nJtTm4GhERkZIpVdiyWq24uroCsGzZMq677joAmjZtSmxsbNlVJyIiNdrVTYMI8HThZEoWK/efdHQ5IiIiJVKqsNWiRQs+/PBDVq1axdKlS+nfvz8AJ06cICAgoEwLFBGRmsvZYmZYuzqAFsoQEZGqp1Rh65VXXuH//u//6NWrF6NHj6ZNmzYAzJ8/3z68UEREpCzkDyVctiuhWHtuZefY+O+CXXyzPrq8SxMREbkop9K8qFevXpw6dYrk5GT8/f3tx++++248PDzKrDgREZFmoT60CPNh54lk5m89wdgr6l+0/UcrD/LpH4dxcTIzon0dXJ0sFVOoiIjIP5TqyVZGRgZZWVn2oHX06FGmTZvG3r17CQoKKtMCRUREri/mnlsHElJ5+9cDQN4Trp0nksu9NhERkQspVdgaOnQoX3zxBQCJiYl06dKF119/nWHDhvHBBx+UaYEiIiJD29bB2WJi+/Ek9sQVHaBsNoOn5m4n+7xVCzdHJ1ZQhSIiIoWVKmxt2rSJ7t27A/Ddd98RHBzM0aNH+eKLL3j77bfLtEAREZFani70bhoMwHcbin669c1f0aw/cgYPFws3d6kHwKbosxVWo4iIyD+VKmylp6fj7e0NwJIlSxgxYgRms5muXbty9OjRMi1QREQE/h5KOG/Lcaz/2HMrLimTlxfuAeCxfk0Y3DoUgM1HFbZERMRxShW2GjZsyLx584iJiWHx4sX07dsXgISEBHx8fMq0QBEREYCeTQKp7eXCqdRsft/7955bhmHw7I87SMnKoW24H2O61adNXT/MJjiRlElcUqYDqxYRkZqsVGHrueee49FHH6V+/fp07tyZbt26AXlPudq1a1emBYqIiEDenlvDz+25NWdjjP34LzviWLorHieziVdGtsZiNuHp6kSTkLxf/mkooYiIOEqpwtb1119PdHQ0GzZsYPHixfbjvXv35s033yyz4kRERM438txQwl93J3A6NYukdCvPzd8JwD29omgS4m1v276eHwCbFbZERMRBSrXPFkBISAghISEcO5Y3Ublu3bra0FhERMpV0xAfWtXxZfvxJOZvPcHeuBROpmTRINCTCVc3LNC2fT1/vl4XzSatSCgiIg5SqidbNpuNKVOm4OvrS0REBBEREfj5+fHf//4Xm8126QuIiIiUUv5CGe+vOMi3f+UNJ3x5RGvcnAtuXtw+Im8vyO3Hk8jO0f+bRESk4pUqbD399NO8++67vPzyy2zevJnNmzfz0ksv8c477/Dss8+WdY0iIiJ217UJw8Vi5mRKFgC3dKlH58hahdrVD/DA38P53ObGSRVdpoiISOnC1ueff84nn3zCPffcQ+vWrWndujX33nsvH3/8MTNmzCj2dVauXMmQIUMICwvDZDIxb968AucNw+C5554jNDQUd3d3+vTpw/79+wu0OXPmDLfccgs+Pj74+fkxfvx4UlNTC7TZtm0b3bt3x83NjfDwcF599dXS3LaIiFQC/p4u9GkeBECwjyuPD2haZDuTyUS7enlPtzSUUEREHKFUYevMmTM0bVr4f25NmzblzJkzxb5OWloabdq04b333ivy/Kuvvsrbb7/Nhx9+yLp16/D09KRfv35kZv69jO8tt9zCzp07Wbp0KQsWLGDlypXcfffd9vPJycn07duXiIgINm7cyGuvvcakSZP46KOPSnDHIiJSmTzUpzE9Ggfyzuj2+Lg5X7CdFskQERFHKtUCGW3atOHdd9/l7bffLnD83XffpXXr1sW+zoABAxgwYECR5wzDYNq0aTzzzDMMHToUgC+++ILg4GDmzZvHqFGj2L17N7/88gt//fUXHTt2BOCdd95h4MCB/O9//yMsLIyvv/6a7OxsPvvsM1xcXGjRogVbtmzhjTfeKBDKRESk6mgU7M0Xd1x6Uab2555sbdaTLRERcYBSha1XX32VQYMGsWzZMvseW2vWrCEmJoaFCxeWSWGHDx8mLi6OPn362I/5+vrSpUsX1qxZw6hRo1izZg1+fn72oAXQp08fzGYz69atY/jw4axZs4YePXrg4uJib9OvXz9eeeUVzp49i7+/f6H3zsrKIisry/55cnIyAFarFavVWib3l3+dsrqeFE39XP7UxxVD/Vw6zUM8MZvgeGIGx06nEOzjdsG26uOKoX4uf+rjiqF+rhiVsZ9LUkupwlbPnj3Zt28f7733Hnv27AFgxIgR3H333bzwwgt07969NJctIC4uDoDg4OACx4ODg+3n4uLiCAoKKnDeycmJWrVqFWgTGRlZ6Br554oKW1OnTmXy5MmFji9ZsgQPD49S3lHRli5dWqbXk6Kpn8uf+rhiqJ9LLtTdwvF0E5/+uJy2AcYl26uPK4b6ufypjyuG+rliVKZ+Tk9PL3bbUu+zFRYWxosvvljg2NatW/n000+r/HyoJ598kocfftj+eXJyMuHh4fTt2xcfH58yeQ+r1crSpUu59tprcXa+8HwDuTzq5/KnPq4Y6ufSW5Ozi2//OoY5sAED+ze5YDv1ccVQP5c/9XHFUD9XjMrYz/mj3oqj1GGrvIWEhAAQHx9PaGio/Xh8fDxt27a1t0lISCjwupycHM6cOWN/fUhICPHx8QXa5H+e3+afXF1dcXV1LXTc2dm5zL/I5XFNKUz9XP7UxxVD/VxyHesH8O1fx9h6LLlYfac+rhjq5/KnPq4Y6ueKUZn6uSR1lGo1wooQGRlJSEgIv/76q/1YcnIy69ats88T69atG4mJiWzcuNHe5rfffsNms9GlSxd7m5UrVxYYW7l06VKaNGlS5BBCERGpXvJXJNymzY1FRKSCOTRspaamsmXLFrZs2QLkLYqxZcsWoqOjMZlMTJw4kRdeeIH58+ezfft2xowZQ1hYGMOGDQOgWbNm9O/fn7vuuov169fz559/ct999zFq1CjCwsIAuPnmm3FxcWH8+PHs3LmTWbNm8dZbbxUYJigiItVXZG1P/M5tbrwrtvhDP0RERC5XiYYRjhgx4qLnExMTS/TmGzZs4Oqrr7Z/nh+Axo4dy4wZM/jPf/5DWload999N4mJiVx11VX88ssvuLn9vZrU119/zX333Ufv3r0xm82MHDmywJL0vr6+LFmyhAkTJtChQwdq167Nc889p2XfRURqCJPJRLtwP5bvPcmmo2dpG+7n6JJERKSGKFHY8vX1veT5MWPGFPt6vXr1wjAuvDKUyWRiypQpTJky5YJtatWqxcyZMy/6Pq1bt2bVqlXFrktERKqX9vX888JW9FnuIPLSLxARESkDJQpb06dPL686REREyk37CG1uLCIiFa/SLpAhIiJSVtqE+9k3N05IznR0OSIiUkMobImISLXn5epE42BvADZFn3VwNSIiUlMobImISI3Qrl7eUMJNGkooIiIVRGFLRERqhPz9tjYd1ZMtERGpGApbIiJSI+QvkrFdmxuLiEgFUdgSEZEaocG5zY2zcmzs1ubGIiJSARS2RESkRsjf3Bi0SIaIiFQMhS0REakxtEiGiIhUJIUtERGpMdrnhy0tkiEiIhVAYUtERGqMNuG+mM5tbnz4VJqjyxERkWpOYUtERGoMbzdnWtf1A+C6d/7gy7VHsdkMxxYlIiLVlsKWiIjUKG/c2IY24X6kZOXw7LwdXP/havbGpTi6LBERqYYUtkREpEaJCvRi7j1XMPm6Fni6WNgUncigt1fx+tL9ZOc6ujoREalOFLZERKTGsZhNjL2iPsse6Unf5sHk2Aw+XHmYV7ZaWH3wtKPLExGRasLJ0QWIiIg4SqivOx+N6cjinXE89+MO4pOzGDtjI0HerkQFehEV5ElUoBcNg7yICvQi1NcNk8nk6LJFRKSKUNgSEZEar1+LEDrV82Xip8v4M8FCQkoWCSlZrDlU8CmXh4uFGzuGM+m6Fg6qVEREqhKFLREREcDbzYmRkTamje9DdGIWBxNSOXgy/yONI6fSSM/O5fM1R5jYpxF+Hi6OLllERCo5hS0REZHzeLs50TbcnbbhfgWOW3Nt9J+2koMn01h76Az9W4Y4pkAREakytECGiIhIMThbzFwRVRuAtYe0iIaIiFyawpaIiEgxdYsKABS2RESkeBS2REREiqlLZC0A9sSlcDo1y8HViIhIZaewJSIiUkwBXq40CfYGYN3hMw6uRkREKjuFLRERkRLIH0q4Rpsfi4jIJShsiYiIlEDXBnlDCTVvS0RELkVhS0REpAS6RAZgMsH+hFROpmjeloiIXJjCloiISAn4e7rQNMQH0NMtERG5OIUtERGREsofSrhGYUtERC5CYUtERKSEujXQflsiInJpClsiIiIllD9v69DJNOKTMx1djoiIVFIKWyIiIiXk6+FMizDN2xIRkYtT2BIRESmFrpEaSigiIhensCUiIlIK2txYREQuRWFLRESkFDpF1sJsgiOn04lNynB0OSIiUgkpbImIiJSCj5szLev4Anq6JSIiRVPYEhERKSUtAS8iIhejsCUiIlJKXfPnbSlsiYhIERS2RERESqlT/VpYzCZizmRw7Gy6o8sREZFKRmFLRESklLxcnWh1bt7W2kNnHFyNiIhUNgpbIiIil0FLwIuIyIUobImIiFyG8xfJMAzDwdWIiEhlorAlIiJyGTpE+ONkNnE8MYOYM9pvS0RE/qawJSIichk8XZ1oE+4HaAl4EREpSGFLRETkMuUPJdQS8CIicj6FLRERkcvUtcHfi2Ro3paIiORT2BIREblMHSL8cbaYiEvO1BLwIiJip7AlIiJymdxdLPRsHATAbZ+u473lB8i16QmXiEhNp7AlIiJSBl6/sQ2DWoeSYzN4bfFebv1kHXFJmY4uS0REHEhhS0REpAz4ujvz7uh2vHp9azxcLKw5dJr+b61k8c44R5cmIiIOorAlIiJSRkwmEzd2DGfB/VfRqo4vielW/vXlRp76YTsZ2bkF2tpsBidTsthxPInlexI4djbdQVWLiEh5cXJ0ASIiItVNg0Avvr/nCl5fspf/W3mImeuiWXvoNI2DvIlPySQ+KZOElCxyzpvXFeLjxp9PXIPFbHJg5SIiUpYUtkRERMqBi5OZJwc2o3ujQB6evYVDJ9M4dDKtQBuTCWp7uZKYnk1cciZbjyXSvp6/gyoWEZGyprAlIiJSjq5qVJtFD3bnh83HcbaYCfZxJdjHjRBfN2p7ueJsMXPv1xtZuD2OlftOKmyJiFQjClsiIiLlLMDLlTu7N7jg+R6NAu1ha2KfxhVYmYiIlCctkCEiIuJgPRoHArAlJpGkdKuDqxERkbKisCUiIuJgYX7uNAzywmbAnwdPObocEREpIwpbIiIilUCPRnlPt1buO+ngSkREpKwobImIiFQCPRrXBvLClmEYl2gtIiJVgcKWiIhIJdAlMgAXJzMnkjI5eDLV0eWIiEgZUNgSERGpBNxdLHSJrAXA7/s0b0tEpDpQ2BIREakkNG9LRKR6UdgSERGpJPKXgF93+DSZ1lwHVyMiIpdLYUtERKSSaBzsRYiPG5lWG38dOePockRE5DIpbImIiFQSJpOJ7o3+XpVQRESqNoUtERGRSiR/KOHvClsiIlWewpaIiEglclXD2phMsC8+ldikDEeXIyIil0FhS0REpBLx93ShdV0/AFZpCXgRkSpNYUtERKSS6Zk/lHC/hhKKiFRlClsiIiKVTM/GeYtk/LH/FLk2w8HViIhIaSlsiYiIVDJt6vrh7eZEUoaVbccSHV2OiIiUksKWiIhIJeNkMXNVw/wl4DVvS0SkqlLYEhERqYTyl4BfeYF5W4Zh8NXao/R+fQU/bD5WkaWJiEgxOTm6ABERESksP2xtiUkkKcOKr7uz/VxcUib/+X6bfePjp+buoHNkAHX83B1Sq4iIFE1PtkRERCqhOn7uRAV6kmszWH3g76GE87eeoN+0lazcdxJXJzP1AzzIsOYyaf5OB1YrIiJFUdgSERGppM4fSpiYns19MzfxwDebScqw0qqOLz8/cBUfjemIk9nE0l3xLNkZ5+CKRUTkfApbIiIilVR+2Fq8M56+b65kwbZYLGYTD/ZuxNx7r6BhkDeNg725q0cDACbN30laVo4jSxYRkfMobImIiFRSXSMDcHEycyYtm4SULBoEejL3nit46NrGOFv+/l/4A9c0IryWOyeSMpm2bJ8DKxYRkfMpbImIiFRS7i4W+jQLAuD2K+uz8IHutAn3K7LdlOtaAvDZn0fYdSK5IssUEZELUNgSERGpxN4a1Y61T/bm+SEtcHO2XLDd1U2DGNgqhFybwVM/bMdmMyqwShERKYrCloiISCXmbDET4utWrLbPD2mBl6sTW2ISmbk+upwrExGRS1HYEhERqSaCfdx4tG9jAF75ZQ8nU7IcXJGISM2msCUiIlKN3NatPq3q+JKSmcMLP+9ydDkiIjWawpaIiEg1YjGbeGl4K8wm+HHLCVbtP+nokkREaiyFLRERkWqmVV1fxnSrD8Cz83aQac11bEEiIjWUwpaIiEg19EjfxgR5u3LkdDofrzzk6HJERGokhS0REZFqyNvNmacHNQPg3eUHiDmT7uCKRERqHoUtERGRauq6NmF0axBAVo6NyT9psQwRkYqmsCUiIlJNmUwmpgxtgZPZxLLd8fy6O97RJYmI1CgKWyIiItVYo2Bvxl8VCcCkn3ZqsQwRkQqksCUiIlLNPdC7ESE+bsScyeCDFQcdXY6ISI1RqcPWpEmTMJlMBT6aNm1qP5+ZmcmECRMICAjAy8uLkSNHEh9fcIhEdHQ0gwYNwsPDg6CgIB577DFycnIq+lZEREQcxtPViWcHNwfgg98PcvR0moMrEhGpGSp12AJo0aIFsbGx9o8//vjDfu6hhx7ip59+Ys6cOfz++++cOHGCESNG2M/n5uYyaNAgsrOzWb16NZ9//jkzZszgueeec8StiIiIOMzAViFc1bA22Tk2Js3fiWEYji5JRKTac3J0AZfi5ORESEhIoeNJSUl8+umnzJw5k2uuuQaA6dOn06xZM9auXUvXrl1ZsmQJu3btYtmyZQQHB9O2bVv++9//8vjjjzNp0iRcXFyKfM+srCyysrLsnycnJwNgtVqxWq1lcl/51ymr60nR1M/lT31cMdTP5a8m9PGzA5sw+L3TLN97kl+2n6BPs6AKr6Em9LOjqY8rhvq5YlTGfi5JLSajEv9qa9KkSbz22mv4+vri5uZGt27dmDp1KvXq1eO3336jd+/enD17Fj8/P/trIiIimDhxIg899BDPPfcc8+fPZ8uWLfbzhw8fpkGDBmzatIl27dpd8H0nT55c6PjMmTPx8PAo69sUERGpMD9Fm1l23EwtV4Mn2+TiYnF0RSIiVUt6ejo333wzSUlJ+Pj4XLRtpX6y1aVLF2bMmEGTJk2IjY1l8uTJdO/enR07dhAXF4eLi0uBoAUQHBxMXFwcAHFxcQQHBxc6n3/uQp588kkefvhh++fJycmEh4fTt2/fS3ZocVmtVpYuXcq1116Ls7NzmVxTClM/lz/1ccVQP5e/mtLHvbJz6P/2amKTMjns3piH+jSs0PevKf3sSOrjiqF+rhiVsZ/zR70VR6UOWwMGDLD/uXXr1nTp0oWIiAhmz56Nu7t7ub2vq6srrq6uhY47OzuX+Re5PK4phamfy5/6uGKon8tfde9jX2dnnh/SnH9/tYlP/jhCp8gArm5a8cMJq3s/Vwbq44qhfq4YlamfS1JHpV8g43x+fn40btyYAwcOEBISQnZ2NomJiQXaxMfH2+d4hYSEFFqdMP/zouaBiYiI1AT9WoTQp1kQ2bk2bp/xF5Pma/8tEZHyUKXCVmpqKgcPHiQ0NJQOHTrg7OzMr7/+aj+/d+9eoqOj6datGwDdunVj+/btJCQk2NssXboUHx8fmjdvXuH1i4iIVAYmk4l3b27PuCvqAzBj9RGGvfcne+NSHFuYiEg1U6nD1qOPPsrvv//OkSNHWL16NcOHD8disTB69Gh8fX0ZP348Dz/8MMuXL2fjxo3cfvvtdOvWja5duwLQt29fmjdvzm233cbWrVtZvHgxzzzzDBMmTChymKCIiEhN4eZsYdJ1LZh+eydqe7mwJy6FIe/+weerj2hZeBGRMlKpw9axY8cYPXo0TZo04cYbbyQgIIC1a9cSGBgIwJtvvsngwYMZOXIkPXr0ICQkhLlz59pfb7FYWLBgARaLhW7dunHrrbcyZswYpkyZ4qhbEhERqVSubhLEogd70KtJINk5Np6fv5M7P9/AqdSsS79YREQuqlIvkPHtt99e9Lybmxvvvfce77333gXbREREsHDhwrIuTUREpNoI9HZl+rhOzFh9hKmL9vDrngT6T1vFKyNb0btZ8KUvICIiRarUT7ZERESkYphMJm6/MpIfJ1xJoyAvTqVmMf7zDTz47WZO6ymXiEipKGyJiIiIXbNQH366/yru6h6J2QQ/bjnBtW+u5MctxzWXS0SkhBS2REREpAA3ZwtPD2rOD/deSdMQb86kZfPgt1sY//kGTiRmOLo8EZEqQ2FLREREitQm3I/5913Fw9c2xsVi5rc9CfR9cyVfrT2KzaanXCIil6KwJSIiIhfk4mTmgd6N+PmBq2hfz4/UrByembeDOz7/i5xcm6PLExGp1BS2RERE5JIaBXsz599X8PyQ5rg7W1ix9yTTlu13dFkiIpWawpaIiIgUi8Wct2Lhaze0BuC9FQf488ApB1clIlJ5KWyJiIhIiQxuHcbozuEYBkyctUUbIIuIXIDCloiIiJTYc4Nb0CjIi5MpWTwye6sWzBARKYLCloiIiJSYu4uFd29uj6uTmd/3neSTPw45uiQRkUpHYUtERERKpUmIN88PaQHAq7/sZWtMomMLEhGpZBS2REREpNRGdw5nUKtQcmwG93+zmeRMq6NLEhGpNBS2REREpNRMJhMvjWhFXX93os+k89Tc7RiG5m+JiIDCloiIiFwmX3dn3h7dDovZxIJtsczeEOPokkREKgWFLREREbls7ev582jfJgA8P38nR06lObgiERHHU9gSERGRMvGvHg3o1iCATKuNj1dpdUIREYUtERERKRNms4kH+zQC4PtNx0hMz3ZwRSIijqWwJSIiImWmS2Qtmof6kGm1MXN9tKPLERFxKIUtERERKTMmk4nxV0UC8MXqo1hzbQ6uSETEcRS2REREpEwNbhNKbS9X4pIzWbQjztHliIg4jMKWiIiIlClXJwu3dY0A4LM/Dju4GhERx1HYEhERkTJ3S9d6uDiZ2RKTyKbos44uR0TEIRS2REREpMzV9nJlWNswAD7V0y0RqaEUtkRERKRc3H5l3kIZv+yI43hihoOrERGpeApbIiIiUi6ahfpwRVQAuTaDL9YccXQ5IiIVTmFLREREys0d555ufbMumvTsHAdXIyJSsRS2REREpNxc0zSI+gEeJGfm8P3GY44uR0SkQilsiYiISLkxm032uVvT/zyCzWYU2W5PXDKPf7eNN5fuwzCKbiMiUtU4OboAERERqd6u71CX/y3Zy6FTaaw8cKrAuR3Hk3jnt/0s3hlvP9axvj/dGwVWdJkiImVOT7ZERESkXHm6OjGqUzgAM1ZHA7D1WBLjZ/zF4Hf+YPHOeEwmqB/gAcBri/fq6ZaIVAt6siUiIiLlbuwV9fn0j8P8efA08SctHFizDgCzCa5rE8aEqxtSy9OFHq8uZ9uxJH7ZEceAVqEOrlpE5PLoyZaIiIiUu7r+HvRvGQLAgWQTFrOJ6zvU5ddHejFtVDsaBXsT4OXK+O4NAHhtyV5ycm2OLFlE5LIpbImIiEiFePjaxrQI86ZbkI0lD17J/25oQ2RtzwJt7uoeib+HM4dOpjF303EHVSoiUjYUtkRERKRCNAzyZt493RgVZaNeLY8i23i7OTPh6oYAvLlsH5nW3IosUUSkTClsiYiISKVya9cIQn3diE3K5Ku1Rx1djohIqSlsiYiISKXi5mxhYp9GALy/4iApmVYHVyQiUjoKWyIiIlLpjGxflwaBnpxJy+aTVYcdXY6ISKkobImIiEil42Qx82jfJgB8suoQp1OzHFyRiEjJKWyJiIhIpTSgZQit6viSlp3Le8sPOrocEZESU9gSERGRSslkMvGf/nlPt75ae5TjiRkFzltzbWyOPssHKw7y4LebWX/4jCPKFBG5ICdHFyAiIiJyIVc1rE23BgGsOXSaN5bs45au9Vh76DTrDp1hw5EzpGX/vTT8zhPJLH2oByaTyYEVi4j8TWFLREREKi2TycRj/Zsw4v3VfL/pGN9vOlbgvK+7M10ia7Fq/ykOJKSy8ehZOtav5aBqRUQKUtgSERGRSq19PX+uaxPG/K0n8PPIC1ddGwTQJTKApiHemM0mHpuzlTkbj/HN+hiFLRGpNBS2REREpNJ7/cY2PNavCXX83DGbCw8THN2lHnM2HuPn7Sd4bkhzfN2dHVCliEhBWiBDREREKj1ni5nwWh5FBi2AduF+NAn2JtNq48ctxyu4OhGRoilsiYiISJVnMpkY1TkcgG/Wx2AYhoMrEhFR2BIREZFqYni7Org4mdkdm8y2Y0nFek2uzSAhObOcKxORmkphS0RERKoFPw8XBrYMAeDbv6Iv2d4wDB74ZjNdpv7Kir0J5V2eiNRAClsiIiJSbYzuXA+AH7ecIDUr56Jtf9xygp+3x2IY8PqSfRp6KCJlTmFLREREqo3OkbVoEOhJenYuP209ccF2CSmZPD9/p/3z7ceT+H3fyYooUURqEIUtERERqTZMJhOjOuUtlPHt+qKHEhqGwdM/7CApw0rLOj6Mu6I+AO/8dkBPt0SkTClsiYiISLUysn1dnC0mth5LYteJ5ELn5289wdJd8ThbTLx2fRvu6RWFi5OZjUfPsvbQGQdULCLVlcKWiIiIVCsBXq70bVH0QhnnDx+8/5pGNAv1IdjHjZs65j0Ne3f5/ootVkSqNYUtERERqXZGd8pbKOOHzcfJyM4F8oYPPvPDDhLTrbQI8+GeXlH29v/q2QAns4k/D5xm49GzDqlZRKofhS0RERGpdq6ICiC8ljspmTn8vD0WgJ+2xbJkVzxOZhP/u6ENzpa/fwyq6+/BiPZ1AHhv+QGH1Cwi1Y/CloiIiFQ7ZrOJUeeebn27PpqTKVk8/+MO4O/hg/90T6+GmE3w254Edhwv3qbIIiIXo7AlIiIi1dINHepiMZvYcPQsd3+5gbPpVpqH+nDv1VFFto+s7cmQNmEAvPubnm6JyOVT2BIREZFqKcjHjd5NgwDYHJ2Ik9nEaze0LjB88J/uu7ohAL/sjGNffEqF1Cki1ZfCloiIiFRbozvXs//5vmsa0iLM96LtGwV7M6Bl3kqGmrslIpdLYUtERESqrR6NA7m2eTB9mgVzb6+GxXrNhHNPt37aeoLDp9IKnTcMg83RZ5ny0y7e/W2/NkIWkQtycnQBIiIiIuXFYjbx8ZiOJXpNyzq+XNM0iN/2JPDBigO8en0bAI6cSmPeluPM23ycI6fT7e1zbAYT+zQu07pFpHpQ2BIRERH5hwlXN+S3PQnM3XScBoFeLNkZx6boRPt5d2cLnSJrsXLfSaYt209kbU+Gtq3juIJFpFJS2BIRERH5hw4R/lzZMIA/D5zm5UV7ADCb4KpGgYxoV4drmwfj6erESwt389HKQzz23Tbq+nvQIcLfwZWLSGWisCUiIiJShP/0a8ptx9YRXsuD4e3qcF3bMIK83Qq0ebx/Uw6dTGPZ7nj+9eUGfrj3SsJreTioYhGpbBS2RERERIrQJtyPbZP6XbSNxWzirVFtueHDNeyKTebOzzfw3T3d8HZzrqAqRaQy02qEIiIiIpfB09WJT8d1JMjblb3xKdz/zWZycm2OLktEKgGFLREREZHLFOrrzidjO+LmbGbF3pO88PNuR5ckIpWAwpaIiIhIGWhd1483b2wLwIzVR/hizRGH1iMijqewJSIiIlJGBrQK5bF+TQCY/NMuJs3fyYnEDAdXJSKOorAlIiIiUobu7RXFqE7h5NoMZqw+Qs/XlvPk3G0cPZ3m6NJEpIIpbImIiIiUIZPJxNQRrfj6zi50bVALa67BN+tjuPp/K3ho1hYOJKQ4ukQRqSBa+l1ERESkjJlMJq5sWJsrG9Zmw5EzvLv8ACv2nuSHzceZt+U4A1qG8PC1TWgY5OXoUkWkHOnJloiIiEg56li/FjNu78xP911FvxbBGAYs3B7HwLdX8cmqQ9hshqNLFJFyorAlIiIiUgFa1fXl/27ryOKJPejZOJDsHBsv/Lybmz9ZS8yZdEeXJyLlQGFLREREpAI1CfFmxu2deHF4SzxcLKw9dIYBb61i9oYYDOPiT7kMwyDTmltBlYrI5dKcLREREZEKZjKZuKVLBFc1rM0js7ey4ehZ/vPdNpbsjGPqiNYEersCkGnNZduxJDYePcvGo2fZFH2Ws+nZXNWwNjd0DKdv82DcnC0OvhsRuRCFLREREREHiQjwZNa/uvHxqkO8sWQfy3YnsGnaSga0DGHH8SR2nkgmp4g5Xav2n2LV/lP4uDkxtG0dbuwYTss6PphMJgfchYhciMKWiIiIiANZzCb+3TOKXk0CeWjWVnbHJvP1umj7+SBvVzrW96d9PX/aR/jj5+7MvC0n+G5DDCeSMvly7VG+XHuUpiHe3NAxnMEtgxx4NyJyPoUtERERkUqgaYgPP064ki/WHOFEYiZt6/nRvp4fdfzcCz2xevjaxjzYuxGrD55i9oZjLN4Zx564FP67YBcvL9pNSz8zvk1O06NxMGaznnaJOIrCloiIiEgl4eJk5s7uDYrV1mI20b1RIN0bBZKUbmX+1uPM2hDDjuPJbD5tZtyMjYTXcuemjuFc3yGcEF+3cq5eRP5JYUtERESkivP1cOa2bvW5rVt9thw9zf/mrmZrkgsxZzL435J9vLF0H1c3CaJvi2CCvN0I8HIhwMuVAE8XLbAhUo4UtkRERESqkRZhPtzQwMb7fXqydM8pZv0Vw/ojZ/h1TwK/7kko1N7L1Ylani4E+7jSqo4fHev70yHCn2AfPQkTuVwKWyIiIiLVkLuLhZEd6jKyQ10OJKQyZ2MMu2NTOJOWxenUbE6nZpOdayM1K4fUrByiz6Tz15GzfPbnYQDq+LnTIcLfvjhHo2AvXJ0q5ilYTq6NvfEpZGTn0jjEGx835wp5X5GyprAlIiIiUs01DPLiyQHNChwzDIOUrBzOpGZzOi2L6DPpbI5OZOPRs+yOTeZ4YgbHEzOYv/UEACYThPq4US/Ag3q1PIgI8Dz3Xw+CvN1wczbj5mzB1clc4iXoz6Rlszk6bx+xjUfPsu1YEunZf2/eXMfPnSYh3jQN8aZpqA9NQ7yJrO2Js8V8+Z1ThWTl5HLoZBr74lPYH5/KvvgUYpMyCfNzo0GgF1GBXjQI9CSqthe+HlUjoCZlWIk5k573cTadLKsNi8WExWTCYjaBYWNPnImkv2JwdXaid7Nganu5OrrsYlPYEhEREamBTCYTPm7O+Lg5U7+2Jx0iajG8XV0AUrNy2BqTWGAz5ZTMHE4kZXIiKZO1h85c9NouTmbcnPLCl5uzBRcnM65O5vP+mxfKLCYTe+NTOHwqrdA1vF2d8HJzIjYp0x78fjtvGKTFbMLX3Rk/d2d83J3x88j7s++5j1zDIC0rl/Ts/2/v7qOiqvM/gL9nBmZ4GIYBBgZQRBQfekBSXAn7ZZ5kAeuUrbur63oqrbWH1R6O5bJuu5GdTrq5W+3ptNXZk9meOj3tUTu7maUmKylqmmj4wCqhaPIg4DADA8ww8/n9Qdy8goDGDAO8X+dwYL73O9/7uZ/7bbof78x32tHs8sDZ1g6nywOnywOjIQiZY6IxfawFNySZoQ+6fNHW7vHiyDk79lU04OCZCwjXB2FiggnjYkPR5L7K5F8yfvn5ZtQ3taGprR3NrnY0tXnQ3NaO5u/vOlY3tuJ/NQ6cqnfC0833rn3zXSOAGlWbxajHGIsRIXodXO0etLV74Wr3XvTbA51Gg6hwPaLD9YgJ1yM63IAYY8fj6HA9YiMMsJpCEGs09JijyxER1De7UN3YinO2FlR9//vMBSfONLSgssGJxpa+JFGHf1UcAwBs+G0Eiy0iIiIiGryMhiDclGrBTakWAD9cNJ+u77gDcbreidMNzaisd+J0gxMXml2qL192fX9Bb29t7/M+x8aGK98llpEchdRYI7RaDRqdbpTVOHC82o7j1Q4cr7KjrNqBZpcHDc0uNDS7ruoYi7+tx8vbTiA0WIepo6MwfawF08fGYEJ8BI6ca8Sebxuwr6IBB05fQFPb5Y4jCH8r+y8mxEfgmgQTxsaGI84Ugrjvi5ToML1q6X0Rwel6Jw6dteHQmUYcPmtD6blGtLq9fY47IiQI460RGG81IjUuAiOjQnHO1oLy80349nwzys83ocbehromF+qaei6KAeBcY2uf9hsVFoy4iBDEmQyIiwiBPkgLj9eLdq/A45WO356O345WN6rtrahqbIWrvfdjsxgNSIoORVJUGMINuh/G8wra2704e+4cYuOsEGhgDh0cd+w6sdgiIiIioh5pNBpYjAZYjAZkJEd126fd40Vruxetbg9a3R13Ujr+/uFOivrOihdujxejosMweZQZ5jB9t+NGhgVjWko0pqVEK21er+B8UxtsTjdsThcaW9ywtbhhb3HD5nSjscWNIJ0G4foghOp1CNfrEGYIQrg+CGF6HaoaW7G7vA7F5fWob3ah6EQdik7UXfb4TSFBmJYSjamjo+F0eVBWbcexKjsqG1pQ62hDraOt2+cHaTvyZjUZEBKsw/FqR7d3coyGICREhiDcEASjIQjhBh2MhmAYDTqEG4IQYzRgXJwR460RsJoMvb5N09HqRkVdMyrqmuH2iOquoiHohzuNHq+godmF+mZXx2f5ml1oaHLhgtOFuiYXzjvaUOtohdsjuOB048L3he+V0Gg6iqnEyBAkRIYiwRyCpKiOt6ImRYchKToUYfrLlyRutxubN5/FbbdNRnDw4Cq0ABZbRERERNQPgnRaGHVaGA2+v7zUajWwmkJ+1IqJv84cBRHB/2qasLu8DrvL67Hn23o4WtsRE65XCrzMlI67XbpLvhza7XZj4783I+WG6ThZ14LjVXZUNjhRY+8ovuqb29DuFVTbW1Ft/+HukV6nxbWJJqSPjMSkkWakJ5kxxhLer18+HRESjEkjzZg00vyjxxIR2Jxu1DhaUfv9sdU6WuHxCHQ6DYK0Gui02u9/dzwO1es6CqvIjnN0NW9BHCpYbBERERHRsKTRaDAhPgIT4iOw+KYUeLyC8462Pt09AgCDDrghyYyfjIntss3t8aK+yYUaeytqHW2wt7gx3tqxr8FUfGi+/1xXVLgeE+MHOprBZ/Cc6X7w6quvYvTo0QgJCUFmZib27ds30CERERERUYDQaTWIjwy54tUUuxOs0yI+MgTpSWb89Forfp4xEmkjIwdVoUU/3rA52x988AGWL1+OgoICfP3110hPT0dubi5qa7t+uR8REREREdGPNWzeRvjiiy9iyZIlWLx4MQDg9ddfxyeffIJ169bh97///QBHdwUaKoDqbwY6ikFD4/EgwXYAmuNeQOefL2IcOrouLdudjhx/Dc1xzyU5vty/CvZt3B726KNxA9vl80z9hTn2D+bZ95hj/2Ce/aNLnkffDIRF9/7EADEsii2Xy4UDBw5g5cqVSptWq0V2djaKi4u79G9ra0NbW5vy2G63A+j4IKTb3Q9fqPD9WBf/7ivt/z6Hbsvv+iWG4SAIwDQAqBjgQIYw5tg/mGffY479g3n2PebYP5hn/7g0z+2LtkBGTB3AiK7s+n1YFFt1dXXweDywWq2qdqvViuPHj3fpv3r1aqxatapL++eff46wsLB+jW3r1q1X1D/e9h1Sw8f30ENw+X/5JyIiIiIavA7t+RqO0IH9GJDT6exz32FRbF2plStXYvny5cpju92OpKQk5OTkwGQy9cs+3G43tm7dip/+9KdX+J0BtwH4U7/EMBxcfZ6pr5hj/2CefY859g/m2feYY/9gnv3j0jzfPNAB4Yd3vfXFsCi2LBYLdDodampqVO01NTWIj++6hqXBYIDBYOjSHhwc3O//MfliTOqKefY95tg/mGffY479g3n2PebYP5hn/wikPF9JHMNiNUK9Xo+MjAxs375dafN6vdi+fTuysrIGMDIiIiIiIhqqhsWdLQBYvnw57r33XkydOhXTpk3Dyy+/jObmZmV1QiIiIiIiov40bIqt+fPn4/z583j66adRXV2NG264AVu2bOmyaAYREREREVF/GDbFFgAsW7YMy5YtG+gwiIiIiIhoGBgWn9kiIiIiIiLyNxZbREREREREPsBii4iIiIiIyAdYbBEREREREfkAiy0iIiIiIiIfYLFFRERERETkAyy2iIiIiIiIfIDFFhERERERkQ+w2CIiIiIiIvIBFltEREREREQ+wGKLiIiIiIjIB1hsERERERER+QCLLSIiIiIiIh8IGugABgMRAQDY7fZ+G9PtdsPpdMJutyM4OLjfxiU15tn3mGP/YJ59jzn2D+bZ95hj/2Ce/SMQ89xZE3TWCD1hsdUHDocDAJCUlDTAkRARERERUSBwOByIjIzssY9G+lKSDXNerxfnzp1DREQENBpNv4xpt9uRlJSEM2fOwGQy9cuY1BXz7HvMsX8wz77HHPsH8+x7zLF/MM/+EYh5FhE4HA4kJiZCq+35U1m8s9UHWq0WI0eO9MnYJpMpYCbOUMY8+x5z7B/Ms+8xx/7BPPsec+wfzLN/BFqee7uj1YkLZBAREREREfkAiy0iIiIiIiIfYLE1QAwGAwoKCmAwGAY6lCGNefY95tg/mGffY479g3n2PebYP5hn/xjseeYCGURERERERD7AO1tEREREREQ+wGKLiIiIiIjIB1hsERERERER+QCLLSIiIiIiIh9gsTUAXn31VYwePRohISHIzMzEvn37BjqkgLF69Wr85Cc/QUREBOLi4nDXXXehrKxM1WfmzJnQaDSqn4ceekjVp7KyErfffjvCwsIQFxeHFStWoL29XdWnsLAQU6ZMgcFgQGpqKtavX98lnqF4rp555pku+Zs4caKyvbW1FUuXLkVMTAyMRiN+/vOfo6amRjUG89u70aNHd8mzRqPB0qVLAXAeX62dO3fijjvuQGJiIjQaDTZt2qTaLiJ4+umnkZCQgNDQUGRnZ+PEiROqPg0NDVi4cCFMJhPMZjPuv/9+NDU1qfocPnwYN998M0JCQpCUlIQXXnihSywfffQRJk6ciJCQEKSlpWHz5s1XHEsg6inHbrcb+fn5SEtLQ3h4OBITE3HPPffg3LlzqjG6m/9r1qxR9RnOOQZ6n8uLFi3qksO8vDxVH87lnvWW4+5eozUaDdauXav04VzuWV+u2wLpuqIvsfQ7Ib96//33Ra/Xy7p16+TIkSOyZMkSMZvNUlNTM9ChBYTc3Fx56623pLS0VEpKSuS2226TUaNGSVNTk9LnlltukSVLlkhVVZXy09jYqGxvb2+X66+/XrKzs+XgwYOyefNmsVgssnLlSqXPt99+K2FhYbJ8+XI5evSovPLKK6LT6WTLli1Kn6F6rgoKCuS6665T5e/8+fPK9oceekiSkpJk+/btsn//frnxxhtl+vTpynbmt29qa2tVOd66dasAkB07dogI5/HV2rx5szz11FOyYcMGASAbN25UbV+zZo1ERkbKpk2b5NChQ3LnnXdKSkqKtLS0KH3y8vIkPT1d9uzZI0VFRZKamioLFixQtjc2NorVapWFCxdKaWmpvPfeexIaGipvvPGG0mfXrl2i0+nkhRdekKNHj8of//hHCQ4Olm+++eaKYglEPeXYZrNJdna2fPDBB3L8+HEpLi6WadOmSUZGhmqM5ORkefbZZ1Xz++LX8eGeY5He5/K9994reXl5qhw2NDSo+nAu96y3HF+c26qqKlm3bp1oNBopLy9X+nAu96wv122BdF3RWyy+wGLLz6ZNmyZLly5VHns8HklMTJTVq1cPYFSBq7a2VgDIf//7X6Xtlltukccee+yyz9m8ebNotVqprq5W2l577TUxmUzS1tYmIiK/+93v5LrrrlM9b/78+ZKbm6s8HqrnqqCgQNLT07vdZrPZJDg4WD766COl7dixYwJAiouLRYT5vVqPPfaYjB07Vrxer4hwHveHSy+evF6vxMfHy9q1a5U2m80mBoNB3nvvPREROXr0qACQr776Sunz6aefikajke+++05ERP7+979LVFSUkmcRkfz8fJkwYYLyeN68eXL77ber4snMzJQHH3ywz7EMBt1doF5q3759AkBOnz6ttCUnJ8tLL7102ecwx2qXK7bmzJlz2edwLl+ZvszlOXPmyK233qpq41y+MpdetwXSdUVfYvEFvo3Qj1wuFw4cOIDs7GylTavVIjs7G8XFxQMYWeBqbGwEAERHR6va3333XVgsFlx//fVYuXIlnE6nsq24uBhpaWmwWq1KW25uLux2O44cOaL0ufg8dPbpPA9D/VydOHECiYmJGDNmDBYuXIjKykoAwIEDB+B2u1XHPXHiRIwaNUo5bub3yrlcLrzzzju47777oNFolHbO4/5VUVGB6upq1fFGRkYiMzNTNX/NZjOmTp2q9MnOzoZWq8XevXuVPjNmzIBer1f65ObmoqysDBcuXFD69JT7vsQyVDQ2NkKj0cBsNqva16xZg5iYGEyePBlr165VvSWIOe6bwsJCxMXFYcKECXj44YdRX1+vbONc7l81NTX45JNPcP/993fZxrncd5detwXSdUVfYvGFIJ+NTF3U1dXB4/GoJhMAWK1WHD9+fICiClxerxePP/44brrpJlx//fVK+69//WskJycjMTERhw8fRn5+PsrKyrBhwwYAQHV1dbc57tzWUx+73Y6WlhZcuHBhyJ6rzMxMrF+/HhMmTEBVVRVWrVqFm2++GaWlpaiuroZer+9y0WS1WnvNXee2nvoMh/x2Z9OmTbDZbFi0aJHSxnnc/zrz0t3xXpyzuLg41fagoCBER0er+qSkpHQZo3NbVFTUZXN/8Ri9xTIUtLa2Ij8/HwsWLIDJZFLaH330UUyZMgXR0dHYvXs3Vq5ciaqqKrz44osAmOO+yMvLw9y5c5GSkoLy8nL84Q9/wOzZs1FcXAydTse53M/efvttREREYO7cuap2zuW+6+66LZCuK/oSiy+w2KKAtXTpUpSWluLLL79UtT/wwAPK32lpaUhISMCsWbNQXl6OsWPH+jvMQWf27NnK35MmTUJmZiaSk5Px4YcfIjQ0dAAjG7refPNNzJ49G4mJiUob5zENdm63G/PmzYOI4LXXXlNtW758ufL3pEmToNfr8eCDD2L16tUwGAz+DnVQ+tWvfqX8nZaWhkmTJmHs2LEoLCzErFmzBjCyoWndunVYuHAhQkJCVO2cy313ueu24Y5vI/Qji8UCnU7XZdWTmpoaxMfHD1BUgWnZsmX4z3/+gx07dmDkyJE99s3MzAQAnDx5EgAQHx/fbY47t/XUx2QyITQ0dFidK7PZjPHjx+PkyZOIj4+Hy+WCzWZT9bn4uJnfK3P69Gls27YNv/nNb3rsx3n843UeU0/HGx8fj9raWtX29vZ2NDQ09Mscv3h7b7EMZp2F1unTp7F161bVXa3uZGZmor29HadOnQLAHF+NMWPGwGKxqF4jOJf7R1FREcrKynp9nQY4ly/nctdtgXRd0ZdYfIHFlh/p9XpkZGRg+/btSpvX68X27duRlZU1gJEFDhHBsmXLsHHjRnzxxRddbs13p6SkBACQkJAAAMjKysI333yj+p9Q58XAtddeq/S5+Dx09uk8D8PpXDU1NaG8vBwJCQnIyMhAcHCw6rjLyspQWVmpHDfze2XeeustxMXF4fbbb++xH+fxj5eSkoL4+HjV8drtduzdu1c1f202Gw4cOKD0+eKLL+D1epWCNysrCzt37oTb7Vb6bN26FRMmTEBUVJTSp6fc9yWWwaqz0Dpx4gS2bduGmJiYXp9TUlICrVarvO2NOb5yZ8+eRX19veo1gnO5f7z55pvIyMhAenp6r305l9V6u24LpOuKvsTiEz5beoO69f7774vBYJD169fL0aNH5YEHHhCz2axagWU4e/jhhyUyMlIKCwtVy6w6nU4RETl58qQ8++yzsn//fqmoqJCPP/5YxowZIzNmzFDG6FxCNCcnR0pKSmTLli0SGxvb7RKiK1askGPHjsmrr77a7RKiQ/FcPfHEE1JYWCgVFRWya9cuyc7OFovFIrW1tSLSsSzqqFGj5IsvvpD9+/dLVlaWZGVlKc9nfvvO4/HIqFGjJD8/X9XOeXz1HA6HHDx4UA4ePCgA5MUXX5SDBw8qK+GtWbNGzGazfPzxx3L48GGZM2dOt0u/T548Wfbu3StffvmljBs3TrVcts1mE6vVKnfffbeUlpbK+++/L2FhYV2Wcg4KCpK//OUvcuzYMSkoKOh2KefeYglEPeXY5XLJnXfeKSNHjpSSkhLV63TnqmG7d++Wl156SUpKSqS8vFzeeecdiY2NlXvuuUfZx3DPsUjPeXY4HPLkk09KcXGxVFRUyLZt22TKlCkybtw4aW1tVcbgXO5Zb68XIh1Lt4eFhclrr73W5fmcy73r7bpNJLCuK3qLxRdYbA2AV155RUaNGiV6vV6mTZsme/bsGeiQAgaAbn/eeustERGprKyUGTNmSHR0tBgMBklNTZUVK1aovp9IROTUqVMye/ZsCQ0NFYvFIk888YS43W5Vnx07dsgNN9wger1exowZo+zjYkPxXM2fP18SEhJEr9fLiBEjZP78+XLy5Elle0tLi/z2t7+VqKgoCQsLk5/97GdSVVWlGoP57ZvPPvtMAEhZWZmqnfP46u3YsaPb14h7771XRDqWUP7Tn/4kVqtVDAaDzJo1q0v+6+vrZcGCBWI0GsVkMsnixYvF4XCo+hw6dEj+7//+TwwGg4wYMULWrFnTJZYPP/xQxo8fL3q9Xq677jr55JNPVNv7Eksg6inHFRUVl32d7vwOuQMHDkhmZqZERkZKSEiIXHPNNfL888+rigSR4Z1jkZ7z7HQ6JScnR2JjYyU4OFiSk5NlyZIlXf6RhHO5Z729XoiIvPHGGxIaGio2m63L8zmXe9fbdZtIYF1X9CWW/qYREfHRTTMiIiIiIqJhi5/ZIiIiIiIi8gEWW0RERERERD7AYouIiIiIiMgHWGwRERERERH5AIstIiIiIiIiH2CxRURERERE5AMstoiIiIiIiHyAxRYREREREZEPsNgiIqJBb9GiRbjrrrsGOgwiIiKVoIEOgIiIqCcajabH7QUFBfjb3/4GEfFTRN1btGgRbDYbNm3aNKBxEBFR4GCxRUREAa2qqkr5+4MPPsDTTz+NsrIypc1oNMJoNA5EaERERD3i2wiJiCigxcfHKz+RkZHQaDSqNqPR2OVthDNnzsQjjzyCxx9/HFFRUbBarfjHP/6B5uZmLF68GBEREUhNTcWnn36q2ldpaSlmz54No9EIq9WKu+++G3V1dcr2f/3rX0hLS0NoaChiYmKQnZ2N5uZmPPPMM3j77bfx8ccfQ6PRQKPRoLCwEABw5swZzJs3D2azGdHR0ZgzZw5OnTqljNkZ+6pVqxAbGwuTyYSHHnoILper1/0SEVFgY7FFRERD0ttvvw2LxYJ9+/bhkUcewcMPP4xf/vKXmD59Or7++mvk5OTg7rvvhtPpBADYbDbceuutmDx5Mvbv348tW7agpqYG8+bNA9Bxh23BggW47777cOzYMRQWFmLu3LkQETz55JOYN28e8vLyUFVVhaqqKkyfPh1utxu5ubmIiIhAUVERdu3aBaPRiLy8PFUxtX37dmXM9957Dxs2bMCqVat63S8REQU2jfDVmoiIBon169fj8ccfh81mU7Vf+nmpmTNnwuPxoKioCADg8XgQGRmJuXPn4p///CcAoLq6GgkJCSguLsaNN96I5557DkVFRfjss8+Ucc+ePYukpCSUlZWhqakJGRkZOHXqFJKTk7vE1t1ntt555x0899xzOHbsmPLZM5fLBbPZjE2bNiEnJweLFi3Cv//9b5w5cwZhYWEAgNdffx0rVqxAY2MjSkpKetwvEREFLn5mi4iIhqRJkyYpf+t0OsTExCAtLU1ps1qtAIDa2loAwKFDh7Bjx45uP/9VXl6OnJwczJo1C2lpacjNzUVOTg5+8YtfICoq6rIxHDp0CCdPnkRERISqvbW1FeXl5crj9PR0pdACgKysLDQ1NeHMmTNIT0+/4v0SEVFgYLFFRERDUnBwsOqxRqNRtXXeafJ6vQCApqYm3HHHHfjzn//cZayEhATodDps3boVu3fvxueff45XXnkFTz31FPbu3YuUlJRuY+i8G/buu+922RYbG9un47ia/RIRUWDgZ7aIiIgATJkyBUeOHMHo0aORmpqq+gkPDwfQUaDddNNNWLVqFQ4ePAi9Xo+NGzcCAPR6PTweT5cxT5w4gbi4uC5jRkZGKv0OHTqElpYW5fGePXtgNBqRlJTU636JiChwsdgiIiICsHTpUjQ0NGDBggX46quvUF5ejs8++wyLFy+Gx+PB3r178fzzz2P//v2orKzEhg0bcP78eVxzzTUAgNGjR+Pw4cMoKytDXV0d3G43Fi5cCIvFgjlz5qCoqAgVFRUoLCzEo48+irNnzyr7drlcuP/++3H06FFs3rwZBQUFWLZsGbRaba/7JSKiwMW3ERIREQFITEzErl27kJ+fj5ycHLS1tSE5ORl5eXnQarUwmUzYuXMnXn75ZdjtdiQnJ+Ovf/0rZs+eDQBYsmQJCgsLMXXqVDQ1NWHHjh2YOXMmdu7cifz8fMydOxcOhwMjRozArFmzYDKZlH3PmjUL48aNw4wZM9DW1oYFCxbgmWeeAYBe90tERIGLqxESERENoO5WMSQioqGBbyMkIiIiIiLyARZbREREREREPsC3ERIREREREfkA72wRERERERH5AIstIiIiIiIiH2CxRURERERE5AMstoiIiIiIiHyAxRYREREREZEPsNgiIiIiIiLyARZbREREREREPsBii4iIiIiIyAf+H5TgQ73vl9k1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV 로그 파일 경로\n",
    "log_csv_path = \"./logs/progress.csv\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(log_csv_path)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data[\"time/total_timesteps\"], data[\"train/value_loss\"], label=\"Value Loss\")\n",
    "plt.plot(data[\"time/total_timesteps\"], data[\"train/policy_gradient_loss\"], label=\"Policy Gradient Loss\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Timesteps\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ef01c8-4750-48b4-a145-9e30057527ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m Monitor(SmartFactoryEnv(n_machines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, energy_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m), filename\u001b[38;5;241m=\u001b[39mlog_dir)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 학습된 모델 평가\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:72\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf_infos\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.8/copy.py:203\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    201\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    202\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m--> 203\u001b[0m append \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m    205\u001b[0m     append(deepcopy(a, memo))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# 평가 환경 생성 및 Monitor 래핑\n",
    "eval_env = Monitor(SmartFactoryEnv(n_machines=5, energy_limit=10), filename=log_dir)\n",
    "\n",
    "# 학습된 모델 평가\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, render=False)\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16039a-a6c7-4cee-a906-fe9aad6dfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보상 시각화\n",
    "rewards = []\n",
    "obs = eval_env.reset()\n",
    "done = False\n",
    "for _ in range(1000):  # 최대 1000 스텝 동안 평가\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = eval_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards, label=\"Reward\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Rewards Over Steps\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea69aa-ff8e-4254-8ba9-8263f196a7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3626d2f-9f38-45d9-86a0-5286c76ca3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75b050-4b4d-4438-8cba-0b4c74683f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
