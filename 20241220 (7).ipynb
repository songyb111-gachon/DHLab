{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3bb6ba70918dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T07:50:49.917672Z",
     "start_time": "2024-12-17T07:50:49.704223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kornia/feature/lightglue.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1024, 1024])\n",
      "Using cuda device\n",
      "Initial MSE: 0.002917, Initial PSNR: 25.350521, 10:02:00\n",
      "Logging to ./ppo_with_mask/PPO_50\n",
      "Executing reset logic for the first step\n",
      "Initial MSE: 0.002917, Initial PSNR: 25.350521, 2024-12-20_10-01-21\n",
      "Step: 1, MSE: 0.069600, PSNR: 11.573917, 10:02:01\n",
      "Step: 2, MSE: 0.069512, PSNR: 11.579404, 10:02:01\n",
      "Step: 3, MSE: 0.069513, PSNR: 11.579318, 10:02:02\n",
      "Step: 4, MSE: 0.069591, PSNR: 11.574482, 10:02:02\n",
      "Step: 5, MSE: 0.069421, PSNR: 11.585088, 10:02:02\n",
      "Step: 6, MSE: 0.069565, PSNR: 11.576078, 10:02:03\n",
      "Step: 7, MSE: 0.069640, PSNR: 11.571426, 10:02:03\n",
      "Step: 8, MSE: 0.069479, PSNR: 11.581439, 10:02:03\n",
      "Step: 9, MSE: 0.069484, PSNR: 11.581178, 10:02:04\n",
      "Step: 10, MSE: 0.069743, PSNR: 11.564980, 10:02:04\n",
      "Step: 11, MSE: 0.069462, PSNR: 11.582527, 10:02:04\n",
      "Step: 12, MSE: 0.069372, PSNR: 11.588146, 10:02:05\n",
      "Step: 13, MSE: 0.069626, PSNR: 11.572300, 10:02:05\n",
      "Step: 14, MSE: 0.069520, PSNR: 11.578918, 10:02:06\n",
      "Step: 15, MSE: 0.069313, PSNR: 11.591844, 10:02:06\n",
      "Step: 16, MSE: 0.069619, PSNR: 11.572695, 10:02:06\n",
      "Step: 17, MSE: 0.069536, PSNR: 11.577911, 10:02:07\n",
      "Step: 18, MSE: 0.069450, PSNR: 11.583256, 10:02:07\n",
      "Step: 19, MSE: 0.069605, PSNR: 11.573610, 10:02:07\n",
      "Step: 20, MSE: 0.069596, PSNR: 11.574179, 10:02:08\n",
      "Step: 21, MSE: 0.069630, PSNR: 11.572035, 10:02:08\n",
      "Step: 22, MSE: 0.069378, PSNR: 11.587781, 10:02:08\n",
      "Step: 23, MSE: 0.069676, PSNR: 11.569164, 10:02:09\n",
      "Step: 24, MSE: 0.069590, PSNR: 11.574524, 10:02:09\n",
      "Step: 25, MSE: 0.069582, PSNR: 11.575014, 10:02:09\n",
      "Step: 26, MSE: 0.069584, PSNR: 11.574888, 10:02:10\n",
      "Step: 27, MSE: 0.069591, PSNR: 11.574448, 10:02:10\n",
      "Step: 28, MSE: 0.069427, PSNR: 11.584696, 10:02:11\n",
      "Step: 29, MSE: 0.069643, PSNR: 11.571246, 10:02:11\n",
      "Step: 30, MSE: 0.069717, PSNR: 11.566596, 10:02:11\n",
      "Step: 31, MSE: 0.069522, PSNR: 11.578798, 10:02:12\n",
      "Step: 32, MSE: 0.069668, PSNR: 11.569693, 10:02:12\n",
      "Step: 33, MSE: 0.069507, PSNR: 11.579743, 10:02:12\n",
      "Step: 34, MSE: 0.069746, PSNR: 11.564791, 10:02:13\n",
      "Step: 35, MSE: 0.069394, PSNR: 11.586777, 10:02:13\n",
      "Step: 36, MSE: 0.069520, PSNR: 11.578920, 10:02:13\n",
      "Step: 37, MSE: 0.069482, PSNR: 11.581306, 10:02:14\n",
      "Step: 38, MSE: 0.069513, PSNR: 11.579342, 10:02:14\n",
      "Step: 39, MSE: 0.069554, PSNR: 11.576759, 10:02:14\n",
      "Step: 40, MSE: 0.069492, PSNR: 11.580657, 10:02:15\n",
      "Step: 41, MSE: 0.069484, PSNR: 11.581152, 10:02:15\n",
      "Step: 42, MSE: 0.069485, PSNR: 11.581079, 10:02:15\n",
      "Step: 43, MSE: 0.069663, PSNR: 11.569992, 10:02:16\n",
      "Step: 44, MSE: 0.069502, PSNR: 11.580021, 10:02:16\n",
      "Step: 45, MSE: 0.069512, PSNR: 11.579376, 10:02:16\n",
      "Step: 46, MSE: 0.069437, PSNR: 11.584111, 10:02:17\n",
      "Step: 47, MSE: 0.069414, PSNR: 11.585526, 10:02:17\n",
      "Step: 48, MSE: 0.069587, PSNR: 11.574739, 10:02:18\n",
      "Step: 49, MSE: 0.069554, PSNR: 11.576781, 10:02:18\n",
      "Step: 50, MSE: 0.069370, PSNR: 11.588298, 10:02:18\n",
      "Step: 51, MSE: 0.069590, PSNR: 11.574535, 10:02:19\n",
      "Step: 52, MSE: 0.069482, PSNR: 11.581263, 10:02:19\n",
      "Step: 53, MSE: 0.069547, PSNR: 11.577217, 10:02:19\n",
      "Step: 54, MSE: 0.069698, PSNR: 11.567772, 10:02:20\n",
      "Step: 55, MSE: 0.069547, PSNR: 11.577226, 10:02:20\n",
      "Step: 56, MSE: 0.069538, PSNR: 11.577784, 10:02:20\n",
      "Step: 57, MSE: 0.069459, PSNR: 11.582705, 10:02:21\n",
      "Step: 58, MSE: 0.069542, PSNR: 11.577516, 10:02:21\n",
      "Step: 59, MSE: 0.069620, PSNR: 11.572633, 10:02:21\n",
      "Step: 60, MSE: 0.069445, PSNR: 11.583585, 10:02:22\n",
      "Step: 61, MSE: 0.069493, PSNR: 11.580597, 10:02:22\n",
      "Step: 62, MSE: 0.069504, PSNR: 11.579899, 10:02:23\n",
      "Step: 63, MSE: 0.069414, PSNR: 11.585513, 10:02:23\n",
      "Step: 64, MSE: 0.069483, PSNR: 11.581190, 10:02:23\n",
      "Step: 65, MSE: 0.069444, PSNR: 11.583649, 10:02:24\n",
      "Step: 66, MSE: 0.069495, PSNR: 11.580443, 10:02:24\n",
      "Step: 67, MSE: 0.069410, PSNR: 11.585774, 10:02:24\n",
      "Step: 68, MSE: 0.069569, PSNR: 11.575830, 10:02:25\n",
      "Step: 69, MSE: 0.069522, PSNR: 11.578772, 10:02:25\n",
      "Step: 70, MSE: 0.069634, PSNR: 11.571759, 10:02:25\n",
      "Step: 71, MSE: 0.069499, PSNR: 11.580214, 10:02:26\n",
      "Step: 72, MSE: 0.069667, PSNR: 11.569724, 10:02:26\n",
      "Step: 73, MSE: 0.069568, PSNR: 11.575932, 10:02:26\n",
      "Step: 74, MSE: 0.069521, PSNR: 11.578818, 10:02:27\n",
      "Step: 75, MSE: 0.069541, PSNR: 11.577563, 10:02:27\n",
      "Step: 76, MSE: 0.069536, PSNR: 11.577897, 10:02:27\n",
      "Step: 77, MSE: 0.069501, PSNR: 11.580069, 10:02:28\n",
      "Step: 78, MSE: 0.069676, PSNR: 11.569153, 10:02:28\n",
      "Step: 79, MSE: 0.069510, PSNR: 11.579557, 10:02:29\n",
      "Step: 80, MSE: 0.069287, PSNR: 11.593471, 10:02:29\n",
      "Step: 81, MSE: 0.069751, PSNR: 11.564503, 10:02:29\n",
      "Step: 82, MSE: 0.069515, PSNR: 11.579213, 10:02:30\n",
      "Step: 83, MSE: 0.069636, PSNR: 11.571650, 10:02:30\n",
      "Step: 84, MSE: 0.069546, PSNR: 11.577295, 10:02:30\n",
      "Step: 85, MSE: 0.069483, PSNR: 11.581187, 10:02:31\n",
      "Step: 86, MSE: 0.069531, PSNR: 11.578218, 10:02:31\n",
      "Step: 87, MSE: 0.069551, PSNR: 11.576979, 10:02:31\n",
      "Step: 88, MSE: 0.069642, PSNR: 11.571320, 10:02:32\n",
      "Step: 89, MSE: 0.069652, PSNR: 11.570675, 10:02:32\n",
      "Step: 90, MSE: 0.069545, PSNR: 11.577360, 10:02:33\n",
      "Step: 91, MSE: 0.069560, PSNR: 11.576378, 10:02:33\n",
      "Step: 92, MSE: 0.069702, PSNR: 11.567564, 10:02:33\n",
      "Step: 93, MSE: 0.069529, PSNR: 11.578314, 10:02:34\n",
      "Step: 94, MSE: 0.069430, PSNR: 11.584554, 10:02:34\n",
      "Step: 95, MSE: 0.069240, PSNR: 11.596443, 10:02:34\n",
      "Step: 96, MSE: 0.069499, PSNR: 11.580217, 10:02:35\n",
      "Step: 97, MSE: 0.069649, PSNR: 11.570882, 10:02:35\n",
      "Step: 98, MSE: 0.069635, PSNR: 11.571714, 10:02:35\n",
      "Step: 99, MSE: 0.069651, PSNR: 11.570757, 10:02:36\n",
      "Step: 100, MSE: 0.069660, PSNR: 11.570138, 10:02:36\n",
      "Step: 101, MSE: 0.069492, PSNR: 11.580640, 10:02:36\n",
      "Step: 102, MSE: 0.069523, PSNR: 11.578711, 10:02:37\n",
      "Step: 103, MSE: 0.069610, PSNR: 11.573300, 10:02:37\n",
      "Step: 104, MSE: 0.069473, PSNR: 11.581841, 10:02:37\n",
      "Step: 105, MSE: 0.069533, PSNR: 11.578066, 10:02:38\n",
      "Step: 106, MSE: 0.069611, PSNR: 11.573227, 10:02:38\n",
      "Step: 107, MSE: 0.069586, PSNR: 11.574780, 10:02:39\n",
      "Step: 108, MSE: 0.069465, PSNR: 11.582346, 10:02:39\n",
      "Step: 109, MSE: 0.069567, PSNR: 11.575994, 10:02:39\n",
      "Step: 110, MSE: 0.069525, PSNR: 11.578577, 10:02:40\n",
      "Step: 111, MSE: 0.069512, PSNR: 11.579404, 10:02:40\n",
      "Step: 112, MSE: 0.069673, PSNR: 11.569340, 10:02:40\n",
      "Step: 113, MSE: 0.069539, PSNR: 11.577713, 10:02:41\n",
      "Step: 114, MSE: 0.069583, PSNR: 11.574964, 10:02:41\n",
      "Step: 115, MSE: 0.069591, PSNR: 11.574459, 10:02:41\n",
      "Step: 116, MSE: 0.069517, PSNR: 11.579062, 10:02:42\n",
      "Step: 117, MSE: 0.069430, PSNR: 11.584536, 10:02:42\n",
      "Step: 118, MSE: 0.069531, PSNR: 11.578209, 10:02:43\n",
      "Step: 119, MSE: 0.069628, PSNR: 11.572165, 10:02:43\n",
      "Step: 120, MSE: 0.069612, PSNR: 11.573134, 10:02:43\n",
      "Step: 121, MSE: 0.069504, PSNR: 11.579875, 10:02:44\n",
      "Step: 122, MSE: 0.069472, PSNR: 11.581926, 10:02:44\n",
      "Step: 123, MSE: 0.069554, PSNR: 11.576809, 10:02:44\n",
      "Step: 124, MSE: 0.069628, PSNR: 11.572160, 10:02:45\n",
      "Step: 125, MSE: 0.069651, PSNR: 11.570745, 10:02:45\n",
      "Step: 126, MSE: 0.069595, PSNR: 11.574215, 10:02:45\n",
      "Step: 127, MSE: 0.069616, PSNR: 11.572923, 10:02:46\n",
      "Step: 128, MSE: 0.069578, PSNR: 11.575258, 10:02:46\n",
      "Step: 129, MSE: 0.069650, PSNR: 11.570778, 10:02:46\n",
      "Step: 130, MSE: 0.069522, PSNR: 11.578762, 10:02:47\n",
      "Step: 131, MSE: 0.069519, PSNR: 11.578989, 10:02:47\n",
      "Step: 132, MSE: 0.069677, PSNR: 11.569086, 10:02:47\n",
      "Step: 133, MSE: 0.069455, PSNR: 11.582968, 10:02:48\n",
      "Step: 134, MSE: 0.069712, PSNR: 11.566916, 10:02:48\n",
      "Step: 135, MSE: 0.069447, PSNR: 11.583476, 10:02:49\n",
      "Step: 136, MSE: 0.069707, PSNR: 11.567231, 10:02:49\n",
      "Step: 137, MSE: 0.069509, PSNR: 11.579595, 10:02:49\n",
      "Step: 138, MSE: 0.069507, PSNR: 11.579741, 10:02:50\n",
      "Step: 139, MSE: 0.069564, PSNR: 11.576133, 10:02:50\n",
      "Step: 140, MSE: 0.069672, PSNR: 11.569420, 10:02:50\n",
      "Step: 141, MSE: 0.069558, PSNR: 11.576530, 10:02:51\n",
      "Step: 142, MSE: 0.069434, PSNR: 11.584297, 10:02:51\n",
      "Step: 143, MSE: 0.069520, PSNR: 11.578904, 10:02:51\n",
      "Step: 144, MSE: 0.069537, PSNR: 11.577826, 10:02:52\n",
      "Step: 145, MSE: 0.069569, PSNR: 11.575863, 10:02:52\n",
      "Step: 146, MSE: 0.069613, PSNR: 11.573091, 10:02:52\n",
      "Step: 147, MSE: 0.069598, PSNR: 11.574049, 10:02:53\n",
      "Step: 148, MSE: 0.069422, PSNR: 11.585006, 10:02:53\n",
      "Step: 149, MSE: 0.069652, PSNR: 11.570636, 10:02:54\n",
      "Step: 150, MSE: 0.069462, PSNR: 11.582541, 10:02:54\n",
      "Step: 151, MSE: 0.069521, PSNR: 11.578810, 10:02:54\n",
      "Step: 152, MSE: 0.069456, PSNR: 11.582907, 10:02:55\n",
      "Step: 153, MSE: 0.069639, PSNR: 11.571474, 10:02:55\n",
      "Step: 154, MSE: 0.069373, PSNR: 11.588111, 10:02:55\n",
      "Step: 155, MSE: 0.069510, PSNR: 11.579518, 10:02:56\n",
      "Step: 156, MSE: 0.069629, PSNR: 11.572128, 10:02:56\n",
      "Step: 157, MSE: 0.069346, PSNR: 11.589798, 10:02:56\n",
      "Step: 158, MSE: 0.069634, PSNR: 11.571812, 10:02:57\n",
      "Step: 159, MSE: 0.069629, PSNR: 11.572115, 10:02:57\n",
      "Step: 160, MSE: 0.069651, PSNR: 11.570749, 10:02:58\n",
      "Step: 161, MSE: 0.069615, PSNR: 11.572971, 10:02:58\n",
      "Step: 162, MSE: 0.069685, PSNR: 11.568595, 10:02:58\n",
      "Step: 163, MSE: 0.069532, PSNR: 11.578141, 10:02:59\n",
      "Step: 164, MSE: 0.069539, PSNR: 11.577719, 10:02:59\n",
      "Step: 165, MSE: 0.069284, PSNR: 11.593653, 10:02:59\n",
      "Step: 166, MSE: 0.069549, PSNR: 11.577120, 10:03:00\n",
      "Step: 167, MSE: 0.069438, PSNR: 11.584034, 10:03:00\n",
      "Step: 168, MSE: 0.069564, PSNR: 11.576156, 10:03:00\n",
      "Step: 169, MSE: 0.069729, PSNR: 11.565875, 10:03:01\n",
      "Step: 170, MSE: 0.069606, PSNR: 11.573525, 10:03:01\n",
      "Step: 171, MSE: 0.069526, PSNR: 11.578510, 10:03:01\n",
      "Step: 172, MSE: 0.069513, PSNR: 11.579351, 10:03:02\n",
      "Step: 173, MSE: 0.069557, PSNR: 11.576562, 10:03:02\n",
      "Step: 174, MSE: 0.069725, PSNR: 11.566086, 10:03:02\n",
      "Step: 175, MSE: 0.069689, PSNR: 11.568333, 10:03:03\n",
      "Step: 176, MSE: 0.069728, PSNR: 11.565933, 10:03:03\n",
      "Step: 177, MSE: 0.069540, PSNR: 11.577650, 10:03:03\n",
      "Step: 178, MSE: 0.069547, PSNR: 11.577213, 10:03:04\n",
      "Step: 179, MSE: 0.069466, PSNR: 11.582294, 10:03:04\n",
      "Step: 180, MSE: 0.069499, PSNR: 11.580187, 10:03:05\n",
      "Step: 181, MSE: 0.069603, PSNR: 11.573720, 10:03:05\n",
      "Step: 182, MSE: 0.069640, PSNR: 11.571441, 10:03:05\n",
      "Step: 183, MSE: 0.069492, PSNR: 11.580621, 10:03:06\n",
      "Step: 184, MSE: 0.069535, PSNR: 11.577936, 10:03:06\n",
      "Step: 185, MSE: 0.069561, PSNR: 11.576372, 10:03:06\n",
      "Step: 186, MSE: 0.069619, PSNR: 11.572725, 10:03:07\n",
      "Step: 187, MSE: 0.069576, PSNR: 11.575392, 10:03:07\n",
      "Step: 188, MSE: 0.069552, PSNR: 11.576876, 10:03:07\n",
      "Step: 189, MSE: 0.069485, PSNR: 11.581069, 10:03:08\n",
      "Step: 190, MSE: 0.069647, PSNR: 11.571006, 10:03:08\n",
      "Step: 191, MSE: 0.069488, PSNR: 11.580901, 10:03:08\n",
      "Step: 192, MSE: 0.069429, PSNR: 11.584595, 10:03:09\n",
      "Step: 193, MSE: 0.069633, PSNR: 11.571825, 10:03:09\n",
      "Step: 194, MSE: 0.069493, PSNR: 11.580612, 10:03:09\n",
      "Step: 195, MSE: 0.069650, PSNR: 11.570803, 10:03:10\n",
      "Step: 196, MSE: 0.069562, PSNR: 11.576277, 10:03:10\n",
      "Step: 197, MSE: 0.069409, PSNR: 11.585837, 10:03:10\n",
      "Step: 198, MSE: 0.069438, PSNR: 11.584038, 10:03:11\n",
      "Step: 199, MSE: 0.069528, PSNR: 11.578403, 10:03:11\n",
      "Step: 200, MSE: 0.069591, PSNR: 11.574467, 10:03:12\n",
      "Step: 201, MSE: 0.069749, PSNR: 11.564648, 10:03:12\n",
      "Step: 202, MSE: 0.069446, PSNR: 11.583500, 10:03:12\n",
      "Step: 203, MSE: 0.069566, PSNR: 11.576031, 10:03:13\n",
      "Step: 204, MSE: 0.069561, PSNR: 11.576370, 10:03:13\n",
      "Step: 205, MSE: 0.069528, PSNR: 11.578378, 10:03:13\n",
      "Step: 206, MSE: 0.069714, PSNR: 11.566832, 10:03:14\n",
      "Step: 207, MSE: 0.069361, PSNR: 11.588849, 10:03:14\n",
      "Step: 208, MSE: 0.069637, PSNR: 11.571589, 10:03:14\n",
      "Step: 209, MSE: 0.069512, PSNR: 11.579384, 10:03:15\n",
      "Step: 210, MSE: 0.069711, PSNR: 11.567002, 10:03:15\n",
      "Step: 211, MSE: 0.069472, PSNR: 11.581890, 10:03:15\n",
      "Step: 212, MSE: 0.069599, PSNR: 11.573942, 10:03:16\n",
      "Step: 213, MSE: 0.069513, PSNR: 11.579357, 10:03:16\n",
      "Step: 214, MSE: 0.069486, PSNR: 11.581002, 10:03:17\n",
      "Step: 215, MSE: 0.069562, PSNR: 11.576271, 10:03:17\n",
      "Step: 216, MSE: 0.069466, PSNR: 11.582278, 10:03:17\n",
      "Step: 217, MSE: 0.069389, PSNR: 11.587081, 10:03:18\n",
      "Step: 218, MSE: 0.069599, PSNR: 11.573986, 10:03:18\n",
      "Step: 219, MSE: 0.069492, PSNR: 11.580678, 10:03:18\n",
      "Step: 220, MSE: 0.069685, PSNR: 11.568632, 10:03:19\n",
      "Step: 221, MSE: 0.069447, PSNR: 11.583473, 10:03:19\n",
      "Step: 222, MSE: 0.069428, PSNR: 11.584631, 10:03:19\n",
      "Step: 223, MSE: 0.069485, PSNR: 11.581086, 10:03:20\n",
      "Step: 224, MSE: 0.069453, PSNR: 11.583059, 10:03:20\n",
      "Step: 225, MSE: 0.069531, PSNR: 11.578240, 10:03:20\n",
      "Step: 226, MSE: 0.069460, PSNR: 11.582624, 10:03:21\n",
      "Step: 227, MSE: 0.069486, PSNR: 11.581055, 10:03:21\n",
      "Step: 228, MSE: 0.069680, PSNR: 11.568936, 10:03:21\n",
      "Step: 229, MSE: 0.069656, PSNR: 11.570412, 10:03:22\n",
      "Step: 230, MSE: 0.069578, PSNR: 11.575260, 10:03:22\n",
      "Step: 231, MSE: 0.069678, PSNR: 11.569070, 10:03:22\n",
      "Step: 232, MSE: 0.069703, PSNR: 11.567490, 10:03:23\n",
      "Step: 233, MSE: 0.069461, PSNR: 11.582613, 10:03:23\n",
      "Step: 234, MSE: 0.069460, PSNR: 11.582663, 10:03:24\n",
      "Step: 235, MSE: 0.069557, PSNR: 11.576578, 10:03:24\n",
      "Step: 236, MSE: 0.069513, PSNR: 11.579337, 10:03:24\n",
      "Step: 237, MSE: 0.069598, PSNR: 11.574055, 10:03:25\n",
      "Step: 238, MSE: 0.069584, PSNR: 11.574907, 10:03:25\n",
      "Step: 239, MSE: 0.069568, PSNR: 11.575928, 10:03:25\n",
      "Step: 240, MSE: 0.069602, PSNR: 11.573790, 10:03:26\n",
      "Step: 241, MSE: 0.069480, PSNR: 11.581390, 10:03:26\n",
      "Step: 242, MSE: 0.069538, PSNR: 11.577786, 10:03:26\n",
      "Step: 243, MSE: 0.069564, PSNR: 11.576141, 10:03:27\n",
      "Step: 244, MSE: 0.069495, PSNR: 11.580472, 10:03:27\n",
      "Step: 245, MSE: 0.069648, PSNR: 11.570921, 10:03:27\n",
      "Step: 246, MSE: 0.069538, PSNR: 11.577755, 10:03:28\n",
      "Step: 247, MSE: 0.069664, PSNR: 11.569895, 10:03:28\n",
      "Step: 248, MSE: 0.069540, PSNR: 11.577623, 10:03:28\n",
      "Step: 249, MSE: 0.069576, PSNR: 11.575390, 10:03:29\n",
      "Step: 250, MSE: 0.069662, PSNR: 11.570011, 10:03:29\n",
      "Step: 251, MSE: 0.069716, PSNR: 11.566683, 10:03:29\n",
      "Step: 252, MSE: 0.069449, PSNR: 11.583330, 10:03:30\n",
      "Step: 253, MSE: 0.069508, PSNR: 11.579624, 10:03:30\n",
      "Step: 254, MSE: 0.069609, PSNR: 11.573344, 10:03:30\n",
      "Step: 255, MSE: 0.069624, PSNR: 11.572411, 10:03:31\n",
      "Step: 256, MSE: 0.069409, PSNR: 11.585861, 10:03:31\n",
      "Step: 257, MSE: 0.069568, PSNR: 11.575894, 10:03:31\n",
      "Step: 258, MSE: 0.069719, PSNR: 11.566462, 10:03:32\n",
      "Step: 259, MSE: 0.069619, PSNR: 11.572729, 10:03:32\n",
      "Step: 260, MSE: 0.069626, PSNR: 11.572267, 10:03:33\n",
      "Step: 261, MSE: 0.069583, PSNR: 11.574955, 10:03:33\n",
      "Step: 262, MSE: 0.069652, PSNR: 11.570675, 10:03:33\n",
      "Step: 263, MSE: 0.069556, PSNR: 11.576672, 10:03:34\n",
      "Step: 264, MSE: 0.069646, PSNR: 11.571012, 10:03:34\n",
      "Step: 265, MSE: 0.069597, PSNR: 11.574121, 10:03:34\n",
      "Step: 266, MSE: 0.069674, PSNR: 11.569281, 10:03:35\n",
      "Step: 267, MSE: 0.069670, PSNR: 11.569527, 10:03:35\n",
      "Step: 268, MSE: 0.069530, PSNR: 11.578283, 10:03:35\n",
      "Step: 269, MSE: 0.069680, PSNR: 11.568913, 10:03:36\n",
      "Step: 270, MSE: 0.069677, PSNR: 11.569084, 10:03:36\n",
      "Step: 271, MSE: 0.069830, PSNR: 11.559553, 10:03:36\n",
      "Step: 272, MSE: 0.069546, PSNR: 11.577307, 10:03:37\n",
      "Step: 273, MSE: 0.069804, PSNR: 11.561194, 10:03:37\n",
      "Step: 274, MSE: 0.069673, PSNR: 11.569367, 10:03:38\n",
      "Step: 275, MSE: 0.069595, PSNR: 11.574206, 10:03:38\n",
      "Step: 276, MSE: 0.069513, PSNR: 11.579330, 10:03:38\n",
      "Step: 277, MSE: 0.069358, PSNR: 11.589029, 10:03:39\n",
      "Step: 278, MSE: 0.069529, PSNR: 11.578336, 10:03:39\n",
      "Step: 279, MSE: 0.069730, PSNR: 11.565775, 10:03:39\n",
      "Step: 280, MSE: 0.069676, PSNR: 11.569175, 10:03:40\n",
      "Step: 281, MSE: 0.069741, PSNR: 11.565098, 10:03:40\n",
      "Step: 282, MSE: 0.069447, PSNR: 11.583453, 10:03:40\n",
      "Step: 283, MSE: 0.069620, PSNR: 11.572662, 10:03:41\n",
      "Step: 284, MSE: 0.069479, PSNR: 11.581465, 10:03:41\n",
      "Step: 285, MSE: 0.069748, PSNR: 11.564704, 10:03:41\n",
      "Step: 286, MSE: 0.069685, PSNR: 11.568626, 10:03:42\n",
      "Step: 287, MSE: 0.069579, PSNR: 11.575246, 10:03:42\n",
      "Step: 288, MSE: 0.069537, PSNR: 11.577831, 10:03:42\n",
      "Step: 289, MSE: 0.069395, PSNR: 11.586704, 10:03:43\n",
      "Step: 290, MSE: 0.069624, PSNR: 11.572420, 10:03:43\n",
      "Step: 291, MSE: 0.069642, PSNR: 11.571316, 10:03:44\n",
      "Step: 292, MSE: 0.069456, PSNR: 11.582927, 10:03:44\n",
      "Step: 293, MSE: 0.069560, PSNR: 11.576410, 10:03:44\n",
      "Step: 294, MSE: 0.069615, PSNR: 11.572953, 10:03:45\n",
      "Step: 295, MSE: 0.069506, PSNR: 11.579790, 10:03:45\n",
      "Step: 296, MSE: 0.069475, PSNR: 11.581724, 10:03:45\n",
      "Step: 297, MSE: 0.069457, PSNR: 11.582860, 10:03:46\n",
      "Step: 298, MSE: 0.069707, PSNR: 11.567222, 10:03:46\n",
      "Step: 299, MSE: 0.069488, PSNR: 11.580894, 10:03:46\n",
      "Step: 300, MSE: 0.069541, PSNR: 11.577572, 10:03:47\n",
      "Step: 301, MSE: 0.069754, PSNR: 11.564326, 10:03:47\n",
      "Step: 302, MSE: 0.069721, PSNR: 11.566354, 10:03:48\n",
      "Step: 303, MSE: 0.069660, PSNR: 11.570143, 10:03:48\n",
      "Step: 304, MSE: 0.069560, PSNR: 11.576422, 10:03:48\n",
      "Step: 305, MSE: 0.069419, PSNR: 11.585205, 10:03:49\n",
      "Step: 306, MSE: 0.069454, PSNR: 11.582999, 10:03:49\n",
      "Step: 307, MSE: 0.069551, PSNR: 11.576975, 10:03:49\n",
      "Step: 308, MSE: 0.069643, PSNR: 11.571217, 10:03:50\n",
      "Step: 309, MSE: 0.069499, PSNR: 11.580215, 10:03:50\n",
      "Step: 310, MSE: 0.069538, PSNR: 11.577761, 10:03:50\n",
      "Step: 311, MSE: 0.069621, PSNR: 11.572582, 10:03:51\n",
      "Step: 312, MSE: 0.069523, PSNR: 11.578684, 10:03:51\n",
      "Step: 313, MSE: 0.069619, PSNR: 11.572691, 10:03:51\n",
      "Step: 314, MSE: 0.069639, PSNR: 11.571472, 10:03:52\n",
      "Step: 315, MSE: 0.069763, PSNR: 11.563774, 10:03:52\n",
      "Step: 316, MSE: 0.069558, PSNR: 11.576537, 10:03:52\n",
      "Step: 317, MSE: 0.069512, PSNR: 11.579399, 10:03:53\n",
      "Step: 318, MSE: 0.069435, PSNR: 11.584227, 10:03:53\n",
      "Step: 319, MSE: 0.069470, PSNR: 11.582051, 10:03:53\n",
      "Step: 320, MSE: 0.069631, PSNR: 11.571984, 10:03:54\n",
      "Step: 321, MSE: 0.069550, PSNR: 11.577054, 10:03:54\n",
      "Step: 322, MSE: 0.069601, PSNR: 11.573859, 10:03:54\n",
      "Step: 323, MSE: 0.069599, PSNR: 11.573964, 10:03:55\n",
      "Step: 324, MSE: 0.069551, PSNR: 11.576972, 10:03:55\n",
      "Step: 325, MSE: 0.069556, PSNR: 11.576625, 10:03:55\n",
      "Step: 326, MSE: 0.069675, PSNR: 11.569201, 10:03:56\n",
      "Step: 327, MSE: 0.069590, PSNR: 11.574506, 10:03:56\n",
      "Step: 328, MSE: 0.069491, PSNR: 11.580720, 10:03:56\n",
      "Step: 329, MSE: 0.069616, PSNR: 11.572884, 10:03:57\n",
      "Step: 330, MSE: 0.069544, PSNR: 11.577405, 10:03:57\n",
      "Step: 331, MSE: 0.069582, PSNR: 11.575017, 10:03:58\n",
      "Step: 332, MSE: 0.069652, PSNR: 11.570694, 10:03:58\n",
      "Step: 333, MSE: 0.069619, PSNR: 11.572741, 10:03:58\n",
      "Step: 334, MSE: 0.069522, PSNR: 11.578750, 10:03:59\n",
      "Step: 335, MSE: 0.069418, PSNR: 11.585266, 10:03:59\n",
      "Step: 336, MSE: 0.069678, PSNR: 11.569035, 10:03:59\n",
      "Step: 337, MSE: 0.069460, PSNR: 11.582624, 10:04:00\n",
      "Step: 338, MSE: 0.069591, PSNR: 11.574497, 10:04:00\n",
      "Step: 339, MSE: 0.069452, PSNR: 11.583168, 10:04:00\n",
      "Step: 340, MSE: 0.069623, PSNR: 11.572496, 10:04:01\n",
      "Step: 341, MSE: 0.069377, PSNR: 11.587848, 10:04:01\n",
      "Step: 342, MSE: 0.069470, PSNR: 11.582021, 10:04:01\n",
      "Step: 343, MSE: 0.069488, PSNR: 11.580905, 10:04:02\n",
      "Step: 344, MSE: 0.069633, PSNR: 11.571844, 10:04:02\n",
      "Step: 345, MSE: 0.069550, PSNR: 11.577055, 10:04:02\n",
      "Step: 346, MSE: 0.069378, PSNR: 11.587812, 10:04:03\n",
      "Step: 347, MSE: 0.069593, PSNR: 11.574353, 10:04:03\n",
      "Step: 348, MSE: 0.069764, PSNR: 11.563659, 10:04:03\n",
      "Step: 349, MSE: 0.069322, PSNR: 11.591273, 10:04:04\n",
      "Step: 350, MSE: 0.069577, PSNR: 11.575351, 10:04:04\n",
      "Step: 351, MSE: 0.069510, PSNR: 11.579510, 10:04:05\n",
      "Step: 352, MSE: 0.069554, PSNR: 11.576787, 10:04:05\n",
      "Step: 353, MSE: 0.069609, PSNR: 11.573344, 10:04:05\n",
      "Step: 354, MSE: 0.069596, PSNR: 11.574183, 10:04:06\n",
      "Step: 355, MSE: 0.069567, PSNR: 11.575980, 10:04:06\n",
      "Step: 356, MSE: 0.069614, PSNR: 11.573049, 10:04:06\n",
      "Step: 357, MSE: 0.069610, PSNR: 11.573296, 10:04:07\n",
      "Step: 358, MSE: 0.069591, PSNR: 11.574499, 10:04:07\n",
      "Step: 359, MSE: 0.069609, PSNR: 11.573344, 10:04:07\n",
      "Step: 360, MSE: 0.069514, PSNR: 11.579296, 10:04:08\n",
      "Step: 361, MSE: 0.069498, PSNR: 11.580305, 10:04:08\n",
      "Step: 362, MSE: 0.069571, PSNR: 11.575693, 10:04:08\n",
      "Step: 363, MSE: 0.069541, PSNR: 11.577602, 10:04:09\n",
      "Step: 364, MSE: 0.069506, PSNR: 11.579750, 10:04:09\n",
      "Step: 365, MSE: 0.069585, PSNR: 11.574869, 10:04:09\n",
      "Step: 366, MSE: 0.069539, PSNR: 11.577738, 10:04:10\n",
      "Step: 367, MSE: 0.069794, PSNR: 11.561800, 10:04:10\n",
      "Step: 368, MSE: 0.069548, PSNR: 11.577145, 10:04:10\n",
      "Step: 369, MSE: 0.069620, PSNR: 11.572634, 10:04:11\n",
      "Step: 370, MSE: 0.069666, PSNR: 11.569807, 10:04:11\n",
      "Step: 371, MSE: 0.069460, PSNR: 11.582676, 10:04:12\n",
      "Step: 372, MSE: 0.069475, PSNR: 11.581699, 10:04:12\n",
      "Step: 373, MSE: 0.069524, PSNR: 11.578641, 10:04:12\n",
      "Step: 374, MSE: 0.069560, PSNR: 11.576382, 10:04:13\n",
      "Step: 375, MSE: 0.069481, PSNR: 11.581367, 10:04:13\n",
      "Step: 376, MSE: 0.069443, PSNR: 11.583704, 10:04:13\n",
      "Step: 377, MSE: 0.069648, PSNR: 11.570906, 10:04:14\n",
      "Step: 378, MSE: 0.069550, PSNR: 11.577004, 10:04:14\n",
      "Step: 379, MSE: 0.069529, PSNR: 11.578348, 10:04:14\n",
      "Step: 380, MSE: 0.069482, PSNR: 11.581287, 10:04:15\n",
      "Step: 381, MSE: 0.069448, PSNR: 11.583426, 10:04:15\n",
      "Step: 382, MSE: 0.069490, PSNR: 11.580809, 10:04:15\n",
      "Step: 383, MSE: 0.069704, PSNR: 11.567426, 10:04:16\n",
      "Step: 384, MSE: 0.069485, PSNR: 11.581062, 10:04:16\n",
      "Step: 385, MSE: 0.069447, PSNR: 11.583476, 10:04:16\n",
      "Step: 386, MSE: 0.069624, PSNR: 11.572426, 10:04:17\n",
      "Step: 387, MSE: 0.069539, PSNR: 11.577725, 10:04:17\n",
      "Step: 388, MSE: 0.069479, PSNR: 11.581493, 10:04:17\n",
      "Step: 389, MSE: 0.069719, PSNR: 11.566511, 10:04:18\n",
      "Step: 390, MSE: 0.069529, PSNR: 11.578312, 10:04:18\n",
      "Step: 391, MSE: 0.069566, PSNR: 11.576051, 10:04:18\n",
      "Step: 392, MSE: 0.069577, PSNR: 11.575326, 10:04:19\n",
      "Step: 393, MSE: 0.069657, PSNR: 11.570332, 10:04:19\n",
      "Step: 394, MSE: 0.069588, PSNR: 11.574640, 10:04:19\n",
      "Step: 395, MSE: 0.069549, PSNR: 11.577108, 10:04:20\n",
      "Step: 396, MSE: 0.069704, PSNR: 11.567408, 10:04:20\n",
      "Step: 397, MSE: 0.069511, PSNR: 11.579449, 10:04:21\n",
      "Step: 398, MSE: 0.069528, PSNR: 11.578424, 10:04:21\n",
      "Step: 399, MSE: 0.069438, PSNR: 11.584042, 10:04:21\n",
      "Step: 400, MSE: 0.069660, PSNR: 11.570167, 10:04:22\n",
      "Step: 401, MSE: 0.069496, PSNR: 11.580418, 10:04:22\n",
      "Step: 402, MSE: 0.069610, PSNR: 11.573299, 10:04:22\n",
      "Step: 403, MSE: 0.069555, PSNR: 11.576708, 10:04:23\n",
      "Step: 404, MSE: 0.069547, PSNR: 11.577194, 10:04:23\n",
      "Step: 405, MSE: 0.069573, PSNR: 11.575605, 10:04:23\n",
      "Step: 406, MSE: 0.069648, PSNR: 11.570890, 10:04:24\n",
      "Step: 407, MSE: 0.069613, PSNR: 11.573111, 10:04:24\n",
      "Step: 408, MSE: 0.069497, PSNR: 11.580320, 10:04:24\n",
      "Step: 409, MSE: 0.069650, PSNR: 11.570777, 10:04:25\n",
      "Step: 410, MSE: 0.069525, PSNR: 11.578581, 10:04:25\n",
      "Step: 411, MSE: 0.069748, PSNR: 11.564702, 10:04:25\n",
      "Step: 412, MSE: 0.069639, PSNR: 11.571453, 10:04:26\n",
      "Step: 413, MSE: 0.069462, PSNR: 11.582532, 10:04:26\n",
      "Step: 414, MSE: 0.069613, PSNR: 11.573094, 10:04:26\n",
      "Step: 415, MSE: 0.069536, PSNR: 11.577904, 10:04:27\n",
      "Step: 416, MSE: 0.069290, PSNR: 11.593283, 10:04:27\n",
      "Step: 417, MSE: 0.069671, PSNR: 11.569491, 10:04:27\n",
      "Step: 418, MSE: 0.069674, PSNR: 11.569270, 10:04:28\n",
      "Step: 419, MSE: 0.069579, PSNR: 11.575195, 10:04:28\n",
      "Step: 420, MSE: 0.069507, PSNR: 11.579710, 10:04:28\n",
      "Step: 421, MSE: 0.069645, PSNR: 11.571108, 10:04:29\n",
      "Step: 422, MSE: 0.069694, PSNR: 11.568050, 10:04:29\n",
      "Step: 423, MSE: 0.069528, PSNR: 11.578390, 10:04:30\n",
      "Step: 424, MSE: 0.069616, PSNR: 11.572939, 10:04:30\n",
      "Step: 425, MSE: 0.069447, PSNR: 11.583481, 10:04:30\n",
      "Step: 426, MSE: 0.069668, PSNR: 11.569660, 10:04:31\n",
      "Step: 427, MSE: 0.069510, PSNR: 11.579518, 10:04:31\n",
      "Step: 428, MSE: 0.069574, PSNR: 11.575502, 10:04:31\n",
      "Step: 429, MSE: 0.069632, PSNR: 11.571905, 10:04:32\n",
      "Step: 430, MSE: 0.069674, PSNR: 11.569294, 10:04:32\n",
      "Step: 431, MSE: 0.069492, PSNR: 11.580634, 10:04:32\n",
      "Step: 432, MSE: 0.069553, PSNR: 11.576832, 10:04:33\n",
      "Step: 433, MSE: 0.069711, PSNR: 11.567015, 10:04:33\n",
      "Step: 434, MSE: 0.069490, PSNR: 11.580777, 10:04:33\n",
      "Step: 435, MSE: 0.069501, PSNR: 11.580103, 10:04:34\n",
      "Step: 436, MSE: 0.069433, PSNR: 11.584358, 10:04:34\n",
      "Step: 437, MSE: 0.069489, PSNR: 11.580847, 10:04:34\n",
      "Step: 438, MSE: 0.069448, PSNR: 11.583406, 10:04:35\n",
      "Step: 439, MSE: 0.069613, PSNR: 11.573069, 10:04:35\n",
      "Step: 440, MSE: 0.069762, PSNR: 11.563797, 10:04:35\n",
      "Step: 441, MSE: 0.069715, PSNR: 11.566749, 10:04:36\n",
      "Step: 442, MSE: 0.069659, PSNR: 11.570215, 10:04:36\n",
      "Step: 443, MSE: 0.069656, PSNR: 11.570417, 10:04:37\n",
      "Step: 444, MSE: 0.069432, PSNR: 11.584419, 10:04:37\n",
      "Step: 445, MSE: 0.069524, PSNR: 11.578674, 10:04:37\n",
      "Step: 446, MSE: 0.069348, PSNR: 11.589643, 10:04:38\n",
      "Step: 447, MSE: 0.069531, PSNR: 11.578212, 10:04:38\n",
      "Step: 448, MSE: 0.069782, PSNR: 11.562593, 10:04:38\n",
      "Step: 449, MSE: 0.069527, PSNR: 11.578454, 10:04:39\n",
      "Step: 450, MSE: 0.069474, PSNR: 11.581806, 10:04:39\n",
      "Step: 451, MSE: 0.069628, PSNR: 11.572180, 10:04:39\n",
      "Step: 452, MSE: 0.069579, PSNR: 11.575230, 10:04:40\n",
      "Step: 453, MSE: 0.069520, PSNR: 11.578888, 10:04:40\n",
      "Step: 454, MSE: 0.069542, PSNR: 11.577558, 10:04:40\n",
      "Step: 455, MSE: 0.069335, PSNR: 11.590480, 10:04:41\n",
      "Step: 456, MSE: 0.069601, PSNR: 11.573837, 10:04:41\n",
      "Step: 457, MSE: 0.069639, PSNR: 11.571495, 10:04:41\n",
      "Step: 458, MSE: 0.069597, PSNR: 11.574116, 10:04:42\n",
      "Step: 459, MSE: 0.069594, PSNR: 11.574277, 10:04:42\n",
      "Step: 460, MSE: 0.069555, PSNR: 11.576704, 10:04:43\n",
      "Step: 461, MSE: 0.069491, PSNR: 11.580714, 10:04:43\n",
      "Step: 462, MSE: 0.069448, PSNR: 11.583409, 10:04:43\n",
      "Step: 463, MSE: 0.069676, PSNR: 11.569161, 10:04:43\n",
      "Step: 464, MSE: 0.069589, PSNR: 11.574584, 10:04:44\n",
      "Step: 465, MSE: 0.069548, PSNR: 11.577130, 10:04:44\n",
      "Step: 466, MSE: 0.069590, PSNR: 11.574505, 10:04:45\n",
      "Step: 467, MSE: 0.069550, PSNR: 11.577026, 10:04:45\n",
      "Step: 468, MSE: 0.069539, PSNR: 11.577735, 10:04:45\n",
      "Step: 469, MSE: 0.069635, PSNR: 11.571747, 10:04:46\n",
      "Step: 470, MSE: 0.069545, PSNR: 11.577316, 10:04:46\n",
      "Step: 471, MSE: 0.069506, PSNR: 11.579771, 10:04:46\n",
      "Step: 472, MSE: 0.069660, PSNR: 11.570148, 10:04:47\n",
      "Step: 473, MSE: 0.069366, PSNR: 11.588544, 10:04:47\n",
      "Step: 474, MSE: 0.069481, PSNR: 11.581347, 10:04:47\n",
      "Step: 475, MSE: 0.069499, PSNR: 11.580194, 10:04:48\n",
      "Step: 476, MSE: 0.069488, PSNR: 11.580888, 10:04:48\n",
      "Step: 477, MSE: 0.069522, PSNR: 11.578771, 10:04:48\n",
      "Step: 478, MSE: 0.069537, PSNR: 11.577824, 10:04:49\n",
      "Step: 479, MSE: 0.069508, PSNR: 11.579637, 10:04:49\n",
      "Step: 480, MSE: 0.069418, PSNR: 11.585285, 10:04:49\n",
      "Step: 481, MSE: 0.069517, PSNR: 11.579115, 10:04:50\n",
      "Step: 482, MSE: 0.069461, PSNR: 11.582622, 10:04:50\n",
      "Step: 483, MSE: 0.069558, PSNR: 11.576509, 10:04:50\n",
      "Step: 484, MSE: 0.069501, PSNR: 11.580063, 10:04:51\n",
      "Step: 485, MSE: 0.069597, PSNR: 11.574095, 10:04:51\n",
      "Step: 486, MSE: 0.069532, PSNR: 11.578163, 10:04:51\n",
      "Step: 487, MSE: 0.069476, PSNR: 11.581660, 10:04:52\n",
      "Step: 488, MSE: 0.069529, PSNR: 11.578364, 10:04:52\n",
      "Step: 489, MSE: 0.069703, PSNR: 11.567489, 10:04:52\n",
      "Step: 490, MSE: 0.069541, PSNR: 11.577562, 10:04:53\n",
      "Step: 491, MSE: 0.069480, PSNR: 11.581404, 10:04:53\n",
      "Step: 492, MSE: 0.069545, PSNR: 11.577334, 10:04:54\n",
      "Step: 493, MSE: 0.069553, PSNR: 11.576855, 10:04:54\n",
      "Step: 494, MSE: 0.069488, PSNR: 11.580912, 10:04:54\n",
      "Step: 495, MSE: 0.069355, PSNR: 11.589226, 10:04:55\n",
      "Step: 496, MSE: 0.069613, PSNR: 11.573106, 10:04:55\n",
      "Step: 497, MSE: 0.069530, PSNR: 11.578257, 10:04:55\n",
      "Step: 498, MSE: 0.069434, PSNR: 11.584273, 10:04:56\n",
      "Step: 499, MSE: 0.069554, PSNR: 11.576761, 10:04:56\n",
      "Step: 500, MSE: 0.069612, PSNR: 11.573164, 10:04:56\n",
      "Step: 501, MSE: 0.069539, PSNR: 11.577688, 10:04:57\n",
      "Step: 502, MSE: 0.069552, PSNR: 11.576900, 10:04:57\n",
      "Step: 503, MSE: 0.069638, PSNR: 11.571568, 10:04:57\n",
      "Step: 504, MSE: 0.069782, PSNR: 11.562584, 10:04:58\n",
      "Step: 505, MSE: 0.069472, PSNR: 11.581873, 10:04:58\n",
      "Step: 506, MSE: 0.069459, PSNR: 11.582747, 10:04:58\n",
      "Step: 507, MSE: 0.069618, PSNR: 11.572813, 10:04:59\n",
      "Step: 508, MSE: 0.069602, PSNR: 11.573799, 10:04:59\n",
      "Step: 509, MSE: 0.069754, PSNR: 11.564288, 10:05:00\n",
      "Step: 510, MSE: 0.069560, PSNR: 11.576391, 10:05:00\n",
      "Step: 511, MSE: 0.069597, PSNR: 11.574121, 10:05:00\n",
      "Step: 512, MSE: 0.069590, PSNR: 11.574549, 10:05:01\n",
      "Step: 513, MSE: 0.069519, PSNR: 11.578981, 10:05:01\n",
      "Step: 514, MSE: 0.069567, PSNR: 11.575964, 10:05:01\n",
      "Step: 515, MSE: 0.069553, PSNR: 11.576866, 10:05:02\n",
      "Step: 516, MSE: 0.069603, PSNR: 11.573711, 10:05:02\n",
      "Step: 517, MSE: 0.069772, PSNR: 11.563206, 10:05:02\n",
      "Step: 518, MSE: 0.069623, PSNR: 11.572451, 10:05:03\n",
      "Step: 519, MSE: 0.069663, PSNR: 11.569995, 10:05:03\n",
      "Step: 520, MSE: 0.069576, PSNR: 11.575428, 10:05:03\n",
      "Step: 521, MSE: 0.069450, PSNR: 11.583296, 10:05:04\n",
      "Step: 522, MSE: 0.069544, PSNR: 11.577381, 10:05:04\n",
      "Step: 523, MSE: 0.069525, PSNR: 11.578617, 10:05:04\n",
      "Step: 524, MSE: 0.069597, PSNR: 11.574089, 10:05:05\n",
      "Step: 525, MSE: 0.069551, PSNR: 11.576957, 10:05:05\n",
      "Step: 526, MSE: 0.069296, PSNR: 11.592939, 10:05:05\n",
      "Step: 527, MSE: 0.069439, PSNR: 11.583956, 10:05:06\n",
      "Step: 528, MSE: 0.069500, PSNR: 11.580171, 10:05:06\n",
      "Step: 529, MSE: 0.069390, PSNR: 11.587005, 10:05:06\n",
      "Step: 530, MSE: 0.069573, PSNR: 11.575617, 10:05:07\n",
      "Step: 531, MSE: 0.069653, PSNR: 11.570606, 10:05:07\n",
      "Step: 532, MSE: 0.069529, PSNR: 11.578346, 10:05:07\n",
      "Step: 533, MSE: 0.069549, PSNR: 11.577115, 10:05:08\n",
      "Step: 534, MSE: 0.069521, PSNR: 11.578869, 10:05:08\n",
      "Step: 535, MSE: 0.069483, PSNR: 11.581200, 10:05:09\n",
      "Step: 536, MSE: 0.069656, PSNR: 11.570434, 10:05:09\n",
      "Step: 537, MSE: 0.069502, PSNR: 11.579998, 10:05:09\n",
      "Step: 538, MSE: 0.069676, PSNR: 11.569194, 10:05:10\n",
      "Step: 539, MSE: 0.069519, PSNR: 11.578975, 10:05:10\n",
      "Step: 540, MSE: 0.069516, PSNR: 11.579159, 10:05:10\n",
      "Step: 541, MSE: 0.069534, PSNR: 11.578001, 10:05:11\n",
      "Step: 542, MSE: 0.069439, PSNR: 11.583938, 10:05:11\n",
      "Step: 543, MSE: 0.069575, PSNR: 11.575493, 10:05:11\n",
      "Step: 544, MSE: 0.069555, PSNR: 11.576728, 10:05:12\n",
      "Step: 545, MSE: 0.069527, PSNR: 11.578468, 10:05:12\n",
      "Step: 546, MSE: 0.069519, PSNR: 11.578997, 10:05:12\n",
      "Step: 547, MSE: 0.069580, PSNR: 11.575147, 10:05:13\n",
      "Step: 548, MSE: 0.069602, PSNR: 11.573786, 10:05:13\n",
      "Step: 549, MSE: 0.069794, PSNR: 11.561827, 10:05:13\n",
      "Step: 550, MSE: 0.069497, PSNR: 11.580335, 10:05:14\n",
      "Step: 551, MSE: 0.069626, PSNR: 11.572300, 10:05:14\n",
      "Step: 552, MSE: 0.069559, PSNR: 11.576481, 10:05:14\n",
      "Step: 553, MSE: 0.069704, PSNR: 11.567422, 10:05:15\n",
      "Step: 554, MSE: 0.069426, PSNR: 11.584749, 10:05:15\n",
      "Step: 555, MSE: 0.069523, PSNR: 11.578720, 10:05:15\n",
      "Step: 556, MSE: 0.069366, PSNR: 11.588544, 10:05:16\n",
      "Step: 557, MSE: 0.069562, PSNR: 11.576304, 10:05:16\n",
      "Step: 558, MSE: 0.069412, PSNR: 11.585648, 10:05:16\n",
      "Step: 559, MSE: 0.069375, PSNR: 11.587943, 10:05:17\n",
      "Step: 560, MSE: 0.069638, PSNR: 11.571517, 10:05:17\n",
      "Step: 561, MSE: 0.069730, PSNR: 11.565795, 10:05:17\n",
      "Step: 562, MSE: 0.069696, PSNR: 11.567919, 10:05:18\n",
      "Step: 563, MSE: 0.069693, PSNR: 11.568131, 10:05:18\n",
      "Step: 564, MSE: 0.069695, PSNR: 11.567959, 10:05:18\n",
      "Step: 565, MSE: 0.069511, PSNR: 11.579479, 10:05:19\n",
      "Step: 566, MSE: 0.069384, PSNR: 11.587428, 10:05:19\n",
      "Step: 567, MSE: 0.069486, PSNR: 11.581040, 10:05:20\n",
      "Step: 568, MSE: 0.069704, PSNR: 11.567432, 10:05:20\n",
      "Step: 569, MSE: 0.069616, PSNR: 11.572915, 10:05:20\n",
      "Step: 570, MSE: 0.069653, PSNR: 11.570599, 10:05:21\n",
      "Step: 571, MSE: 0.069536, PSNR: 11.577929, 10:05:21\n",
      "Step: 572, MSE: 0.069636, PSNR: 11.571659, 10:05:21\n",
      "Step: 573, MSE: 0.069528, PSNR: 11.578431, 10:05:22\n",
      "Step: 574, MSE: 0.069711, PSNR: 11.566983, 10:05:22\n",
      "Step: 575, MSE: 0.069455, PSNR: 11.582955, 10:05:22\n",
      "Step: 576, MSE: 0.069610, PSNR: 11.573315, 10:05:23\n",
      "Step: 577, MSE: 0.069515, PSNR: 11.579187, 10:05:23\n",
      "Step: 578, MSE: 0.069496, PSNR: 11.580423, 10:05:23\n",
      "Step: 579, MSE: 0.069502, PSNR: 11.580008, 10:05:24\n",
      "Step: 580, MSE: 0.069583, PSNR: 11.574976, 10:05:24\n",
      "Step: 581, MSE: 0.069534, PSNR: 11.578033, 10:05:24\n",
      "Step: 582, MSE: 0.069424, PSNR: 11.584874, 10:05:25\n",
      "Step: 583, MSE: 0.069694, PSNR: 11.568047, 10:05:25\n",
      "Step: 584, MSE: 0.069508, PSNR: 11.579667, 10:05:25\n",
      "Step: 585, MSE: 0.069501, PSNR: 11.580082, 10:05:26\n",
      "Step: 586, MSE: 0.069490, PSNR: 11.580781, 10:05:26\n",
      "Step: 587, MSE: 0.069642, PSNR: 11.571275, 10:05:27\n",
      "Step: 588, MSE: 0.069664, PSNR: 11.569919, 10:05:27\n",
      "Step: 589, MSE: 0.069464, PSNR: 11.582385, 10:05:27\n",
      "Step: 590, MSE: 0.069402, PSNR: 11.586281, 10:05:28\n",
      "Step: 591, MSE: 0.069457, PSNR: 11.582828, 10:05:28\n",
      "Step: 592, MSE: 0.069374, PSNR: 11.588011, 10:05:28\n",
      "Step: 593, MSE: 0.069423, PSNR: 11.584959, 10:05:29\n",
      "Step: 594, MSE: 0.069356, PSNR: 11.589135, 10:05:29\n",
      "Step: 595, MSE: 0.069537, PSNR: 11.577855, 10:05:29\n",
      "Step: 596, MSE: 0.069758, PSNR: 11.564089, 10:05:30\n",
      "Step: 597, MSE: 0.069512, PSNR: 11.579379, 10:05:30\n",
      "Step: 598, MSE: 0.069610, PSNR: 11.573256, 10:05:30\n",
      "Step: 599, MSE: 0.069566, PSNR: 11.576015, 10:05:31\n",
      "Step: 600, MSE: 0.069622, PSNR: 11.572562, 10:05:31\n",
      "Step: 601, MSE: 0.069548, PSNR: 11.577153, 10:05:31\n",
      "Step: 602, MSE: 0.069663, PSNR: 11.569966, 10:05:32\n",
      "Step: 603, MSE: 0.069547, PSNR: 11.577209, 10:05:32\n",
      "Step: 604, MSE: 0.069610, PSNR: 11.573290, 10:05:32\n",
      "Step: 605, MSE: 0.069488, PSNR: 11.580925, 10:05:33\n",
      "Step: 606, MSE: 0.069329, PSNR: 11.590824, 10:05:33\n",
      "Step: 607, MSE: 0.069578, PSNR: 11.575270, 10:05:33\n",
      "Step: 608, MSE: 0.069540, PSNR: 11.577645, 10:05:34\n",
      "Step: 609, MSE: 0.069653, PSNR: 11.570574, 10:05:34\n",
      "Step: 610, MSE: 0.069540, PSNR: 11.577634, 10:05:34\n",
      "Step: 611, MSE: 0.069490, PSNR: 11.580768, 10:05:35\n",
      "Step: 612, MSE: 0.069566, PSNR: 11.576043, 10:05:35\n",
      "Step: 613, MSE: 0.069536, PSNR: 11.577884, 10:05:36\n",
      "Step: 614, MSE: 0.069563, PSNR: 11.576205, 10:05:36\n",
      "Step: 615, MSE: 0.069436, PSNR: 11.584149, 10:05:36\n",
      "Step: 616, MSE: 0.069523, PSNR: 11.578732, 10:05:37\n",
      "Step: 617, MSE: 0.069448, PSNR: 11.583432, 10:05:37\n",
      "Step: 618, MSE: 0.069731, PSNR: 11.565747, 10:05:37\n",
      "Step: 619, MSE: 0.069539, PSNR: 11.577684, 10:05:38\n",
      "Step: 620, MSE: 0.069535, PSNR: 11.577953, 10:05:38\n",
      "Step: 621, MSE: 0.069498, PSNR: 11.580296, 10:05:38\n",
      "Step: 622, MSE: 0.069745, PSNR: 11.564893, 10:05:39\n",
      "Step: 623, MSE: 0.069554, PSNR: 11.576754, 10:05:39\n",
      "Step: 624, MSE: 0.069422, PSNR: 11.585033, 10:05:39\n",
      "Step: 625, MSE: 0.069586, PSNR: 11.574801, 10:05:40\n",
      "Step: 626, MSE: 0.069512, PSNR: 11.579393, 10:05:40\n",
      "Step: 627, MSE: 0.069627, PSNR: 11.572252, 10:05:40\n",
      "Step: 628, MSE: 0.069468, PSNR: 11.582137, 10:05:41\n",
      "Step: 629, MSE: 0.069637, PSNR: 11.571608, 10:05:41\n",
      "Step: 630, MSE: 0.069517, PSNR: 11.579083, 10:05:41\n",
      "Step: 631, MSE: 0.069483, PSNR: 11.581187, 10:05:42\n",
      "Step: 632, MSE: 0.069645, PSNR: 11.571102, 10:05:42\n",
      "Step: 633, MSE: 0.069771, PSNR: 11.563253, 10:05:43\n",
      "Step: 634, MSE: 0.069633, PSNR: 11.571840, 10:05:43\n",
      "Step: 635, MSE: 0.069473, PSNR: 11.581841, 10:05:43\n",
      "Step: 636, MSE: 0.069504, PSNR: 11.579908, 10:05:44\n",
      "Step: 637, MSE: 0.069582, PSNR: 11.575052, 10:05:44\n",
      "Step: 638, MSE: 0.069612, PSNR: 11.573135, 10:05:44\n",
      "Step: 639, MSE: 0.069651, PSNR: 11.570711, 10:05:45\n",
      "Step: 640, MSE: 0.069600, PSNR: 11.573906, 10:05:45\n",
      "Step: 641, MSE: 0.069516, PSNR: 11.579181, 10:05:45\n",
      "Step: 642, MSE: 0.069569, PSNR: 11.575865, 10:05:46\n",
      "Step: 643, MSE: 0.069512, PSNR: 11.579385, 10:05:46\n",
      "Step: 644, MSE: 0.069321, PSNR: 11.591350, 10:05:46\n",
      "Step: 645, MSE: 0.069517, PSNR: 11.579103, 10:05:47\n",
      "Step: 646, MSE: 0.069526, PSNR: 11.578502, 10:05:47\n",
      "Step: 647, MSE: 0.069591, PSNR: 11.574450, 10:05:47\n",
      "Step: 648, MSE: 0.069490, PSNR: 11.580807, 10:05:48\n",
      "Step: 649, MSE: 0.069628, PSNR: 11.572160, 10:05:48\n",
      "Step: 650, MSE: 0.069498, PSNR: 11.580300, 10:05:48\n",
      "Step: 651, MSE: 0.069640, PSNR: 11.571392, 10:05:49\n",
      "Step: 652, MSE: 0.069624, PSNR: 11.572390, 10:05:49\n",
      "Step: 653, MSE: 0.069424, PSNR: 11.584931, 10:05:49\n",
      "Step: 654, MSE: 0.069567, PSNR: 11.575993, 10:05:50\n",
      "Step: 655, MSE: 0.069633, PSNR: 11.571864, 10:05:50\n",
      "Step: 656, MSE: 0.069500, PSNR: 11.580133, 10:05:50\n",
      "Step: 657, MSE: 0.069668, PSNR: 11.569682, 10:05:51\n",
      "Step: 658, MSE: 0.069405, PSNR: 11.586117, 10:05:51\n",
      "Step: 659, MSE: 0.069519, PSNR: 11.578937, 10:05:52\n",
      "Step: 660, MSE: 0.069606, PSNR: 11.573538, 10:05:52\n",
      "Step: 661, MSE: 0.069515, PSNR: 11.579204, 10:05:52\n",
      "Step: 662, MSE: 0.069635, PSNR: 11.571731, 10:05:53\n",
      "Step: 663, MSE: 0.069360, PSNR: 11.588909, 10:05:53\n",
      "Step: 664, MSE: 0.069451, PSNR: 11.583244, 10:05:53\n",
      "Step: 665, MSE: 0.069555, PSNR: 11.576728, 10:05:54\n",
      "Step: 666, MSE: 0.069531, PSNR: 11.578209, 10:05:54\n",
      "Step: 667, MSE: 0.069596, PSNR: 11.574142, 10:05:54\n",
      "Step: 668, MSE: 0.069524, PSNR: 11.578642, 10:05:55\n",
      "Step: 669, MSE: 0.069422, PSNR: 11.585002, 10:05:55\n",
      "Step: 670, MSE: 0.069619, PSNR: 11.572747, 10:05:55\n",
      "Step: 671, MSE: 0.069820, PSNR: 11.560187, 10:05:56\n",
      "Step: 672, MSE: 0.069407, PSNR: 11.585950, 10:05:56\n",
      "Step: 673, MSE: 0.069515, PSNR: 11.579201, 10:05:56\n",
      "Step: 674, MSE: 0.069536, PSNR: 11.577926, 10:05:57\n",
      "Step: 675, MSE: 0.069656, PSNR: 11.570389, 10:05:57\n",
      "Step: 676, MSE: 0.069577, PSNR: 11.575374, 10:05:57\n",
      "Step: 677, MSE: 0.069454, PSNR: 11.583021, 10:05:58\n",
      "Step: 678, MSE: 0.069613, PSNR: 11.573122, 10:05:58\n",
      "Step: 679, MSE: 0.069477, PSNR: 11.581585, 10:05:58\n",
      "Step: 680, MSE: 0.069608, PSNR: 11.573414, 10:05:59\n",
      "Step: 681, MSE: 0.069654, PSNR: 11.570531, 10:05:59\n",
      "Step: 682, MSE: 0.069415, PSNR: 11.585459, 10:05:59\n",
      "Step: 683, MSE: 0.069474, PSNR: 11.581802, 10:06:00\n",
      "Step: 684, MSE: 0.069616, PSNR: 11.572929, 10:06:00\n",
      "Step: 685, MSE: 0.069606, PSNR: 11.573553, 10:06:00\n",
      "Step: 686, MSE: 0.069603, PSNR: 11.573734, 10:06:01\n",
      "Step: 687, MSE: 0.069649, PSNR: 11.570865, 10:06:01\n",
      "Step: 688, MSE: 0.069502, PSNR: 11.580033, 10:06:01\n",
      "Step: 689, MSE: 0.069648, PSNR: 11.570888, 10:06:02\n",
      "Step: 690, MSE: 0.069480, PSNR: 11.581419, 10:06:02\n",
      "Step: 691, MSE: 0.069430, PSNR: 11.584549, 10:06:02\n",
      "Step: 692, MSE: 0.069650, PSNR: 11.570808, 10:06:03\n",
      "Step: 693, MSE: 0.069417, PSNR: 11.585316, 10:06:03\n",
      "Step: 694, MSE: 0.069643, PSNR: 11.571243, 10:06:03\n",
      "Step: 695, MSE: 0.069569, PSNR: 11.575870, 10:06:04\n",
      "Step: 696, MSE: 0.069526, PSNR: 11.578526, 10:06:04\n",
      "Step: 697, MSE: 0.069671, PSNR: 11.569481, 10:06:04\n",
      "Step: 698, MSE: 0.069631, PSNR: 11.571968, 10:06:05\n",
      "Step: 699, MSE: 0.069766, PSNR: 11.563557, 10:06:05\n",
      "Step: 700, MSE: 0.069617, PSNR: 11.572844, 10:06:06\n",
      "Step: 701, MSE: 0.069652, PSNR: 11.570684, 10:06:06\n",
      "Step: 702, MSE: 0.069698, PSNR: 11.567774, 10:06:06\n",
      "Step: 703, MSE: 0.069614, PSNR: 11.573009, 10:06:07\n",
      "Step: 704, MSE: 0.069351, PSNR: 11.589449, 10:06:07\n",
      "Step: 705, MSE: 0.069552, PSNR: 11.576901, 10:06:07\n",
      "Step: 706, MSE: 0.069572, PSNR: 11.575684, 10:06:08\n",
      "Step: 707, MSE: 0.069638, PSNR: 11.571535, 10:06:08\n",
      "Step: 708, MSE: 0.069545, PSNR: 11.577351, 10:06:08\n",
      "Step: 709, MSE: 0.069470, PSNR: 11.582029, 10:06:09\n",
      "Step: 710, MSE: 0.069483, PSNR: 11.581235, 10:06:09\n",
      "Step: 711, MSE: 0.069583, PSNR: 11.574998, 10:06:09\n",
      "Step: 712, MSE: 0.069588, PSNR: 11.574644, 10:06:10\n",
      "Step: 713, MSE: 0.069408, PSNR: 11.585918, 10:06:10\n",
      "Step: 714, MSE: 0.069612, PSNR: 11.573133, 10:06:10\n",
      "Step: 715, MSE: 0.069581, PSNR: 11.575064, 10:06:11\n",
      "Step: 716, MSE: 0.069733, PSNR: 11.565614, 10:06:11\n",
      "Step: 717, MSE: 0.069611, PSNR: 11.573198, 10:06:11\n",
      "Step: 718, MSE: 0.069606, PSNR: 11.573561, 10:06:12\n",
      "Step: 719, MSE: 0.069627, PSNR: 11.572226, 10:06:12\n",
      "Step: 720, MSE: 0.069547, PSNR: 11.577216, 10:06:12\n",
      "Step: 721, MSE: 0.069651, PSNR: 11.570719, 10:06:13\n",
      "Step: 722, MSE: 0.069666, PSNR: 11.569816, 10:06:13\n",
      "Step: 723, MSE: 0.069420, PSNR: 11.585171, 10:06:14\n",
      "Step: 724, MSE: 0.069634, PSNR: 11.571779, 10:06:14\n",
      "Step: 725, MSE: 0.069474, PSNR: 11.581749, 10:06:14\n",
      "Step: 726, MSE: 0.069502, PSNR: 11.580024, 10:06:14\n",
      "Step: 727, MSE: 0.069546, PSNR: 11.577268, 10:06:15\n",
      "Step: 728, MSE: 0.069545, PSNR: 11.577373, 10:06:15\n",
      "Step: 729, MSE: 0.069434, PSNR: 11.584264, 10:06:16\n",
      "Step: 730, MSE: 0.069534, PSNR: 11.578041, 10:06:16\n",
      "Step: 731, MSE: 0.069707, PSNR: 11.567241, 10:06:16\n",
      "Step: 732, MSE: 0.069548, PSNR: 11.577160, 10:06:17\n",
      "Step: 733, MSE: 0.069702, PSNR: 11.567565, 10:06:17\n",
      "Step: 734, MSE: 0.069682, PSNR: 11.568814, 10:06:17\n",
      "Step: 735, MSE: 0.069477, PSNR: 11.581589, 10:06:18\n",
      "Step: 736, MSE: 0.069421, PSNR: 11.585084, 10:06:18\n",
      "Step: 737, MSE: 0.069503, PSNR: 11.579952, 10:06:18\n",
      "Step: 738, MSE: 0.069432, PSNR: 11.584401, 10:06:19\n",
      "Step: 739, MSE: 0.069671, PSNR: 11.569509, 10:06:19\n",
      "Step: 740, MSE: 0.069511, PSNR: 11.579479, 10:06:19\n",
      "Step: 741, MSE: 0.069540, PSNR: 11.577656, 10:06:20\n",
      "Step: 742, MSE: 0.069631, PSNR: 11.571985, 10:06:20\n",
      "Step: 743, MSE: 0.069572, PSNR: 11.575675, 10:06:21\n",
      "Step: 744, MSE: 0.069517, PSNR: 11.579089, 10:06:21\n",
      "Step: 745, MSE: 0.069541, PSNR: 11.577582, 10:06:21\n",
      "Step: 746, MSE: 0.069576, PSNR: 11.575417, 10:06:22\n",
      "Step: 747, MSE: 0.069571, PSNR: 11.575688, 10:06:22\n",
      "Step: 748, MSE: 0.069504, PSNR: 11.579906, 10:06:22\n",
      "Step: 749, MSE: 0.069650, PSNR: 11.570796, 10:06:23\n",
      "Step: 750, MSE: 0.069528, PSNR: 11.578399, 10:06:23\n",
      "Step: 751, MSE: 0.069671, PSNR: 11.569468, 10:06:23\n",
      "Step: 752, MSE: 0.069587, PSNR: 11.574701, 10:06:24\n",
      "Step: 753, MSE: 0.069507, PSNR: 11.579694, 10:06:24\n",
      "Step: 754, MSE: 0.069603, PSNR: 11.573745, 10:06:24\n",
      "Step: 755, MSE: 0.069525, PSNR: 11.578615, 10:06:25\n",
      "Step: 756, MSE: 0.069587, PSNR: 11.574703, 10:06:25\n",
      "Step: 757, MSE: 0.069410, PSNR: 11.585779, 10:06:25\n",
      "Step: 758, MSE: 0.069607, PSNR: 11.573495, 10:06:26\n",
      "Step: 759, MSE: 0.069575, PSNR: 11.575499, 10:06:26\n",
      "Step: 760, MSE: 0.069558, PSNR: 11.576500, 10:06:26\n",
      "Step: 761, MSE: 0.069433, PSNR: 11.584324, 10:06:27\n",
      "Step: 762, MSE: 0.069687, PSNR: 11.568513, 10:06:27\n",
      "Step: 763, MSE: 0.069466, PSNR: 11.582279, 10:06:27\n",
      "Step: 764, MSE: 0.069479, PSNR: 11.581464, 10:06:28\n",
      "Step: 765, MSE: 0.069643, PSNR: 11.571197, 10:06:28\n",
      "Step: 766, MSE: 0.069663, PSNR: 11.569979, 10:06:28\n",
      "Step: 767, MSE: 0.069528, PSNR: 11.578402, 10:06:29\n",
      "Step: 768, MSE: 0.069498, PSNR: 11.580267, 10:06:29\n",
      "Step: 769, MSE: 0.069490, PSNR: 11.580788, 10:06:29\n",
      "Step: 770, MSE: 0.069591, PSNR: 11.574465, 10:06:30\n",
      "Step: 771, MSE: 0.069552, PSNR: 11.576882, 10:06:30\n",
      "Step: 772, MSE: 0.069590, PSNR: 11.574521, 10:06:30\n",
      "Step: 773, MSE: 0.069505, PSNR: 11.579827, 10:06:31\n",
      "Step: 774, MSE: 0.069622, PSNR: 11.572509, 10:06:31\n",
      "Step: 775, MSE: 0.069532, PSNR: 11.578129, 10:06:31\n",
      "Step: 776, MSE: 0.069556, PSNR: 11.576680, 10:06:32\n",
      "Step: 777, MSE: 0.069546, PSNR: 11.577293, 10:06:32\n",
      "Step: 778, MSE: 0.069515, PSNR: 11.579233, 10:06:32\n",
      "Step: 779, MSE: 0.069600, PSNR: 11.573915, 10:06:33\n",
      "Step: 780, MSE: 0.069668, PSNR: 11.569677, 10:06:33\n",
      "Step: 781, MSE: 0.069338, PSNR: 11.590318, 10:06:34\n",
      "Step: 782, MSE: 0.069496, PSNR: 11.580383, 10:06:34\n",
      "Step: 783, MSE: 0.069522, PSNR: 11.578759, 10:06:34\n",
      "Step: 784, MSE: 0.069710, PSNR: 11.567050, 10:06:35\n",
      "Step: 785, MSE: 0.069510, PSNR: 11.579502, 10:06:35\n",
      "Step: 786, MSE: 0.069556, PSNR: 11.576671, 10:06:35\n",
      "Step: 787, MSE: 0.069656, PSNR: 11.570424, 10:06:36\n",
      "Step: 788, MSE: 0.069724, PSNR: 11.566195, 10:06:36\n",
      "Step: 789, MSE: 0.069510, PSNR: 11.579504, 10:06:36\n",
      "Step: 790, MSE: 0.069520, PSNR: 11.578890, 10:06:37\n",
      "Step: 791, MSE: 0.069616, PSNR: 11.572903, 10:06:37\n",
      "Step: 792, MSE: 0.069519, PSNR: 11.578988, 10:06:37\n",
      "Step: 793, MSE: 0.069656, PSNR: 11.570445, 10:06:38\n",
      "Step: 794, MSE: 0.069775, PSNR: 11.562979, 10:06:38\n",
      "Step: 795, MSE: 0.069411, PSNR: 11.585691, 10:06:38\n",
      "Step: 796, MSE: 0.069538, PSNR: 11.577783, 10:06:39\n",
      "Step: 797, MSE: 0.069518, PSNR: 11.579035, 10:06:39\n",
      "Step: 798, MSE: 0.069522, PSNR: 11.578791, 10:06:39\n",
      "Step: 799, MSE: 0.069320, PSNR: 11.591415, 10:06:40\n",
      "Step: 800, MSE: 0.069481, PSNR: 11.581322, 10:06:40\n",
      "Step: 801, MSE: 0.069507, PSNR: 11.579718, 10:06:40\n",
      "Step: 802, MSE: 0.069442, PSNR: 11.583797, 10:06:41\n",
      "Step: 803, MSE: 0.069662, PSNR: 11.570024, 10:06:41\n",
      "Step: 804, MSE: 0.069560, PSNR: 11.576420, 10:06:41\n",
      "Step: 805, MSE: 0.069757, PSNR: 11.564101, 10:06:42\n",
      "Step: 806, MSE: 0.069696, PSNR: 11.567918, 10:06:42\n",
      "Step: 807, MSE: 0.069561, PSNR: 11.576330, 10:06:42\n",
      "Step: 808, MSE: 0.069509, PSNR: 11.579578, 10:06:43\n",
      "Step: 809, MSE: 0.069640, PSNR: 11.571426, 10:06:43\n",
      "Step: 810, MSE: 0.069537, PSNR: 11.577814, 10:06:44\n",
      "Step: 811, MSE: 0.069539, PSNR: 11.577711, 10:06:44\n",
      "Step: 812, MSE: 0.069682, PSNR: 11.568777, 10:06:44\n",
      "Step: 813, MSE: 0.069331, PSNR: 11.590699, 10:06:45\n",
      "Step: 814, MSE: 0.069463, PSNR: 11.582484, 10:06:45\n",
      "Step: 815, MSE: 0.069484, PSNR: 11.581127, 10:06:45\n",
      "Step: 816, MSE: 0.069453, PSNR: 11.583069, 10:06:46\n",
      "Step: 817, MSE: 0.069752, PSNR: 11.564419, 10:06:46\n",
      "Step: 818, MSE: 0.069556, PSNR: 11.576635, 10:06:46\n",
      "Step: 819, MSE: 0.069457, PSNR: 11.582863, 10:06:47\n",
      "Step: 820, MSE: 0.069532, PSNR: 11.578129, 10:06:47\n",
      "Step: 821, MSE: 0.069461, PSNR: 11.582594, 10:06:48\n",
      "Step: 822, MSE: 0.069451, PSNR: 11.583200, 10:06:48\n",
      "Step: 823, MSE: 0.069501, PSNR: 11.580106, 10:06:48\n",
      "Step: 824, MSE: 0.069491, PSNR: 11.580734, 10:06:49\n",
      "Step: 825, MSE: 0.069610, PSNR: 11.573309, 10:06:49\n",
      "Step: 826, MSE: 0.069713, PSNR: 11.566889, 10:06:49\n",
      "Step: 827, MSE: 0.069474, PSNR: 11.581772, 10:06:50\n",
      "Step: 828, MSE: 0.069568, PSNR: 11.575875, 10:06:50\n",
      "Step: 829, MSE: 0.069618, PSNR: 11.572763, 10:06:50\n",
      "Step: 830, MSE: 0.069405, PSNR: 11.586109, 10:06:51\n",
      "Step: 831, MSE: 0.069627, PSNR: 11.572194, 10:06:51\n",
      "Step: 832, MSE: 0.069551, PSNR: 11.576960, 10:06:51\n",
      "Step: 833, MSE: 0.069478, PSNR: 11.581514, 10:06:52\n",
      "Step: 834, MSE: 0.069501, PSNR: 11.580111, 10:06:52\n",
      "Step: 835, MSE: 0.069501, PSNR: 11.580116, 10:06:52\n",
      "Step: 836, MSE: 0.069506, PSNR: 11.579762, 10:06:53\n",
      "Step: 837, MSE: 0.069488, PSNR: 11.580888, 10:06:53\n",
      "Step: 838, MSE: 0.069606, PSNR: 11.573562, 10:06:54\n",
      "Step: 839, MSE: 0.069511, PSNR: 11.579494, 10:06:54\n",
      "Step: 840, MSE: 0.069462, PSNR: 11.582531, 10:06:54\n",
      "Step: 841, MSE: 0.069563, PSNR: 11.576235, 10:06:55\n",
      "Step: 842, MSE: 0.069566, PSNR: 11.576014, 10:06:55\n",
      "Step: 843, MSE: 0.069480, PSNR: 11.581388, 10:06:55\n",
      "Step: 844, MSE: 0.069599, PSNR: 11.573950, 10:06:56\n",
      "Step: 845, MSE: 0.069507, PSNR: 11.579702, 10:06:56\n",
      "Step: 846, MSE: 0.069643, PSNR: 11.571236, 10:06:56\n",
      "Step: 847, MSE: 0.069585, PSNR: 11.574848, 10:06:57\n",
      "Step: 848, MSE: 0.069604, PSNR: 11.573635, 10:06:57\n",
      "Step: 849, MSE: 0.069621, PSNR: 11.572603, 10:06:57\n",
      "Step: 850, MSE: 0.069515, PSNR: 11.579233, 10:06:58\n",
      "Step: 851, MSE: 0.069648, PSNR: 11.570916, 10:06:58\n",
      "Step: 852, MSE: 0.069511, PSNR: 11.579486, 10:06:59\n",
      "Step: 853, MSE: 0.069597, PSNR: 11.574100, 10:06:59\n",
      "Step: 854, MSE: 0.069398, PSNR: 11.586499, 10:06:59\n",
      "Step: 855, MSE: 0.069644, PSNR: 11.571177, 10:07:00\n",
      "Step: 856, MSE: 0.069576, PSNR: 11.575418, 10:07:00\n",
      "Step: 857, MSE: 0.069648, PSNR: 11.570913, 10:07:00\n",
      "Step: 858, MSE: 0.069715, PSNR: 11.566721, 10:07:01\n",
      "Step: 859, MSE: 0.069497, PSNR: 11.580362, 10:07:01\n",
      "Step: 860, MSE: 0.069646, PSNR: 11.571054, 10:07:01\n",
      "Step: 861, MSE: 0.069442, PSNR: 11.583780, 10:07:02\n",
      "Step: 862, MSE: 0.069534, PSNR: 11.578049, 10:07:02\n",
      "Step: 863, MSE: 0.069687, PSNR: 11.568509, 10:07:02\n",
      "Step: 864, MSE: 0.069482, PSNR: 11.581266, 10:07:03\n",
      "Step: 865, MSE: 0.069631, PSNR: 11.572000, 10:07:03\n",
      "Step: 866, MSE: 0.069578, PSNR: 11.575277, 10:07:03\n",
      "Step: 867, MSE: 0.069503, PSNR: 11.579964, 10:07:04\n",
      "Step: 868, MSE: 0.069644, PSNR: 11.571175, 10:07:04\n",
      "Step: 869, MSE: 0.069602, PSNR: 11.573800, 10:07:05\n",
      "Step: 870, MSE: 0.069537, PSNR: 11.577850, 10:07:05\n",
      "Step: 871, MSE: 0.069539, PSNR: 11.577731, 10:07:05\n",
      "Step: 872, MSE: 0.069543, PSNR: 11.577477, 10:07:06\n",
      "Step: 873, MSE: 0.069539, PSNR: 11.577744, 10:07:06\n",
      "Step: 874, MSE: 0.069766, PSNR: 11.563560, 10:07:06\n",
      "Step: 875, MSE: 0.069720, PSNR: 11.566438, 10:07:07\n",
      "Step: 876, MSE: 0.069747, PSNR: 11.564775, 10:07:07\n",
      "Step: 877, MSE: 0.069526, PSNR: 11.578537, 10:07:07\n",
      "Step: 878, MSE: 0.069446, PSNR: 11.583537, 10:07:08\n",
      "Step: 879, MSE: 0.069636, PSNR: 11.571675, 10:07:08\n",
      "Step: 880, MSE: 0.069460, PSNR: 11.582682, 10:07:09\n",
      "Step: 881, MSE: 0.069575, PSNR: 11.575478, 10:07:09\n",
      "Step: 882, MSE: 0.069615, PSNR: 11.572950, 10:07:09\n",
      "Step: 883, MSE: 0.069493, PSNR: 11.580582, 10:07:10\n",
      "Step: 884, MSE: 0.069657, PSNR: 11.570369, 10:07:10\n",
      "Step: 885, MSE: 0.069378, PSNR: 11.587777, 10:07:10\n",
      "Step: 886, MSE: 0.069691, PSNR: 11.568240, 10:07:11\n",
      "Step: 887, MSE: 0.069548, PSNR: 11.577162, 10:07:11\n",
      "Step: 888, MSE: 0.069520, PSNR: 11.578911, 10:07:11\n",
      "Step: 889, MSE: 0.069568, PSNR: 11.575895, 10:07:12\n",
      "Step: 890, MSE: 0.069579, PSNR: 11.575231, 10:07:12\n",
      "Step: 891, MSE: 0.069552, PSNR: 11.576897, 10:07:12\n",
      "Step: 892, MSE: 0.069517, PSNR: 11.579096, 10:07:13\n",
      "Step: 893, MSE: 0.069689, PSNR: 11.568333, 10:07:13\n",
      "Step: 894, MSE: 0.069565, PSNR: 11.576118, 10:07:13\n",
      "Step: 895, MSE: 0.069515, PSNR: 11.579190, 10:07:14\n",
      "Step: 896, MSE: 0.069534, PSNR: 11.578028, 10:07:14\n",
      "Step: 897, MSE: 0.069627, PSNR: 11.572212, 10:07:15\n",
      "Step: 898, MSE: 0.069606, PSNR: 11.573543, 10:07:15\n",
      "Step: 899, MSE: 0.069654, PSNR: 11.570529, 10:07:15\n",
      "Step: 900, MSE: 0.069622, PSNR: 11.572562, 10:07:16\n",
      "Step: 901, MSE: 0.069680, PSNR: 11.568935, 10:07:16\n",
      "Step: 902, MSE: 0.069547, PSNR: 11.577223, 10:07:16\n",
      "Step: 903, MSE: 0.069507, PSNR: 11.579693, 10:07:17\n",
      "Step: 904, MSE: 0.069491, PSNR: 11.580688, 10:07:17\n",
      "Step: 905, MSE: 0.069429, PSNR: 11.584567, 10:07:17\n",
      "Step: 906, MSE: 0.069419, PSNR: 11.585192, 10:07:18\n",
      "Step: 907, MSE: 0.069449, PSNR: 11.583358, 10:07:18\n",
      "Step: 908, MSE: 0.069742, PSNR: 11.565060, 10:07:18\n",
      "Step: 909, MSE: 0.069495, PSNR: 11.580450, 10:07:19\n",
      "Step: 910, MSE: 0.069533, PSNR: 11.578107, 10:07:19\n",
      "Step: 911, MSE: 0.069690, PSNR: 11.568291, 10:07:19\n",
      "Step: 912, MSE: 0.069422, PSNR: 11.585002, 10:07:20\n",
      "Step: 913, MSE: 0.069677, PSNR: 11.569118, 10:07:20\n",
      "Step: 914, MSE: 0.069528, PSNR: 11.578423, 10:07:20\n",
      "Step: 915, MSE: 0.069534, PSNR: 11.578032, 10:07:21\n",
      "Step: 916, MSE: 0.069464, PSNR: 11.582379, 10:07:21\n",
      "Step: 917, MSE: 0.069507, PSNR: 11.579718, 10:07:22\n",
      "Step: 918, MSE: 0.069725, PSNR: 11.566086, 10:07:22\n",
      "Step: 919, MSE: 0.069597, PSNR: 11.574085, 10:07:22\n",
      "Step: 920, MSE: 0.069542, PSNR: 11.577516, 10:07:23\n",
      "Step: 921, MSE: 0.069441, PSNR: 11.583839, 10:07:23\n",
      "Step: 922, MSE: 0.069384, PSNR: 11.587420, 10:07:23\n",
      "Step: 923, MSE: 0.069544, PSNR: 11.577405, 10:07:24\n",
      "Step: 924, MSE: 0.069525, PSNR: 11.578596, 10:07:24\n",
      "Step: 925, MSE: 0.069654, PSNR: 11.570563, 10:07:24\n",
      "Step: 926, MSE: 0.069509, PSNR: 11.579611, 10:07:25\n",
      "Step: 927, MSE: 0.069559, PSNR: 11.576487, 10:07:25\n",
      "Step: 928, MSE: 0.069480, PSNR: 11.581383, 10:07:25\n",
      "Step: 929, MSE: 0.069481, PSNR: 11.581317, 10:07:26\n",
      "Step: 930, MSE: 0.069610, PSNR: 11.573261, 10:07:26\n",
      "Step: 931, MSE: 0.069500, PSNR: 11.580135, 10:07:27\n",
      "Step: 932, MSE: 0.069653, PSNR: 11.570589, 10:07:27\n",
      "Step: 933, MSE: 0.069585, PSNR: 11.574849, 10:07:27\n",
      "Step: 934, MSE: 0.069640, PSNR: 11.571390, 10:07:28\n",
      "Step: 935, MSE: 0.069626, PSNR: 11.572277, 10:07:28\n",
      "Step: 936, MSE: 0.069475, PSNR: 11.581739, 10:07:28\n",
      "Step: 937, MSE: 0.069631, PSNR: 11.571975, 10:07:29\n",
      "Step: 938, MSE: 0.069665, PSNR: 11.569830, 10:07:29\n",
      "Step: 939, MSE: 0.069561, PSNR: 11.576327, 10:07:29\n",
      "Step: 940, MSE: 0.069482, PSNR: 11.581284, 10:07:30\n",
      "Step: 941, MSE: 0.069449, PSNR: 11.583356, 10:07:30\n",
      "Step: 942, MSE: 0.069558, PSNR: 11.576537, 10:07:30\n",
      "Step: 943, MSE: 0.069603, PSNR: 11.573698, 10:07:31\n",
      "Step: 944, MSE: 0.069513, PSNR: 11.579343, 10:07:31\n",
      "Step: 945, MSE: 0.069739, PSNR: 11.565239, 10:07:31\n",
      "Step: 946, MSE: 0.069677, PSNR: 11.569133, 10:07:32\n",
      "Step: 947, MSE: 0.069680, PSNR: 11.568942, 10:07:32\n",
      "Step: 948, MSE: 0.069622, PSNR: 11.572552, 10:07:32\n",
      "Step: 949, MSE: 0.069478, PSNR: 11.581522, 10:07:33\n",
      "Step: 950, MSE: 0.069396, PSNR: 11.586686, 10:07:33\n",
      "Step: 951, MSE: 0.069554, PSNR: 11.576759, 10:07:34\n",
      "Step: 952, MSE: 0.069513, PSNR: 11.579324, 10:07:34\n",
      "Step: 953, MSE: 0.069590, PSNR: 11.574533, 10:07:34\n",
      "Step: 954, MSE: 0.069664, PSNR: 11.569891, 10:07:35\n",
      "Step: 955, MSE: 0.069622, PSNR: 11.572560, 10:07:35\n",
      "Step: 956, MSE: 0.069403, PSNR: 11.586200, 10:07:35\n",
      "Step: 957, MSE: 0.069502, PSNR: 11.580011, 10:07:36\n",
      "Step: 958, MSE: 0.069443, PSNR: 11.583728, 10:07:36\n",
      "Step: 959, MSE: 0.069511, PSNR: 11.579494, 10:07:36\n",
      "Step: 960, MSE: 0.069522, PSNR: 11.578780, 10:07:37\n",
      "Step: 961, MSE: 0.069614, PSNR: 11.573023, 10:07:37\n",
      "Step: 962, MSE: 0.069638, PSNR: 11.571523, 10:07:37\n",
      "Step: 963, MSE: 0.069464, PSNR: 11.582399, 10:07:38\n",
      "Step: 964, MSE: 0.069668, PSNR: 11.569693, 10:07:38\n",
      "Step: 965, MSE: 0.069507, PSNR: 11.579735, 10:07:38\n",
      "Step: 966, MSE: 0.069368, PSNR: 11.588391, 10:07:39\n",
      "Step: 967, MSE: 0.069533, PSNR: 11.578075, 10:07:39\n",
      "Step: 968, MSE: 0.069713, PSNR: 11.566841, 10:07:39\n",
      "Step: 969, MSE: 0.069703, PSNR: 11.567486, 10:07:40\n",
      "Step: 970, MSE: 0.069628, PSNR: 11.572137, 10:07:40\n",
      "Step: 971, MSE: 0.069473, PSNR: 11.581868, 10:07:40\n",
      "Step: 972, MSE: 0.069660, PSNR: 11.570181, 10:07:41\n",
      "Step: 973, MSE: 0.069638, PSNR: 11.571559, 10:07:41\n",
      "Step: 974, MSE: 0.069642, PSNR: 11.571293, 10:07:42\n",
      "Step: 975, MSE: 0.069631, PSNR: 11.571965, 10:07:42\n",
      "Step: 976, MSE: 0.069481, PSNR: 11.581367, 10:07:42\n",
      "Step: 977, MSE: 0.069547, PSNR: 11.577225, 10:07:43\n",
      "Step: 978, MSE: 0.069518, PSNR: 11.578998, 10:07:43\n",
      "Step: 979, MSE: 0.069710, PSNR: 11.567029, 10:07:43\n",
      "Step: 980, MSE: 0.069378, PSNR: 11.587790, 10:07:44\n",
      "Step: 981, MSE: 0.069581, PSNR: 11.575115, 10:07:44\n",
      "Step: 982, MSE: 0.069534, PSNR: 11.578056, 10:07:44\n",
      "Step: 983, MSE: 0.069547, PSNR: 11.577205, 10:07:45\n",
      "Step: 984, MSE: 0.069594, PSNR: 11.574265, 10:07:45\n",
      "Step: 985, MSE: 0.069617, PSNR: 11.572818, 10:07:45\n",
      "Step: 986, MSE: 0.069639, PSNR: 11.571464, 10:07:46\n",
      "Step: 987, MSE: 0.069644, PSNR: 11.571179, 10:07:46\n",
      "Step: 988, MSE: 0.069428, PSNR: 11.584671, 10:07:46\n",
      "Step: 989, MSE: 0.069480, PSNR: 11.581426, 10:07:47\n",
      "Step: 990, MSE: 0.069620, PSNR: 11.572638, 10:07:47\n",
      "Step: 991, MSE: 0.069622, PSNR: 11.572521, 10:07:47\n",
      "Step: 992, MSE: 0.069638, PSNR: 11.571543, 10:07:48\n",
      "Step: 993, MSE: 0.069680, PSNR: 11.568913, 10:07:48\n",
      "Step: 994, MSE: 0.069447, PSNR: 11.583467, 10:07:48\n",
      "Step: 995, MSE: 0.069608, PSNR: 11.573410, 10:07:49\n",
      "Step: 996, MSE: 0.069498, PSNR: 11.580275, 10:07:49\n",
      "Step: 997, MSE: 0.069545, PSNR: 11.577343, 10:07:49\n",
      "Step: 998, MSE: 0.069510, PSNR: 11.579541, 10:07:50\n",
      "Step: 999, MSE: 0.069615, PSNR: 11.573001, 10:07:50\n",
      "Step: 1000, MSE: 0.069528, PSNR: 11.578383, 10:07:51\n",
      "Step: 1001, MSE: 0.069464, PSNR: 11.582374, 10:07:51\n",
      "Step: 1002, MSE: 0.069578, PSNR: 11.575268, 10:07:51\n",
      "Step: 1003, MSE: 0.069502, PSNR: 11.580032, 10:07:52\n",
      "Step: 1004, MSE: 0.069491, PSNR: 11.580733, 10:07:52\n",
      "Step: 1005, MSE: 0.069490, PSNR: 11.580771, 10:07:52\n",
      "Step: 1006, MSE: 0.069570, PSNR: 11.575805, 10:07:53\n",
      "Step: 1007, MSE: 0.069603, PSNR: 11.573700, 10:07:53\n",
      "Step: 1008, MSE: 0.069418, PSNR: 11.585254, 10:07:53\n",
      "Step: 1009, MSE: 0.069607, PSNR: 11.573458, 10:07:54\n",
      "Step: 1010, MSE: 0.069689, PSNR: 11.568341, 10:07:54\n",
      "Step: 1011, MSE: 0.069609, PSNR: 11.573339, 10:07:54\n",
      "Step: 1012, MSE: 0.069568, PSNR: 11.575891, 10:07:55\n",
      "Step: 1013, MSE: 0.069643, PSNR: 11.571249, 10:07:55\n",
      "Step: 1014, MSE: 0.069641, PSNR: 11.571339, 10:07:55\n",
      "Step: 1015, MSE: 0.069532, PSNR: 11.578164, 10:07:56\n",
      "Step: 1016, MSE: 0.069474, PSNR: 11.581782, 10:07:56\n",
      "Step: 1017, MSE: 0.069288, PSNR: 11.593408, 10:07:57\n",
      "Step: 1018, MSE: 0.069409, PSNR: 11.585815, 10:07:57\n",
      "Step: 1019, MSE: 0.069546, PSNR: 11.577250, 10:07:57\n",
      "Step: 1020, MSE: 0.069575, PSNR: 11.575471, 10:07:58\n",
      "Step: 1021, MSE: 0.069501, PSNR: 11.580114, 10:07:58\n",
      "Step: 1022, MSE: 0.069512, PSNR: 11.579412, 10:07:58\n",
      "Step: 1023, MSE: 0.069576, PSNR: 11.575420, 10:07:59\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 359  |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 427\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# PPO \u001b[39;00m\n\u001b[1;32m    416\u001b[0m ppo_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    418\u001b[0m     venv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ppo_with_mask/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m )\n\u001b[0;32m--> 427\u001b[0m \u001b[43mppo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[1;32m    430\u001b[0m ppo_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_with_mask_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/ppo/ppo.py:213\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 213\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py:738\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    736\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[1;32m    737\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 738\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    740\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/distributions.py:175\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.log_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Get the log probabilities of actions according to the distribution.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Note that you must first call the ``proba_distribution()`` method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sum_independent_dims(log_prob)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/distributions/normal.py:84\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[1;32m     82\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     83\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 84\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[1;32m     90\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.96 GiB is free. Process 704520 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 21.30 GiB memory in use. Of the allocated memory 19.73 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torchOptics.optics as tt\n",
    "import torch.nn as nn\n",
    "import torchOptics.metrics as tm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3 import PPO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#      \n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self, num_hologram, final='Sigmoid', in_planes=3,\n",
    "                 channels=[32, 64, 128, 256, 512, 1024, 2048, 4096],\n",
    "                 convReLU=True, convBN=True, poolReLU=True, poolBN=True,\n",
    "                 deconvReLU=True, deconvBN=True):\n",
    "        super(BinaryNet, self).__init__()\n",
    "\n",
    "        def CRB2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            if relu:\n",
    "                layers += [nn.Tanh()]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        def TRB2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True, relu=True, bn=True):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                          kernel_size=2, stride=2, padding=0,\n",
    "                                          bias=True)]\n",
    "            if bn:\n",
    "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            if relu:\n",
    "                layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)  # * list unpacking\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        self.enc1_1 = CRB2d(in_planes, channels[0], relu=convReLU, bn=convBN)\n",
    "        self.enc1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.pool1 = CRB2d(channels[0], channels[0], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc2_1 = CRB2d(channels[0], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.enc2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.pool2 = CRB2d(channels[1], channels[1], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc3_1 = CRB2d(channels[1], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.enc3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.pool3 = CRB2d(channels[2], channels[2], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc4_1 = CRB2d(channels[2], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.enc4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.pool4 = CRB2d(channels[3], channels[3], stride=2, relu=poolReLU, bn=poolBN)\n",
    "\n",
    "        self.enc5_1 = CRB2d(channels[3], channels[4], relu=convReLU, bn=convBN)\n",
    "        self.enc5_2 = CRB2d(channels[4], channels[4], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv4 = TRB2d(channels[4], channels[3], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec4_1 = CRB2d(channels[4], channels[3], relu=convReLU, bn=convBN)\n",
    "        self.dec4_2 = CRB2d(channels[3], channels[3], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv3 = TRB2d(channels[3], channels[2], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec3_1 = CRB2d(channels[3], channels[2], relu=convReLU, bn=convBN)\n",
    "        self.dec3_2 = CRB2d(channels[2], channels[2], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv2 = TRB2d(channels[2], channels[1], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec2_1 = CRB2d(channels[2], channels[1], relu=convReLU, bn=convBN)\n",
    "        self.dec2_2 = CRB2d(channels[1], channels[1], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.deconv1 = TRB2d(channels[1], channels[0], relu=deconvReLU, bn=deconvBN, stride=2)\n",
    "        self.dec1_1 = CRB2d(channels[1], channels[0], relu=convReLU, bn=convBN)\n",
    "        self.dec1_2 = CRB2d(channels[0], channels[0], relu=convReLU, bn=convBN)\n",
    "\n",
    "        self.classifier = CRB2d(channels[0], num_hologram, relu=False, bn=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "\n",
    "        deconv4 = self.deconv4(enc5_2)\n",
    "        concat4 = torch.cat((deconv4, enc4_2), dim=1)\n",
    "        dec4_1 = self.dec4_1(concat4)\n",
    "        dec4_2 = self.dec4_2(dec4_1)\n",
    "\n",
    "        deconv3 = self.deconv3(dec4_2)\n",
    "        concat3 = torch.cat((deconv3, enc3_2), dim=1)\n",
    "        dec3_1 = self.dec3_1(concat3)\n",
    "        dec3_2 = self.dec3_2(dec3_1)\n",
    "\n",
    "        deconv2 = self.deconv2(dec3_2)\n",
    "        concat2 = torch.cat((deconv2, enc2_2), dim=1)\n",
    "        dec2_1 = self.dec2_1(concat2)\n",
    "        dec2_2 = self.dec2_2(dec2_1)\n",
    "\n",
    "        deconv1 = self.deconv1(dec2_2)\n",
    "        concat1 = torch.cat((deconv1, enc1_2), dim=1)\n",
    "        dec1_1 = self.dec1_1(concat1)\n",
    "        dec1_2 = self.dec1_2(dec1_1)\n",
    "\n",
    "        # Final classifier\n",
    "        out = self.classifier(dec1_2)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False,\n",
    "                  convBN=False, poolReLU=False, poolBN=False,\n",
    "                  deconvReLU=False, deconvBN=False).cuda()\n",
    "test = torch.randn(1, 1, 1024, 1024).cuda()\n",
    "out = model(test)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "class Dataset512(Dataset):\n",
    "    def __init__(self, target_dir, meta, transform=None, isTrain=True, padding=0):\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "        self.meta = meta\n",
    "        self.isTrain = isTrain\n",
    "        self.target_list = sorted(glob.glob(target_dir+'*.png'))\n",
    "        self.center_crop = torchvision.transforms.CenterCrop(1024)\n",
    "        self.random_crop = torchvision.transforms.RandomCrop((1024, 1024))\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        target = tt.imread(self.target_list[idx], meta=meta, gray=True).unsqueeze(0)\n",
    "        if target.shape[-1] < 1024 or target.shape[-2] < 1024:\n",
    "            target = torchvision.transforms.Resize(1024)(target)\n",
    "        if self.isTrain:\n",
    "            target = self.random_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        else:\n",
    "            target = self.center_crop(target)\n",
    "            target = torchvision.transforms.functional.pad(target, (self.padding, self.padding, self.padding, self.padding))\n",
    "        return target\n",
    "\n",
    "\n",
    "#BinaryHologramEnv \n",
    "class BinaryHologramEnv(gym.Env):\n",
    "    def __init__(self, target_function, trainloader, max_steps=100000, T_PSNR=30, T_steps=1000):\n",
    "        \"\"\"\n",
    "        target_function:   (MSE  PSNR)  .\n",
    "        trainloader:   .\n",
    "        max_steps:   .\n",
    "        T_PSNR:  PSNR .\n",
    "        T_steps: PSNR     .\n",
    "        \"\"\"\n",
    "        super(BinaryHologramEnv, self).__init__()\n",
    "\n",
    "        #   (1, 8, 1024, 1024)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.float32)\n",
    "\n",
    "        #  : (1, 8, 1024, 1024)   \n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1, 8, 1024, 1024), dtype=np.int8)\n",
    "\n",
    "        #     \n",
    "        self.target_function = target_function  # BinaryNet \n",
    "        self.trainloader = trainloader          #   \n",
    "\n",
    "        #  \n",
    "        self.max_steps = max_steps\n",
    "        self.T_PSNR = T_PSNR\n",
    "        self.T_steps = T_steps\n",
    "\n",
    "        #  \n",
    "        self.state = None\n",
    "        self.observation = None\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "\n",
    "        #     \n",
    "        self.data_iter = iter(self.trainloader)\n",
    "        self.target_image = None\n",
    "\n",
    "    def reset(self, seed=None, options=None, lr=1e-4, z=2e-3):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            self.target_image = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.trainloader)\n",
    "            self.target_image = next(self.data_iter)\n",
    "\n",
    "        self.target_image = self.target_image.cuda()\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.psnr_sustained_steps = 0\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  #  \n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta  \n",
    "\n",
    "        # \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE  PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "\n",
    "    def initialize_state(self, z=2e-3):\n",
    "        \"\"\"\n",
    "          ,     .\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            model_output = self.target_function(self.target_image)\n",
    "        self.observation = model_output.cpu().numpy()  # (1, 8, 1024, 1024)\n",
    "\n",
    "        self.state = (self.observation >= 0.5).astype(np.int8)  #  \n",
    "\n",
    "        binary = torch.tensor(self.state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta  \n",
    "\n",
    "        # \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE  PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "\n",
    "        print(f\"Initial MSE: {mse:.6f}, Initial PSNR: {psnr:.6f}, {current_date}\")\n",
    "\n",
    "        #  \n",
    "        self.observation = result.detach().cpu().numpy()\n",
    "        mask = self.create_action_mask(self.observation)\n",
    "\n",
    "        return self.observation, {\"state\": self.state, \"mask\": mask}\n",
    "\n",
    "    def step(self, action, lr=1e-4, z=2e-3):\n",
    "        #       \n",
    "        if self.steps == 0:\n",
    "            print(\"Executing reset logic for the first step\")\n",
    "            self.steps += 1  #  \n",
    "            # reset      \n",
    "            observation, info = self.initialize_state(z)\n",
    "            return observation, 0.0, False, False, info\n",
    "\n",
    "        #       \n",
    "        new_state = np.logical_xor(self.state, action).astype(np.int8)\n",
    "\n",
    "        #    torch  \n",
    "        binary = torch.tensor(new_state, dtype=torch.float32).cuda()\n",
    "        binary = tt.Tensor(binary, meta={'dx': (7.56e-6, 7.56e-6), 'wl': 515e-9})  # meta  \n",
    "\n",
    "        #  \n",
    "        sim = tt.simulate(binary, z).abs()**2\n",
    "        result = torch.mean(sim, dim=1, keepdim=True)\n",
    "\n",
    "        # MSE  PSNR \n",
    "        mse = tt.relativeLoss(result, self.target_image, F.mse_loss).detach().cpu().numpy()\n",
    "        psnr = tt.relativeLoss(result, self.target_image, tm.get_PSNR)\n",
    "        reward = -mse\n",
    "\n",
    "        #  \n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Step: {self.steps}, MSE: {mse:.6f}, PSNR: {psnr:.6f}, {current_time}\")\n",
    "\n",
    "        #  \n",
    "        self.state = new_state\n",
    "        self.observation = self.state  #    \n",
    "\n",
    "        #  \n",
    "        terminated = self.steps >= self.max_steps or self.psnr_sustained_steps >= self.T_steps\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        if psnr >= self.T_PSNR:\n",
    "            self.psnr_sustained_steps += 1\n",
    "        else:\n",
    "            self.psnr_sustained_steps = 0\n",
    "\n",
    "        #   \n",
    "        mask = self.create_action_mask(self.observation)\n",
    "        info = {\"mse\": mse, \"psnr\": psnr, \"mask\": mask}\n",
    "\n",
    "        del binary, sim, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.steps += 1\n",
    "        return self.observation, reward, terminated, truncated, info\n",
    "\n",
    "    def create_action_mask(self, observation):\n",
    "        \"\"\"\n",
    "            .\n",
    "         0~0.2  ->  0 .\n",
    "         0.8~1  ->  1 .\n",
    "        \"\"\"\n",
    "        mask = np.ones_like(observation, dtype=np.int8)  #    \n",
    "        mask[observation <= 0.2] = 0  #  0~0.2  0 \n",
    "        mask[observation >= 0.8] = 1  #  0.8~1  1 \n",
    "        return mask\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "batch_size = 1\n",
    "target_dir = '/nfs/dataset/DIV2K/DIV2K_train_HR/DIV2K_train_HR/'\n",
    "valid_dir = '/nfs/dataset/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR/'\n",
    "meta = {'wl': (515e-9), 'dx': (7.56e-6, 7.56e-6)}  #  \n",
    "padding = 0\n",
    "\n",
    "# Dataset512  \n",
    "train_dataset = Dataset512(target_dir=target_dir, meta=meta, isTrain=True, padding=padding)\n",
    "valid_dataset = Dataset512(target_dir=valid_dir, meta=meta, isTrain=False, padding=padding)\n",
    "\n",
    "# DataLoader \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# BinaryNet  \n",
    "model = BinaryNet(num_hologram=8, in_planes=1, convReLU=False, convBN=False,\n",
    "                  poolReLU=False, poolBN=False, deconvReLU=False, deconvBN=False).cuda()\n",
    "model.load_state_dict(torch.load('result_v/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002/2024-12-15 14:02:27.770108_pre_reinforce_8_0.002'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#   \n",
    "def mask_fn(env):\n",
    "    return env.create_action_mask(env.observation)\n",
    "\n",
    "#      \n",
    "env = BinaryHologramEnv(\n",
    "    target_function=model,\n",
    "    trainloader=train_loader,  #  train_loader \n",
    "    max_steps=100000,\n",
    "    T_PSNR=30,\n",
    "    T_steps=1000\n",
    ")\n",
    "\n",
    "# ActionMasker  \n",
    "env = ActionMasker(env, mask_fn)\n",
    "\n",
    "# Vectorized  \n",
    "venv = make_vec_env(lambda: env, n_envs=1)\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "# PPO \n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    "    verbose=2,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"./ppo_with_mask/\"\n",
    ")\n",
    "\n",
    "ppo_model.learn(total_timesteps=10000000)\n",
    "\n",
    "#  \n",
    "ppo_model.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "# VecNormalize \n",
    "venv.save(f\"ppo_with_mask_{current_date}\")\n",
    "\n",
    "\n",
    "\n",
    "#   \n",
    "#eval_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# EvalCallback \n",
    "#eval_callback = EvalCallback(\n",
    "#    eval_env,\n",
    "#    best_model_save_path='./logs/',\n",
    "#    log_path='./logs/',\n",
    "#    eval_freq=10000,  #   ( )\n",
    "#    deterministic=True,\n",
    "#    render=False\n",
    "#)\n",
    "\n",
    "#ppo_model = PPO(\n",
    "#    \"MlpPolicy\",\n",
    "#    venv,\n",
    "#    verbose=2,\n",
    "#    n_steps=1024,\n",
    "#    batch_size=64,\n",
    "#    gamma=0.99,\n",
    "#    learning_rate=3e-4,\n",
    "#    tensorboard_log=\"./ppo_with_mask/\"\n",
    "#)\n",
    "\n",
    "#   ( )\n",
    "#ppo_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c83-99b3-4f86-bc23-89c6b278e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c7a2-bd3a-440c-9ad3-3ec9a08af86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
